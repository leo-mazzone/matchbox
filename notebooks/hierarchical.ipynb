{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical algorithm optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rustworkx as rx\n",
    "from collections import Counter\n",
    "\n",
    "def verify_components(table) -> dict:\n",
    "    \"\"\"\n",
    "    Fast verification of connected components using rustworkx.\n",
    "    \n",
    "    Args:\n",
    "        table: PyArrow table with 'left', 'right' columns\n",
    "    \n",
    "    Returns:\n",
    "        dictionary containing basic component statistics\n",
    "    \"\"\"\n",
    "    # Create graph directly from arrays\n",
    "    graph = rx.PyDiGraph()\n",
    "    \n",
    "    # Add all unique nodes at once\n",
    "    unique_nodes = set(table['left'].to_numpy()) | set(table['right'].to_numpy())\n",
    "    graph.add_nodes_from(range(len(unique_nodes)))\n",
    "    \n",
    "    # Create node mapping and edges in one pass\n",
    "    node_to_idx = {node: idx for idx, node in enumerate(unique_nodes)}\n",
    "    edges = [(node_to_idx[left], node_to_idx[right], prob) \n",
    "            for left, right, prob in zip(table['left'].to_numpy(), \n",
    "                                       table['right'].to_numpy(),\n",
    "                                       table['probability'].to_numpy())]\n",
    "    \n",
    "    # Add all edges at once\n",
    "    graph.add_edges_from(edges)\n",
    "    \n",
    "    # Get components and their sizes\n",
    "    components = rx.weakly_connected_components(graph)\n",
    "    component_sizes = Counter(len(component) for component in components)\n",
    "    \n",
    "    return {\n",
    "        'num_components': len(components),\n",
    "        'total_nodes': len(unique_nodes),\n",
    "        'total_edges': len(edges),\n",
    "        'component_sizes': component_sizes,\n",
    "        'min_component_size': min(component_sizes.keys()),\n",
    "        'max_component_size': max(component_sizes.keys())\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_max_possible_edges(n_nodes: int, num_components: int) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the maximum possible number of edges given n nodes split into k components.\n",
    "    \n",
    "    Args:\n",
    "        n_nodes: Total number of nodes\n",
    "        num_components: Number of components to split into\n",
    "        \n",
    "    Returns:\n",
    "        Maximum possible number of edges\n",
    "    \"\"\"\n",
    "    nodes_per_component = n_nodes // num_components\n",
    "    max_edges_per_component = nodes_per_component * nodes_per_component  # Complete bipartite graph\n",
    "    return max_edges_per_component * num_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import rustworkx as rx\n",
    "from typing import List, Tuple\n",
    "from decimal import Decimal\n",
    "\n",
    "def split_values_into_components(values: List[int], num_components: int) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Split values into non-overlapping groups for each component.\n",
    "    \n",
    "    Args:\n",
    "        values: List of values to split\n",
    "        num_components: Number of components to create\n",
    "        \n",
    "    Returns:\n",
    "        List of arrays, one for each component\n",
    "    \"\"\"\n",
    "    values = np.array(values)\n",
    "    np.random.shuffle(values)\n",
    "    return np.array_split(values, num_components)\n",
    "\n",
    "\n",
    "def generate_arrow_data(\n",
    "    left_values: List[int],\n",
    "    right_values: List[int],\n",
    "    prob_range: Tuple[float, float],\n",
    "    num_components: int,\n",
    "    total_rows: int\n",
    ") -> pa.Table:\n",
    "    \"\"\"\n",
    "    Generate dummy arrow data with guaranteed isolated components.\n",
    "    \n",
    "    Args:\n",
    "        left_values: List of integers to use for left column\n",
    "        right_values: List of integers to use for right column\n",
    "        prob_range: Tuple of (min_prob, max_prob) to constrain probabilities\n",
    "        num_components: Number of distinct connected components to generate\n",
    "        total_rows: Total number of rows to generate\n",
    "    \n",
    "    Returns:\n",
    "        PyArrow Table with 'left', 'right', and 'probability' columns\n",
    "    \"\"\"\n",
    "    if len(left_values) < 2 or len(right_values) < 2:\n",
    "        raise ValueError(\"Need at least 2 possible values for both left and right\")\n",
    "    if num_components > min(len(left_values), len(right_values)):\n",
    "        raise ValueError(\"Cannot have more components than minimum of left/right values\")\n",
    "    \n",
    "    # Calculate maximum possible edges\n",
    "    min_nodes = min(len(left_values), len(right_values))\n",
    "    max_possible_edges = calculate_max_possible_edges(min_nodes, num_components)\n",
    "    \n",
    "    if total_rows > max_possible_edges:\n",
    "        raise ValueError(\n",
    "            f\"Cannot generate {total_rows:,} edges with {num_components:,} components. \"\n",
    "            f\"Maximum possible edges is {max_possible_edges:,} given {min_nodes:,} nodes. \"\n",
    "            \"Either increase the number of nodes, decrease the number of components, \"\n",
    "            \"or decrease the total edges requested.\"\n",
    "        )\n",
    "    \n",
    "    # Convert probability range to integers (60-80 for 0.60-0.80)\n",
    "    prob_min = int(prob_range[0] * 100)\n",
    "    prob_max = int(prob_range[1] * 100)\n",
    "    \n",
    "    # Split values into completely separate groups for each component\n",
    "    left_components = split_values_into_components(left_values, num_components)\n",
    "    right_components = split_values_into_components(right_values, num_components)\n",
    "    \n",
    "    # Calculate base number of edges per component\n",
    "    base_edges_per_component = total_rows // num_components\n",
    "    remaining_edges = total_rows % num_components\n",
    "    \n",
    "    all_edges = []\n",
    "    \n",
    "    # Generate edges for each component\n",
    "    for comp_idx in range(num_components):\n",
    "        comp_left_values = left_components[comp_idx]\n",
    "        comp_right_values = right_components[comp_idx]\n",
    "        \n",
    "        # Calculate edges for this component\n",
    "        edges_in_component = base_edges_per_component\n",
    "        if comp_idx < remaining_edges:  # Distribute remaining edges\n",
    "            edges_in_component += 1\n",
    "            \n",
    "        # Ensure basic connectivity within the component\n",
    "        base_edges = []\n",
    "        \n",
    "        # Create a spanning tree-like structure\n",
    "        for i in range(len(comp_left_values)):\n",
    "            base_edges.append((\n",
    "                comp_left_values[i],\n",
    "                comp_right_values[i % len(comp_right_values)],\n",
    "                np.random.randint(prob_min, prob_max + 1)\n",
    "            ))\n",
    "        \n",
    "        # Generate remaining random edges strictly within this component\n",
    "        remaining_edges = edges_in_component - len(base_edges)\n",
    "        if remaining_edges > 0:\n",
    "            random_lefts = np.random.choice(comp_left_values, size=remaining_edges)\n",
    "            random_rights = np.random.choice(comp_right_values, size=remaining_edges)\n",
    "            random_probs = np.random.randint(prob_min, prob_max + 1, size=remaining_edges)\n",
    "            \n",
    "            component_edges = base_edges + list(zip(random_lefts, random_rights, random_probs))\n",
    "        else:\n",
    "            component_edges = base_edges\n",
    "            \n",
    "        all_edges.extend(component_edges)\n",
    "    \n",
    "    # Convert to arrays\n",
    "    lefts, rights, probs = zip(*all_edges)\n",
    "    \n",
    "    # Create PyArrow arrays\n",
    "    left_array = pa.array(lefts, type=pa.int64())\n",
    "    right_array = pa.array(rights, type=pa.int64())\n",
    "    decimal_probs = [Decimal(str(p/100)) for p in probs]\n",
    "    prob_array = pa.array(decimal_probs, type=pa.decimal128(precision=3, scale=2))\n",
    "    \n",
    "    return pa.table([left_array, right_array, prob_array],\n",
    "                   names=['left', 'right', 'probability'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x10d274c90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components found: 10\n",
      "Total nodes: 20000\n",
      "Total edges: 1000009\n",
      "\n",
      "Component sizes:\n",
      "Size 2000: 10 components\n"
     ]
    }
   ],
   "source": [
    "left_values = list(range(10_000))\n",
    "right_values = list(range(10_000, 20_000))\n",
    "prob_range = (0.6, 0.8)\n",
    "num_components = 10\n",
    "total_rows = 1_000_000\n",
    "\n",
    "table = generate_arrow_data(\n",
    "    left_values=left_values,\n",
    "    right_values=right_values,\n",
    "    prob_range=prob_range,\n",
    "    num_components=num_components,\n",
    "    total_rows=total_rows\n",
    ")\n",
    "\n",
    "results = verify_components(table)\n",
    "print(f\"Number of components found: {results['num_components']}\")\n",
    "print(f\"Total nodes: {results['total_nodes']}\")\n",
    "print(f\"Total edges: {results['total_edges']}\")\n",
    "print(\"\\nComponent sizes:\")\n",
    "for size, count in sorted(results['component_sizes'].items()):\n",
    "    print(f\"Size {size}: {count} components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_values = list(range(int(2e7)))\n",
    "right_values = list(range(int(2e7), int(4e7)))\n",
    "prob_range = (0.7, 1.0)\n",
    "num_components = 200_000\n",
    "total_rows = int(1e8)\n",
    "\n",
    "table = generate_arrow_data(\n",
    "    left_values=left_values,\n",
    "    right_values=right_values,\n",
    "    prob_range=prob_range,\n",
    "    num_components=num_components,\n",
    "    total_rows=total_rows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components found: 206763\n",
      "Total nodes: 40000000\n",
      "Total edges: 100000400\n",
      "\n",
      "Component sizes:\n",
      "Size 2: 6755 components\n",
      "Size 4: 8 components\n",
      "Size 194: 1 components\n",
      "Size 196: 124 components\n",
      "Size 198: 6520 components\n",
      "Size 200: 193355 components\n"
     ]
    }
   ],
   "source": [
    "results = verify_components(table)\n",
    "print(f\"Number of components found: {results['num_components']}\")\n",
    "print(f\"Total nodes: {results['total_nodes']}\")\n",
    "print(f\"Total edges: {results['total_edges']}\")\n",
    "print(\"\\nComponent sizes:\")\n",
    "for size, count in sorted(results['component_sizes'].items()):\n",
    "    print(f\"Size {size}: {count} components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of components found: 206763\n",
    "* Total nodes: 40000000\n",
    "* Total edges: 100000400\n",
    "\n",
    "Component sizes:\n",
    "* Size 2: 6755 components\n",
    "* Size 4: 8 components\n",
    "* Size 194: 1 components\n",
    "* Size 196: 124 components\n",
    "* Size 198: 6520 components\n",
    "* Size 200: 193355 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "pq.write_table(table, Path.cwd() / 'hierarchical_cc200k.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_values = list(range(int(2e5)))\n",
    "right_values = list(range(int(2e5), int(4e5)))\n",
    "prob_range = (0.7, 1.0)\n",
    "num_components = 2_000\n",
    "total_rows = int(1e6)\n",
    "\n",
    "table2 = generate_arrow_data(\n",
    "    left_values=left_values,\n",
    "    right_values=right_values,\n",
    "    prob_range=prob_range,\n",
    "    num_components=num_components,\n",
    "    total_rows=total_rows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components found: 2,080\n",
      "Total nodes: 400,000\n",
      "Total edges: 1,000,400\n",
      "\n",
      "Component sizes:\n",
      "Size 2: 80 components\n",
      "Size 196: 1 components\n",
      "Size 198: 78 components\n",
      "Size 200: 1,921 components\n"
     ]
    }
   ],
   "source": [
    "results2 = verify_components(table2)\n",
    "print(f\"Number of components found: {results2['num_components']:,}\")\n",
    "print(f\"Total nodes: {results2['total_nodes']:,}\")\n",
    "print(f\"Total edges: {results2['total_edges']:,}\")\n",
    "print(\"\\nComponent sizes:\")\n",
    "for size, count in sorted(results2['component_sizes'].items()):\n",
    "    print(f\"Size {size:,}: {count:,} components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of components found: 2,080\n",
    "* Total nodes: 400,000\n",
    "* Total edges: 1,000,400\n",
    "\n",
    "Component sizes:\n",
    "* Size 2: 80 components\n",
    "* Size 196: 1 components\n",
    "* Size 198: 78 components\n",
    "* Size 200: 1,921 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "pq.write_table(table2, Path.cwd() / 'hierarchical_cc2k.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "h2 = pq.read_table('hierarchical_cc2k.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "left: int64\n",
       "right: int64\n",
       "probability: decimal128(3, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plan.\n",
    "\n",
    "* Find components and their sizes at lowest threshold (rustworkx)\n",
    "* Use this to dask.groupby the data for parallel per-component processing\n",
    "* Ensure we implement early stopping!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2080,\n",
       " pyarrow.Table\n",
       " left: int64\n",
       " right: int64\n",
       " probability: decimal128(3, 2)\n",
       " component: int64\n",
       " ----\n",
       " left: [[197413,116407,114551,160857,6412,...,39429,156175,197197,48177,121674]]\n",
       " right: [[326505,384163,344025,248700,258884,...,311452,278755,204144,357956,378111]]\n",
       " probability: [[1.00,1.00,1.00,1.00,1.00,...,0.70,0.70,0.70,0.70,0.70]]\n",
       " component: [[0,0,0,0,0,...,2079,2079,2079,2079,2079]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import rustworkx as rx\n",
    "import numpy as np\n",
    "\n",
    "def attach_independent_components(table: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Returns the original table with an additional 'component' column indicating\n",
    "    which connected component each edge belongs to.\n",
    "    \"\"\"\n",
    "    # Create dictionary array from sorted unique values\n",
    "    unique = pc.unique(\n",
    "        pa.concat_arrays([\n",
    "            table['left'].combine_chunks(),\n",
    "            table['right'].combine_chunks()\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    # Get indices into unique array for graph construction\n",
    "    left_indices = pc.index_in(table['left'], unique)\n",
    "    right_indices = pc.index_in(table['right'], unique)\n",
    "    \n",
    "    # Create and process graph\n",
    "    n_nodes = len(unique)\n",
    "    n_edges = len(table)\n",
    "    \n",
    "    graph = rx.PyGraph(\n",
    "        node_count_hint=n_nodes,\n",
    "        edge_count_hint=n_edges\n",
    "    )\n",
    "    graph.add_nodes_from(range(n_nodes))\n",
    "\n",
    "    edges = tuple(zip(left_indices.to_numpy(), right_indices.to_numpy()))\n",
    "    graph.add_edges_from_no_data(edges)\n",
    "    \n",
    "    # Get components and create mapping array\n",
    "    components = rx.connected_components(graph)\n",
    "    \n",
    "    # Convert components to numpy arrays\n",
    "    component_indices = np.concatenate([np.array(list(c)) for c in components])\n",
    "    component_labels = np.repeat(np.arange(len(components)), [len(c) for c in components])\n",
    "    \n",
    "    # Create mapping array and fill with component labels\n",
    "    node_to_component = np.zeros(len(unique), dtype=np.int64)\n",
    "    node_to_component[component_indices] = component_labels\n",
    "    \n",
    "    # Use the indices we already have to map back to components  \n",
    "    edge_components = pa.array(node_to_component[left_indices.to_numpy()])\n",
    "    \n",
    "    return table.append_column('component', edge_components).sort_by(\n",
    "        [('component', 'ascending'), ('probability', 'descending')]\n",
    "    )\n",
    "\n",
    "cc2 = attach_independent_components(h2)\n",
    "len(pc.unique(cc2.column('component'))), cc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         34170 function calls in 1.108 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 49 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        2    0.000    0.000    1.108    0.554 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3541(run_code)\n",
      "        2    0.000    0.000    1.108    0.554 {built-in method builtins.exec}\n",
      "        1    0.058    0.058    1.108    1.108 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_25768/655470289.py:1(<module>)\n",
      "        1    0.397    0.397    1.050    1.050 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_25768/2783446239.py:5(attach_independent_components)\n",
      "        3    0.296    0.099    0.296    0.099 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/compute.py:249(wrapper)\n",
      "        1    0.118    0.118    0.118    0.118 {connected_components}\n",
      "        1    0.088    0.088    0.088    0.088 {method 'add_edges_from_no_data' of 'rustworkx.PyGraph' objects}\n",
      "        1    0.066    0.066    0.066    0.066 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/compute.py:239(wrapper)\n",
      "        1    0.007    0.007    0.033    0.033 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_25768/2783446239.py:39(<listcomp>)\n",
      "     2080    0.026    0.000    0.026    0.000 {built-in method numpy.array}\n",
      "     2850    0.008    0.000    0.021    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py:775(_clean_thread_parent_frames)\n",
      "        1    0.017    0.017    0.017    0.017 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/compute.py:446(take)\n",
      "        1    0.013    0.013    0.013    0.013 {method 'add_nodes_from' of 'rustworkx.PyGraph' objects}\n",
      "     1425    0.005    0.000    0.008    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py:790(<setcomp>)\n",
      "     1425    0.004    0.000    0.005    0.000 /Users/willlangdale/.pyenv/versions/3.11.8/lib/python3.11/threading.py:1501(enumerate)\n",
      "    11400    0.003    0.000    0.003    0.000 /Users/willlangdale/.pyenv/versions/3.11.8/lib/python3.11/threading.py:1168(ident)\n",
      "     5700    0.001    0.000    0.001    0.000 {method 'keys' of 'dict' objects}\n",
      "        1    0.000    0.000    0.001    0.001 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_25768/2783446239.py:40(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:467(repeat)\n",
      "        1    0.000    0.000    0.000    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:51(_wrapfunc)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x17d84cdd0>"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "from pstats import SortKey\n",
    "\n",
    "# Profile the function\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "components = attach_independent_components(table2)\n",
    "pr.disable()\n",
    "\n",
    "# Print stats sorted by cumulative time\n",
    "ps = pstats.Stats(pr).sort_stats(SortKey.CUMULATIVE)\n",
    "ps.print_stats(20)  # Show top 20 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         3411228 function calls in 121.198 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 48 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        2    0.000    0.000  139.163   69.581 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3541(run_code)\n",
      "        2    0.001    0.000  139.162   69.581 {built-in method builtins.exec}\n",
      "        1   47.532   47.532  121.195  121.195 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_25768/2801194547.py:1(find_independent_components)\n",
      "        1   24.123   24.123   24.123   24.123 {method 'add_edges_from_no_data' of 'rustworkx.PyGraph' objects}\n",
      "        1   18.713   18.713   18.731   18.731 {connected_components}\n",
      "        2   14.375    7.187   14.375    7.187 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/compute.py:249(wrapper)\n",
      "        1    7.661    7.661    7.661    7.661 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/compute.py:239(wrapper)\n",
      "        1    1.303    1.303    4.041    4.041 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_25768/2801194547.py:35(<listcomp>)\n",
      "   206763    2.738    0.000    2.738    0.000 {built-in method numpy.array}\n",
      "   285488    0.966    0.000    2.629    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py:775(_clean_thread_parent_frames)\n",
      "        1    1.879    1.879    1.879    1.879 {method 'add_nodes_from' of 'rustworkx.PyGraph' objects}\n",
      "   142744    0.579    0.000    0.912    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py:790(<setcomp>)\n",
      "   142744    0.527    0.000    0.629    0.000 /Users/willlangdale/.pyenv/versions/3.11.8/lib/python3.11/threading.py:1501(enumerate)\n",
      "  1141952    0.333    0.000    0.333    0.000 /Users/willlangdale/.pyenv/versions/3.11.8/lib/python3.11/threading.py:1168(ident)\n",
      "        1    0.001    0.001    0.167    0.167 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:467(repeat)\n",
      "        1    0.000    0.000    0.166    0.166 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:51(_wrapfunc)\n",
      "        1    0.166    0.166    0.166    0.166 {method 'repeat' of 'numpy.ndarray' objects}\n",
      "   570976    0.082    0.000    0.082    0.000 {method 'keys' of 'dict' objects}\n",
      "        1    0.053    0.053    0.073    0.073 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_25768/2801194547.py:36(<listcomp>)\n",
      "   285488    0.053    0.000    0.053    0.000 {method 'values' of 'dict' objects}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x16d1a8190>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "from pstats import SortKey\n",
    "\n",
    "# Profile the function\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "_ = attach_independent_components(table)\n",
    "pr.disable()\n",
    "\n",
    "# Print stats sorted by cumulative time\n",
    "ps = pstats.Stats(pr).sort_stats(SortKey.CUMULATIVE)\n",
    "ps.print_stats(20)  # Show top 20 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process a single component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def combine_integers(*n: int) -> int:\n",
    "    \"\"\"\n",
    "    Combine n integers into a single negative integer.\n",
    "\n",
    "    Used to create a symmetric deterministic hash of two integers that populates the\n",
    "    range of integers efficiently and reduces the likelihood of collisions.\n",
    "\n",
    "    Aims to vectorise amazingly when used in Arrow.\n",
    "\n",
    "    Does this by:\n",
    "\n",
    "    * Using a Mersenne prime as a modulus\n",
    "    * Making negative integers positive with modulo, sped up with bitwise operations\n",
    "    * Combining using symmetric operations with coprime multipliers\n",
    "\n",
    "    Args:\n",
    "        *args: Variable number of integers to combine\n",
    "\n",
    "    Returns:\n",
    "        A negative integer\n",
    "    \"\"\"\n",
    "    P = 2147483647\n",
    "\n",
    "    total = 0\n",
    "    product = 1\n",
    "\n",
    "    for x in sorted(n):\n",
    "        x_pos = (x ^ (x >> 31)) - (x >> 31)\n",
    "        total = (total + x_pos) % P\n",
    "        product = (product * x_pos) % P\n",
    "\n",
    "    result = (31 * total + 37 * product) % P\n",
    "\n",
    "    return -result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def combine_strings(*n: str) -> str:\n",
    "    \"\"\"\n",
    "    Combine n strings into a single string.\n",
    "\n",
    "    Args:\n",
    "        *args: Variable number of strings to combine\n",
    "        \n",
    "    Returns:\n",
    "        A single string\n",
    "    \"\"\"\n",
    "    letters = set(chain.from_iterable(n))\n",
    "    return \"\".join(sorted(list(letters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import TypeVar, Generic, Hashable, Iterator\n",
    "import pandas as pd\n",
    "\n",
    "T = TypeVar('T', bound=Hashable)\n",
    "\n",
    "class UnionFindWithDiff(Generic[T]):\n",
    "    def __init__(self):\n",
    "        self.parent: dict[T, T] = {}\n",
    "        self.rank: dict[T, int] = {}\n",
    "        self._shadow_parent: dict[T, T] = {}\n",
    "        self._shadow_rank: dict[T, int] = {}\n",
    "        self._pending_pairs: list[tuple[T, T]] = []\n",
    "        \n",
    "    def make_set(self, x: T) -> None:\n",
    "        if x not in self.parent:\n",
    "            self.parent[x] = x\n",
    "            self.rank[x] = 0\n",
    "    \n",
    "    def find(self, x: T, parent_dict: dict[T, T] | None = None) -> T:\n",
    "        if parent_dict is None:\n",
    "            parent_dict = self.parent\n",
    "            \n",
    "        if x not in parent_dict:\n",
    "            self.make_set(x)\n",
    "            if parent_dict is self._shadow_parent:\n",
    "                self._shadow_parent[x] = x\n",
    "                self._shadow_rank[x] = 0\n",
    "        \n",
    "        while parent_dict[x] != x:\n",
    "            parent_dict[x] = parent_dict[parent_dict[x]]\n",
    "            x = parent_dict[x]\n",
    "        return x\n",
    "    \n",
    "    def union(self, x: T, y: T) -> None:\n",
    "        root_x = self.find(x)\n",
    "        root_y = self.find(y)\n",
    "        \n",
    "        if root_x != root_y:\n",
    "            self._pending_pairs.append((x, y))\n",
    "            \n",
    "            if self.rank[root_x] < self.rank[root_y]:\n",
    "                root_x, root_y = root_y, root_x\n",
    "            self.parent[root_y] = root_x\n",
    "            if self.rank[root_x] == self.rank[root_y]:\n",
    "                self.rank[root_x] += 1\n",
    "    \n",
    "    def get_component(self, x: T, parent_dict: dict[T, T] | None = None) -> set[T]:\n",
    "        if parent_dict is None:\n",
    "            parent_dict = self.parent\n",
    "        \n",
    "        root = self.find(x, parent_dict)\n",
    "        return {y for y in parent_dict if self.find(y, parent_dict) == root}\n",
    "    \n",
    "    def get_components(self, parent_dict: dict[T, T] | None = None) -> list[set[T]]:\n",
    "        if parent_dict is None:\n",
    "            parent_dict = self.parent\n",
    "            \n",
    "        components = defaultdict(set)\n",
    "        for x in parent_dict:\n",
    "            root = self.find(x, parent_dict)\n",
    "            components[root].add(x)\n",
    "        return list(components.values())\n",
    "    \n",
    "    def diff(self) -> Iterator[tuple[set[T], set[T]]]:\n",
    "        \"\"\"\n",
    "        Returns differences including all pairwise merges that occurred since last diff,\n",
    "        excluding cases where old_comp == new_comp.\n",
    "        \"\"\"\n",
    "        # Get current state before processing pairs\n",
    "        current_components = self.get_components()\n",
    "        reported_pairs = set()\n",
    "        \n",
    "        # Process pending pairs\n",
    "        for x, y in self._pending_pairs:\n",
    "            # Find the final component containing the pair\n",
    "            final_component = next(comp for comp in current_components \n",
    "                                 if x in comp and y in comp)\n",
    "            \n",
    "            # Only report if the pair forms a proper subset of the final component\n",
    "            pair_component = {x, y}\n",
    "            if (pair_component != final_component and \n",
    "                frozenset((frozenset(pair_component), frozenset(final_component))) not in reported_pairs):\n",
    "                reported_pairs.add(frozenset((frozenset(pair_component), frozenset(final_component))))\n",
    "                yield (pair_component, final_component)\n",
    "        \n",
    "        self._pending_pairs.clear()\n",
    "        \n",
    "        # Handle initial state\n",
    "        if not self._shadow_parent:\n",
    "            self._shadow_parent = self.parent.copy()\n",
    "            self._shadow_rank = self.rank.copy()\n",
    "            return\n",
    "        \n",
    "        # Get old components\n",
    "        old_components = self.get_components(self._shadow_parent)\n",
    "        \n",
    "        # Report changes between old and new states\n",
    "        for old_comp in old_components:\n",
    "            if len(old_comp) > 1:  # Only consider non-singleton old components\n",
    "                sample_elem = next(iter(old_comp))\n",
    "                new_comp = next(comp for comp in current_components if sample_elem in comp)\n",
    "                \n",
    "                # Only yield if the components are different and this pair hasn't been reported\n",
    "                if (old_comp != new_comp and \n",
    "                    frozenset((frozenset(old_comp), frozenset(new_comp))) not in reported_pairs):\n",
    "                    reported_pairs.add(frozenset((frozenset(old_comp), frozenset(new_comp))))\n",
    "                    yield (old_comp, new_comp)\n",
    "        \n",
    "        # Update shadow copy\n",
    "        self._shadow_parent = self.parent.copy()\n",
    "        self._shadow_rank = self.rank.copy()\n",
    "\n",
    "\n",
    "def component_to_hierarchy(key: str | int | tuple, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert pairwise probabilities into a hierarchical representation.\n",
    "    Assumes data is pre-sorted by probability descending.\n",
    "    \n",
    "    Args:\n",
    "        key: Group key (ignored in this implementation)\n",
    "        df: DataFrame with columns ['left', 'right', 'probability']\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns ['parent', 'child', 'probability'] representing hierarchical merges\n",
    "    \"\"\"\n",
    "    hierarchy: list[tuple[int, int, float]] = []\n",
    "    uf = UnionFindWithDiff[int]()\n",
    "\n",
    "    for threshold in df[\"probability\"].unique():\n",
    "        current_probs = df[df[\"probability\"] == threshold]\n",
    "\n",
    "        for _, row in current_probs.iterrows():\n",
    "            uf.union(row[\"left\"], row[\"right\"])\n",
    "            parent = combine_integers(row[\"left\"], row[\"right\"])\n",
    "            hierarchy.extend([\n",
    "                (parent, row[\"left\"], threshold),\n",
    "                (parent, row[\"right\"], threshold)\n",
    "            ])\n",
    "\n",
    "        for old_comp, new_comp in uf.diff():\n",
    "            if len(old_comp) > 1:\n",
    "                parent = combine_integers(*new_comp)\n",
    "                child = combine_integers(*old_comp)\n",
    "                hierarchy.extend([\n",
    "                    (parent, child, threshold)\n",
    "                ])\n",
    "            else:\n",
    "                parent = combine_integers(*new_comp)\n",
    "                hierarchy.extend([\n",
    "                    (parent, old_comp.pop(), threshold)\n",
    "                ])\n",
    "\n",
    "    return pd.DataFrame(hierarchy, columns=['parent', 'child', 'probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cc2.to_pandas()\n",
    "cc_15_pd = df[df['component'] == 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         142304 function calls (139675 primitive calls) in 0.136 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 276 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        2    0.000    0.000    0.136    0.068 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3541(run_code)\n",
      "        2    0.000    0.000    0.136    0.068 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    0.136    0.136 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_68435/3996032216.py:1(<module>)\n",
      "        1    0.007    0.007    0.136    0.136 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_68435/807932433.py:115(component_to_hierarchy)\n",
      "      532    0.002    0.000    0.059    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/frame.py:1505(iterrows)\n",
      "      532    0.007    0.000    0.054    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/series.py:389(__init__)\n",
      "     3006    0.007    0.000    0.027    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/series.py:1095(__getitem__)\n",
      "      536    0.004    0.000    0.018    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/construction.py:517(sanitize_array)\n",
      "      298    0.003    0.000    0.017    0.000 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_68435/807932433.py:65(diff)\n",
      "       63    0.000    0.000    0.014    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4062(__getitem__)\n",
      "       61    0.007    0.000    0.013    0.000 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_68435/807932433.py:55(get_components)\n",
      "       31    0.000    0.000    0.012    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4130(_getitem_bool_array)\n",
      "     3006    0.004    0.000    0.012    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/series.py:1220(_get_value)\n",
      "32333/32271    0.006    0.000    0.008    0.000 {built-in method builtins.isinstance}\n",
      "      504    0.005    0.000    0.008    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/dtypes/cast.py:1156(maybe_infer_to_datetimelike)\n",
      "       31    0.000    0.000    0.008    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/ops/common.py:62(new_method)\n",
      "      594    0.002    0.000    0.007    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/generic.py:6301(__setattr__)\n",
      "       31    0.000    0.000    0.007    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/arraylike.py:38(__eq__)\n",
      "       31    0.000    0.000    0.007    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/generic.py:4142(_take_with_is_copy)\n",
      "       31    0.000    0.000    0.007    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/series.py:6110(_cmp_method)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x12219f390>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "from pstats import SortKey\n",
    "\n",
    "# Profile the function\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "_ = component_to_hierarchy(15, cc_15_pd)\n",
    "pr.disable()\n",
    "\n",
    "# Print stats sorted by cumulative time\n",
    "ps = pstats.Stats(pr).sort_stats(SortKey.CUMULATIVE)\n",
    "ps.print_stats(20)  # Show top 20 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "def component_to_hierarchy_pa(table: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Convert pairwise probabilities into a hierarchical representation.\n",
    "    Assumes data is pre-sorted by probability descending.\n",
    "    \n",
    "    Args:\n",
    "        key: Group key (ignored in this implementation)\n",
    "        table: Arrow Table with columns ['left', 'right', 'probability']\n",
    "    \n",
    "    Returns:\n",
    "        Arrow Table with columns ['parent', 'child', 'probability'] representing hierarchical merges\n",
    "    \"\"\"\n",
    "    hierarchy: list[tuple[int, int, float]] = []\n",
    "    uf = UnionFindWithDiff[int]()\n",
    "\n",
    "    # Get unique probabilities\n",
    "    probs = pc.unique(table['probability'])\n",
    "\n",
    "    for threshold in probs:\n",
    "        # Get current probability rows\n",
    "        mask = pc.equal(table['probability'], threshold)\n",
    "        current_probs = table.filter(mask)\n",
    "        threshold_float = float(threshold.as_py())\n",
    "        \n",
    "        # Process each row\n",
    "        for row in zip(\n",
    "            current_probs['left'].to_numpy(),\n",
    "            current_probs['right'].to_numpy()\n",
    "        ):\n",
    "            left, right = row\n",
    "            uf.union(left, right)\n",
    "            parent = combine_integers(left, right)\n",
    "            hierarchy.extend([\n",
    "                (parent, left, threshold_float),\n",
    "                (parent, right, threshold_float)\n",
    "            ])\n",
    "\n",
    "        # Process UnionFind diffs - exact same logic as original\n",
    "        for old_comp, new_comp in uf.diff():\n",
    "            if len(old_comp) > 1:\n",
    "                parent = combine_integers(*new_comp)\n",
    "                child = combine_integers(*old_comp)\n",
    "                hierarchy.extend([\n",
    "                    (parent, child, threshold_float)\n",
    "                ])\n",
    "            else:\n",
    "                parent = combine_integers(*new_comp)\n",
    "                hierarchy.extend([\n",
    "                    (parent, old_comp.pop(), threshold_float)\n",
    "                ])\n",
    "\n",
    "    parents, children, probs = zip(*hierarchy)\n",
    "    return pa.table({\n",
    "        'parent': pa.array(parents, type=pa.int64()),\n",
    "        'child': pa.array(children, type=pa.int64()),\n",
    "        'probability': pa.array(probs, type=pa.float64())\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_15 = filtered_table = cc2.filter(pc.equal(cc2['component'], 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         26701 function calls in 0.034 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 54 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        2    0.000    0.000    0.034    0.017 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3541(run_code)\n",
      "        2    0.000    0.000    0.034    0.017 {built-in method builtins.exec}\n",
      "        1    0.007    0.007    0.034    0.034 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_68435/2661828505.py:5(component_to_hierarchy_pa)\n",
      "      298    0.003    0.000    0.021    0.000 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_68435/807932433.py:65(diff)\n",
      "       61    0.008    0.000    0.017    0.000 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_68435/807932433.py:55(get_components)\n",
      "    11218    0.009    0.000    0.009    0.000 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_68435/807932433.py:20(find)\n",
      "      501    0.001    0.000    0.002    0.000 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_68435/807932433.py:35(union)\n",
      "       31    0.002    0.000    0.002    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/compute.py:249(wrapper)\n",
      "    10483    0.002    0.000    0.002    0.000 {method 'add' of 'set' objects}\n",
      "       32    0.001    0.000    0.001    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/compute.py:239(wrapper)\n",
      "      745    0.000    0.000    0.001    0.000 {built-in method builtins.next}\n",
      "      542    0.000    0.000    0.000    0.000 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_68435/807932433.py:102(<genexpr>)\n",
      "      398    0.000    0.000    0.000    0.000 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_68435/807932433.py:77(<genexpr>)\n",
      "      200    0.000    0.000    0.000    0.000 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_68435/807932433.py:15(make_set)\n",
      "      768    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}\n",
      "       31    0.000    0.000    0.000    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/compute.py:216(_handle_options)\n",
      "      601    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "       62    0.000    0.000    0.000    0.000 {method 'copy' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 /Users/willlangdale/.pyenv/versions/3.11.8/lib/python3.11/codeop.py:120(__call__)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.compile}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x11c183250>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "from pstats import SortKey\n",
    "\n",
    "# Profile the function\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "_ = component_to_hierarchy_pa(cc_15)\n",
    "pr.disable()\n",
    "\n",
    "# Print stats sorted by cumulative time\n",
    "ps = pstats.Stats(pr).sort_stats(SortKey.CUMULATIVE)\n",
    "ps.print_stats(20)  # Show top 20 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running test case 1...\n",
      "✓ Test case 1 passed\n",
      "\n",
      "Running test case 2...\n",
      "✓ Test case 2 passed\n",
      "\n",
      "Running test case 3...\n",
      "✓ Test case 3 passed\n"
     ]
    }
   ],
   "source": [
    "from unittest.mock import patch\n",
    "\n",
    "test_cases = [\n",
    "    # Test case 1: Equal probabilities\n",
    "    (\n",
    "        {\n",
    "            \"left\": [\"a\", \"b\", \"c\"],\n",
    "            \"right\": [\"b\", \"c\", \"d\"],\n",
    "            \"probability\": [1.0, 1.0, 1.0],\n",
    "        },\n",
    "        {\n",
    "            (\"ab\", \"a\", 1.0),\n",
    "            (\"ab\", \"b\", 1.0),\n",
    "            (\"bc\", \"b\", 1.0),\n",
    "            (\"bc\", \"c\", 1.0),\n",
    "            (\"cd\", \"c\", 1.0),\n",
    "            (\"cd\", \"d\", 1.0),\n",
    "            (\"abcd\", \"ab\", 1.0),\n",
    "            (\"abcd\", \"bc\", 1.0),\n",
    "            (\"abcd\", \"cd\", 1.0),\n",
    "        },\n",
    "    ),\n",
    "    # Test case 2: Asymmetric probabilities\n",
    "    (\n",
    "        {\n",
    "            \"left\": [\"w\", \"x\", \"y\"],\n",
    "            \"right\": [\"x\", \"y\", \"z\"],\n",
    "            \"probability\": [0.9, 0.85, 0.8],\n",
    "        },\n",
    "        {\n",
    "            (\"wx\", \"w\", 0.9),\n",
    "            (\"wx\", \"x\", 0.9),\n",
    "            (\"xy\", \"x\", 0.85),\n",
    "            (\"xy\", \"y\", 0.85),\n",
    "            (\"wxy\", \"wx\", 0.85),\n",
    "            (\"wxy\", \"xy\", 0.85),\n",
    "            (\"yz\", \"y\", 0.8),\n",
    "            (\"yz\", \"z\", 0.8),\n",
    "            (\"wxyz\", \"wxy\", 0.8),\n",
    "            (\"wxyz\", \"yz\", 0.8),\n",
    "        },\n",
    "    ),\n",
    "    # Test case 3: Single two-item component\n",
    "    (\n",
    "        {\n",
    "            \"left\": [\"x\"],\n",
    "            \"right\": [\"y\"],\n",
    "            \"probability\": [0.9],\n",
    "        },\n",
    "        {\n",
    "            (\"xy\", \"x\", 0.9),\n",
    "            (\"xy\", \"y\", 0.9),\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "for i, (prob_data, expected_relations) in enumerate(test_cases, 1):\n",
    "    print(f\"\\nRunning test case {i}...\")\n",
    "    \n",
    "    with patch('__main__.combine_integers', side_effect=combine_strings):\n",
    "        # Pandas\n",
    "        # probabilities = (\n",
    "        #     pd.DataFrame.from_dict(prob_data)\n",
    "        #     .assign(probability=lambda df: df['probability'].astype(float))\n",
    "        #     .sort_values(by=\"probability\", ascending=False)\n",
    "        # )\n",
    "        # hierarchy_true = (\n",
    "        #         pd.DataFrame.from_records(\n",
    "        #         list(expected_relations), \n",
    "        #         columns=[\"parent\", \"child\", \"probability\"]\n",
    "        #     )\n",
    "        #     .sort_values(by=[\"probability\", \"parent\", \"child\"], ascending=[False, True, True])\n",
    "        #     .dropna(how='all')\n",
    "        #     .reset_index(drop=True)\n",
    "        # )\n",
    "\n",
    "        # hierarchy = component_to_hierarchy(0, probabilities)\n",
    "\n",
    "        # hierarchy = hierarchy.sort_values(\n",
    "        #     by=[\"probability\", \"parent\", \"child\"],\n",
    "        #     ascending=[False, True, True]\n",
    "        # ).reset_index(drop=True)\n",
    "\n",
    "        # Arrow\n",
    "        probabilities = (\n",
    "            pa.Table.from_pydict(prob_data)\n",
    "            .cast(pa.schema([\n",
    "                ('left', pa.string()),\n",
    "                ('right', pa.string()),\n",
    "                ('probability', pa.float64()),\n",
    "            ]))\n",
    "            .sort_by([('probability', 'descending')])\n",
    "        )\n",
    "\n",
    "        parents, children, probs = zip(*expected_relations)\n",
    "\n",
    "        hierarchy_true = (\n",
    "            pa.table(\n",
    "                [parents, children, probs], \n",
    "                names=['parent', 'child', 'probability']\n",
    "            )\n",
    "            .sort_by([\n",
    "                ('probability', 'descending'),\n",
    "                ('parent', 'ascending'),\n",
    "                ('child', 'ascending')\n",
    "            ])\n",
    "            .filter(pc.is_valid(pc.field('parent')))\n",
    "        )\n",
    "\n",
    "        hierarchy = (\n",
    "            component_to_hierarchy_pa(probabilities)\n",
    "            .sort_by([\n",
    "                ('probability', 'descending'),\n",
    "                ('parent', 'ascending'),\n",
    "                ('child', 'ascending')\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            assert hierarchy.equals(hierarchy_true)\n",
    "            print(f\"✓ Test case {i} passed\")\n",
    "        except AssertionError:\n",
    "            print(f\"✗ Test case {i} failed\")\n",
    "            print(\"\\nExpected DataFrame:\")\n",
    "            print(hierarchy_true)\n",
    "            print(\"\\nActual DataFrame:\")\n",
    "            print(hierarchy)\n",
    "            print(\"\\nDifferences:\")\n",
    "            if hierarchy.shape != hierarchy_true.shape:\n",
    "                print(f\"Shape mismatch: Expected {hierarchy_true.shape}, got {hierarchy.shape}\")\n",
    "            else:\n",
    "                # Show where values differ\n",
    "                differences = (hierarchy != hierarchy_true).any(axis=1)\n",
    "                if differences.any():\n",
    "                    print(\"\\nMismatched rows:\")\n",
    "                    print(\"Expected:\")\n",
    "                    print(hierarchy_true[differences])\n",
    "                    print(\"\\nGot:\")\n",
    "                    print(hierarchy[differences])\n",
    "            # Continue to next test case instead of raising\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all components in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attach_independent_components' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpq\u001b[39;00m\n\u001b[1;32m      3\u001b[0m h2 \u001b[38;5;241m=\u001b[39m pq\u001b[38;5;241m.\u001b[39mread_table(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhierarchical_cc2k.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m cc2 \u001b[38;5;241m=\u001b[39m \u001b[43mattach_independent_components\u001b[49m(h2)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# h200 = pq.read_table('hierarchical_cc200k.parquet')\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# cc200 = attach_independent_components(h200)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'attach_independent_components' is not defined"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "h2 = pq.read_table('hierarchical_cc2k.parquet')\n",
    "cc2 = attach_independent_components(h2)\n",
    "\n",
    "# h200 = pq.read_table('hierarchical_cc200k.parquet')\n",
    "# cc200 = attach_independent_components(h200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from multiprocessing import cpu_count\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "def setup_client(port: int = 8787) -> Client:\n",
    "    \"\"\"\n",
    "    Creates a Dask client optimized for local computation.\n",
    "    Returns the client object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = Client(f'tcp://localhost:{port}')\n",
    "    except Exception:\n",
    "        n_workers = cpu_count()\n",
    "        cluster = LocalCluster(\n",
    "            dashboard_address=f':{port}',\n",
    "            n_workers=n_workers,\n",
    "            threads_per_worker=1,  # One thread per worker for CPU-bound tasks\n",
    "            memory_limit='auto'    # Automatically determine memory limits\n",
    "        )\n",
    "        client = Client(cluster)\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster import component_to_hierarchy\n",
    "\n",
    "def to_hierarchical_clusters(client: Client, table: pa.Table) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform hierarchical clustering on an Arrow table of pairwise probabilities.\n",
    "    \"\"\"\n",
    "    pdf = table.to_pandas()\n",
    "    \n",
    "    ddf = dd.from_pandas(\n",
    "        pdf, \n",
    "        npartitions=len(client.cluster.worker_spec)\n",
    "    )\n",
    "    \n",
    "    result_ddf = ddf.groupby('component').apply(\n",
    "        component_to_hierarchy, \n",
    "        meta={\n",
    "            'parent': int, \n",
    "            'child': int, \n",
    "            'probability': float\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return result_ddf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msetup_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8787\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m client:\n\u001b[1;32m      2\u001b[0m     h_out \u001b[38;5;241m=\u001b[39m to_hierarchical_clusters(client, cc2)\n\u001b[1;32m      4\u001b[0m h_out\n",
      "Cell \u001b[0;32mIn[15], line 13\u001b[0m, in \u001b[0;36msetup_client\u001b[0;34m(port)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mCreates a Dask client optimized for local computation.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03mReturns the client object.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     client \u001b[38;5;241m=\u001b[39m \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtcp://localhost:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mport\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     n_workers \u001b[38;5;241m=\u001b[39m cpu_count()\n",
      "File \u001b[0;32m~/DS/matchbox/.venv/lib/python3.11/site-packages/distributed/client.py:1231\u001b[0m, in \u001b[0;36mClient.__init__\u001b[0;34m(self, address, loop, timeout, set_as_default, scheduler_file, security, asynchronous, name, heartbeat_interval, serializers, deserializers, extensions, direct_to_workers, connection_limit, **kwargs)\u001b[0m\n\u001b[1;32m   1228\u001b[0m preload_argv \u001b[38;5;241m=\u001b[39m dask\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistributed.client.preload-argv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreloads \u001b[38;5;241m=\u001b[39m preloading\u001b[38;5;241m.\u001b[39mprocess_preloads(\u001b[38;5;28mself\u001b[39m, preload, preload_argv)\n\u001b[0;32m-> 1231\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1232\u001b[0m Client\u001b[38;5;241m.\u001b[39m_instances\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecreate_tasks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReplayTaskClient\n",
      "File \u001b[0;32m~/DS/matchbox/.venv/lib/python3.11/site-packages/distributed/client.py:1433\u001b[0m, in \u001b[0;36mClient.start\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1431\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_started \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m   1432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1433\u001b[0m     \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DS/matchbox/.venv/lib/python3.11/site-packages/distributed/utils.py:436\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m e\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[0;32m--> 436\u001b[0m         \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "File \u001b[0;32m~/DS/matchbox/.venv/lib/python3.11/site-packages/distributed/utils.py:425\u001b[0m, in \u001b[0;36msync.<locals>.wait\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 425\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    427\u001b[0m         loop\u001b[38;5;241m.\u001b[39madd_callback(cancel)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with setup_client(port=8787) as client:\n",
    "    h_out = to_hierarchical_clusters(client, cc2)\n",
    "\n",
    "h_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing\n",
    "from cluster import process_component\n",
    "\n",
    "def to_hierarchical_clusters_pa(table: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Parallel processing of components using Arrow.\n",
    "    \"\"\"\n",
    "    components = pc.unique(table['component'])\n",
    "    n_cores = multiprocessing.cpu_count()\n",
    "    results = []\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=n_cores) as executor:\n",
    "        futures = [\n",
    "            executor.submit(process_component, comp, table) \n",
    "            for comp in components\n",
    "        ]\n",
    "        results = [future.result() for future in futures]\n",
    "    \n",
    "    return pa.concat_tables(results) if results else pa.table({\n",
    "        'parent': pa.array([], type=pa.int64()),\n",
    "        'child': pa.array([], type=pa.int64()),\n",
    "        'probability': pa.array([], type=pa.float64())\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "h_out = to_hierarchical_clusters_pa(cc2)\n",
    "\n",
    "h_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
