{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical algorithm optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rustworkx as rx\n",
    "from collections import Counter\n",
    "\n",
    "def verify_components(table) -> dict:\n",
    "    \"\"\"\n",
    "    Fast verification of connected components using rustworkx.\n",
    "    \n",
    "    Args:\n",
    "        table: PyArrow table with 'left', 'right' columns\n",
    "    \n",
    "    Returns:\n",
    "        dictionary containing basic component statistics\n",
    "    \"\"\"\n",
    "    # Create graph directly from arrays\n",
    "    graph = rx.PyDiGraph()\n",
    "    \n",
    "    # Add all unique nodes at once\n",
    "    unique_nodes = set(table['left'].to_numpy()) | set(table['right'].to_numpy())\n",
    "    graph.add_nodes_from(range(len(unique_nodes)))\n",
    "    \n",
    "    # Create node mapping and edges in one pass\n",
    "    node_to_idx = {node: idx for idx, node in enumerate(unique_nodes)}\n",
    "    edges = [(node_to_idx[left], node_to_idx[right], prob) \n",
    "            for left, right, prob in zip(table['left'].to_numpy(), \n",
    "                                       table['right'].to_numpy(),\n",
    "                                       table['probability'].to_numpy())]\n",
    "    \n",
    "    # Add all edges at once\n",
    "    graph.add_edges_from(edges)\n",
    "    \n",
    "    # Get components and their sizes\n",
    "    components = rx.weakly_connected_components(graph)\n",
    "    component_sizes = Counter(len(component) for component in components)\n",
    "    \n",
    "    return {\n",
    "        'num_components': len(components),\n",
    "        'total_nodes': len(unique_nodes),\n",
    "        'total_edges': len(edges),\n",
    "        'component_sizes': component_sizes,\n",
    "        'min_component_size': min(component_sizes.keys()),\n",
    "        'max_component_size': max(component_sizes.keys())\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_max_possible_edges(n_nodes: int, num_components: int) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the maximum possible number of edges given n nodes split into k components.\n",
    "    \n",
    "    Args:\n",
    "        n_nodes: Total number of nodes\n",
    "        num_components: Number of components to split into\n",
    "        \n",
    "    Returns:\n",
    "        Maximum possible number of edges\n",
    "    \"\"\"\n",
    "    nodes_per_component = n_nodes // num_components\n",
    "    max_edges_per_component = nodes_per_component * nodes_per_component  # Complete bipartite graph\n",
    "    return max_edges_per_component * num_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import rustworkx as rx\n",
    "from typing import List, Tuple\n",
    "from decimal import Decimal\n",
    "\n",
    "def split_values_into_components(values: List[int], num_components: int) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Split values into non-overlapping groups for each component.\n",
    "    \n",
    "    Args:\n",
    "        values: List of values to split\n",
    "        num_components: Number of components to create\n",
    "        \n",
    "    Returns:\n",
    "        List of arrays, one for each component\n",
    "    \"\"\"\n",
    "    values = np.array(values)\n",
    "    np.random.shuffle(values)\n",
    "    return np.array_split(values, num_components)\n",
    "\n",
    "\n",
    "def generate_arrow_data(\n",
    "    left_values: List[int],\n",
    "    right_values: List[int],\n",
    "    prob_range: Tuple[float, float],\n",
    "    num_components: int,\n",
    "    total_rows: int\n",
    ") -> pa.Table:\n",
    "    \"\"\"\n",
    "    Generate dummy arrow data with guaranteed isolated components.\n",
    "    \n",
    "    Args:\n",
    "        left_values: List of integers to use for left column\n",
    "        right_values: List of integers to use for right column\n",
    "        prob_range: Tuple of (min_prob, max_prob) to constrain probabilities\n",
    "        num_components: Number of distinct connected components to generate\n",
    "        total_rows: Total number of rows to generate\n",
    "    \n",
    "    Returns:\n",
    "        PyArrow Table with 'left', 'right', and 'probability' columns\n",
    "    \"\"\"\n",
    "    if len(left_values) < 2 or len(right_values) < 2:\n",
    "        raise ValueError(\"Need at least 2 possible values for both left and right\")\n",
    "    if num_components > min(len(left_values), len(right_values)):\n",
    "        raise ValueError(\"Cannot have more components than minimum of left/right values\")\n",
    "    \n",
    "    # Calculate maximum possible edges\n",
    "    min_nodes = min(len(left_values), len(right_values))\n",
    "    max_possible_edges = calculate_max_possible_edges(min_nodes, num_components)\n",
    "    \n",
    "    if total_rows > max_possible_edges:\n",
    "        raise ValueError(\n",
    "            f\"Cannot generate {total_rows:,} edges with {num_components:,} components. \"\n",
    "            f\"Maximum possible edges is {max_possible_edges:,} given {min_nodes:,} nodes. \"\n",
    "            \"Either increase the number of nodes, decrease the number of components, \"\n",
    "            \"or decrease the total edges requested.\"\n",
    "        )\n",
    "    \n",
    "    # Convert probability range to integers (60-80 for 0.60-0.80)\n",
    "    prob_min = int(prob_range[0] * 100)\n",
    "    prob_max = int(prob_range[1] * 100)\n",
    "    \n",
    "    # Split values into completely separate groups for each component\n",
    "    left_components = split_values_into_components(left_values, num_components)\n",
    "    right_components = split_values_into_components(right_values, num_components)\n",
    "    \n",
    "    # Calculate base number of edges per component\n",
    "    base_edges_per_component = total_rows // num_components\n",
    "    remaining_edges = total_rows % num_components\n",
    "    \n",
    "    all_edges = []\n",
    "    \n",
    "    # Generate edges for each component\n",
    "    for comp_idx in range(num_components):\n",
    "        comp_left_values = left_components[comp_idx]\n",
    "        comp_right_values = right_components[comp_idx]\n",
    "        \n",
    "        # Calculate edges for this component\n",
    "        edges_in_component = base_edges_per_component\n",
    "        if comp_idx < remaining_edges:  # Distribute remaining edges\n",
    "            edges_in_component += 1\n",
    "            \n",
    "        # Ensure basic connectivity within the component\n",
    "        base_edges = []\n",
    "        \n",
    "        # Create a spanning tree-like structure\n",
    "        for i in range(len(comp_left_values)):\n",
    "            base_edges.append((\n",
    "                comp_left_values[i],\n",
    "                comp_right_values[i % len(comp_right_values)],\n",
    "                np.random.randint(prob_min, prob_max + 1)\n",
    "            ))\n",
    "        \n",
    "        # Generate remaining random edges strictly within this component\n",
    "        remaining_edges = edges_in_component - len(base_edges)\n",
    "        if remaining_edges > 0:\n",
    "            random_lefts = np.random.choice(comp_left_values, size=remaining_edges)\n",
    "            random_rights = np.random.choice(comp_right_values, size=remaining_edges)\n",
    "            random_probs = np.random.randint(prob_min, prob_max + 1, size=remaining_edges)\n",
    "            \n",
    "            component_edges = base_edges + list(zip(random_lefts, random_rights, random_probs))\n",
    "        else:\n",
    "            component_edges = base_edges\n",
    "            \n",
    "        all_edges.extend(component_edges)\n",
    "    \n",
    "    # Convert to arrays\n",
    "    lefts, rights, probs = zip(*all_edges)\n",
    "    \n",
    "    # Create PyArrow arrays\n",
    "    left_array = pa.array(lefts, type=pa.int64())\n",
    "    right_array = pa.array(rights, type=pa.int64())\n",
    "    decimal_probs = [Decimal(str(p/100)) for p in probs]\n",
    "    prob_array = pa.array(decimal_probs, type=pa.decimal128(precision=3, scale=2))\n",
    "    \n",
    "    return pa.table([left_array, right_array, prob_array],\n",
    "                   names=['left', 'right', 'probability'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components found: 10\n",
      "Total nodes: 20000\n",
      "Total edges: 1000009\n",
      "\n",
      "Component sizes:\n",
      "Size 2000: 10 components\n"
     ]
    }
   ],
   "source": [
    "left_values = list(range(10_000))\n",
    "right_values = list(range(10_000, 20_000))\n",
    "prob_range = (0.6, 0.8)\n",
    "num_components = 10\n",
    "total_rows = 1_000_000\n",
    "\n",
    "table = generate_arrow_data(\n",
    "    left_values=left_values,\n",
    "    right_values=right_values,\n",
    "    prob_range=prob_range,\n",
    "    num_components=num_components,\n",
    "    total_rows=total_rows\n",
    ")\n",
    "\n",
    "results = verify_components(table)\n",
    "print(f\"Number of components found: {results['num_components']}\")\n",
    "print(f\"Total nodes: {results['total_nodes']}\")\n",
    "print(f\"Total edges: {results['total_edges']}\")\n",
    "print(\"\\nComponent sizes:\")\n",
    "for size, count in sorted(results['component_sizes'].items()):\n",
    "    print(f\"Size {size}: {count} components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_values = list(range(int(2e7)))\n",
    "right_values = list(range(int(2e7), int(4e7)))\n",
    "prob_range = (0.7, 1.0)\n",
    "num_components = 200_000\n",
    "total_rows = int(1e8)\n",
    "\n",
    "table = generate_arrow_data(\n",
    "    left_values=left_values,\n",
    "    right_values=right_values,\n",
    "    prob_range=prob_range,\n",
    "    num_components=num_components,\n",
    "    total_rows=total_rows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components found: 206763\n",
      "Total nodes: 40000000\n",
      "Total edges: 100000400\n",
      "\n",
      "Component sizes:\n",
      "Size 2: 6755 components\n",
      "Size 4: 8 components\n",
      "Size 194: 1 components\n",
      "Size 196: 124 components\n",
      "Size 198: 6520 components\n",
      "Size 200: 193355 components\n"
     ]
    }
   ],
   "source": [
    "results = verify_components(table)\n",
    "print(f\"Number of components found: {results['num_components']}\")\n",
    "print(f\"Total nodes: {results['total_nodes']}\")\n",
    "print(f\"Total edges: {results['total_edges']}\")\n",
    "print(\"\\nComponent sizes:\")\n",
    "for size, count in sorted(results['component_sizes'].items()):\n",
    "    print(f\"Size {size}: {count} components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of components found: 206763\n",
    "* Total nodes: 40000000\n",
    "* Total edges: 100000400\n",
    "\n",
    "Component sizes:\n",
    "* Size 2: 6755 components\n",
    "* Size 4: 8 components\n",
    "* Size 194: 1 components\n",
    "* Size 196: 124 components\n",
    "* Size 198: 6520 components\n",
    "* Size 200: 193355 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "pq.write_table(table, Path.cwd() / 'hierarchical_cc200k.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_values = list(range(int(2e5)))\n",
    "right_values = list(range(int(2e5), int(4e5)))\n",
    "prob_range = (0.7, 1.0)\n",
    "num_components = 2_000\n",
    "total_rows = int(1e6)\n",
    "\n",
    "table2 = generate_arrow_data(\n",
    "    left_values=left_values,\n",
    "    right_values=right_values,\n",
    "    prob_range=prob_range,\n",
    "    num_components=num_components,\n",
    "    total_rows=total_rows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components found: 2,080\n",
      "Total nodes: 400,000\n",
      "Total edges: 1,000,400\n",
      "\n",
      "Component sizes:\n",
      "Size 2: 80 components\n",
      "Size 196: 1 components\n",
      "Size 198: 78 components\n",
      "Size 200: 1,921 components\n"
     ]
    }
   ],
   "source": [
    "results2 = verify_components(table2)\n",
    "print(f\"Number of components found: {results2['num_components']:,}\")\n",
    "print(f\"Total nodes: {results2['total_nodes']:,}\")\n",
    "print(f\"Total edges: {results2['total_edges']:,}\")\n",
    "print(\"\\nComponent sizes:\")\n",
    "for size, count in sorted(results2['component_sizes'].items()):\n",
    "    print(f\"Size {size:,}: {count:,} components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of components found: 2,080\n",
    "* Total nodes: 400,000\n",
    "* Total edges: 1,000,400\n",
    "\n",
    "Component sizes:\n",
    "* Size 2: 80 components\n",
    "* Size 196: 1 components\n",
    "* Size 198: 78 components\n",
    "* Size 200: 1,921 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "pq.write_table(table2, Path.cwd() / 'hierarchical_cc2k.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "h2 = pq.read_table('hierarchical_cc2k.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "left: int64\n",
       "right: int64\n",
       "probability: decimal128(3, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plan.\n",
    "\n",
    "* Find components and their sizes at lowest threshold (rustworkx)\n",
    "* Use this to dask.groupby the data for parallel per-component processing\n",
    "* Ensure we implement early stopping!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2080,\n",
       " pyarrow.Table\n",
       " left: int64\n",
       " right: int64\n",
       " probability: decimal128(3, 2)\n",
       " component: int64\n",
       " ----\n",
       " left: [[197413,116407,114551,160857,6412,...,39429,156175,197197,48177,121674]]\n",
       " right: [[326505,384163,344025,248700,258884,...,311452,278755,204144,357956,378111]]\n",
       " probability: [[1.00,1.00,1.00,1.00,1.00,...,0.70,0.70,0.70,0.70,0.70]]\n",
       " component: [[0,0,0,0,0,...,2079,2079,2079,2079,2079]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import rustworkx as rx\n",
    "\n",
    "def attach_independent_components(table: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Returns the original table with an additional 'component' column indicating\n",
    "    which connected component each edge belongs to.\n",
    "    \"\"\"\n",
    "    # Create dictionary array from sorted unique values\n",
    "    unique = pc.unique(\n",
    "        pa.concat_arrays([\n",
    "            table['left'].combine_chunks(),\n",
    "            table['right'].combine_chunks()\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    # Get indices into unique array for graph construction\n",
    "    left_indices = pc.index_in(table['left'], unique)\n",
    "    right_indices = pc.index_in(table['right'], unique)\n",
    "    \n",
    "    # Create and process graph\n",
    "    n_nodes = len(unique)\n",
    "    n_edges = len(table)\n",
    "    \n",
    "    graph = rx.PyGraph(\n",
    "        node_count_hint=n_nodes,\n",
    "        edge_count_hint=n_edges\n",
    "    )\n",
    "    graph.add_nodes_from(range(n_nodes))\n",
    "\n",
    "    edges = tuple(zip(left_indices.to_numpy(), right_indices.to_numpy()))\n",
    "    graph.add_edges_from_no_data(edges)\n",
    "    \n",
    "    # Get components and create mapping array\n",
    "    components = rx.connected_components(graph)\n",
    "    \n",
    "    # Convert components to numpy arrays\n",
    "    component_indices = np.concatenate([np.array(list(c)) for c in components])\n",
    "    component_labels = np.repeat(np.arange(len(components)), [len(c) for c in components])\n",
    "    \n",
    "    # Create mapping array and fill with component labels\n",
    "    node_to_component = np.zeros(len(unique), dtype=np.int64)\n",
    "    node_to_component[component_indices] = component_labels\n",
    "    \n",
    "    # Use the indices we already have to map back to components  \n",
    "    edge_components = pa.array(node_to_component[left_indices.to_numpy()])\n",
    "    \n",
    "    return table.append_column('component', edge_components).sort_by(\n",
    "        [('component', 'ascending'), ('probability', 'descending')]\n",
    "    )\n",
    "\n",
    "cc = attach_independent_components(table2)\n",
    "len(pc.unique(cc.column('component'))), cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         29986 function calls in 0.775 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 35 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        2    0.000    0.000    0.874    0.437 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3541(run_code)\n",
      "        2    0.000    0.000    0.874    0.437 {built-in method builtins.exec}\n",
      "        1    0.373    0.373    0.775    0.775 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_25768/4265471472.py:6(find_independent_components)\n",
      "        1    0.102    0.102    0.103    0.103 {connected_components}\n",
      "        1    0.097    0.097    0.097    0.097 {method 'add_edges_from_no_data' of 'rustworkx.PyGraph' objects}\n",
      "        2    0.090    0.045    0.090    0.045 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/compute.py:249(wrapper)\n",
      "        1    0.074    0.074    0.074    0.074 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/compute.py:239(wrapper)\n",
      "     2850    0.009    0.000    0.025    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py:775(_clean_thread_parent_frames)\n",
      "        1    0.013    0.013    0.013    0.013 {method 'add_nodes_from' of 'rustworkx.PyGraph' objects}\n",
      "     1425    0.006    0.000    0.009    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py:790(<setcomp>)\n",
      "     1425    0.005    0.000    0.006    0.000 /Users/willlangdale/.pyenv/versions/3.11.8/lib/python3.11/threading.py:1501(enumerate)\n",
      "    11400    0.003    0.000    0.003    0.000 /Users/willlangdale/.pyenv/versions/3.11.8/lib/python3.11/threading.py:1168(ident)\n",
      "     5700    0.001    0.000    0.001    0.000 {method 'keys' of 'dict' objects}\n",
      "     2850    0.001    0.000    0.001    0.000 {method 'values' of 'dict' objects}\n",
      "     1425    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}\n",
      "     2853    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "        2    0.000    0.000    0.000    0.000 /Users/willlangdale/.pyenv/versions/3.11.8/lib/python3.11/codeop.py:120(__call__)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.compile}\n",
      "        2    0.000    0.000    0.000    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/compute.py:216(_handle_options)\n",
      "        2    0.000    0.000    0.000    0.000 /Users/willlangdale/.pyenv/versions/3.11.8/lib/python3.11/contextlib.py:299(helper)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x15e22c1d0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "from pstats import SortKey\n",
    "\n",
    "# Profile the function\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "components = attach_independent_components(table2)\n",
    "pr.disable()\n",
    "\n",
    "# Print stats sorted by cumulative time\n",
    "ps = pstats.Stats(pr).sort_stats(SortKey.CUMULATIVE)\n",
    "ps.print_stats(20)  # Show top 20 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         3411228 function calls in 121.198 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 48 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        2    0.000    0.000  139.163   69.581 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3541(run_code)\n",
      "        2    0.001    0.000  139.162   69.581 {built-in method builtins.exec}\n",
      "        1   47.532   47.532  121.195  121.195 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_25768/2801194547.py:1(find_independent_components)\n",
      "        1   24.123   24.123   24.123   24.123 {method 'add_edges_from_no_data' of 'rustworkx.PyGraph' objects}\n",
      "        1   18.713   18.713   18.731   18.731 {connected_components}\n",
      "        2   14.375    7.187   14.375    7.187 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/compute.py:249(wrapper)\n",
      "        1    7.661    7.661    7.661    7.661 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/compute.py:239(wrapper)\n",
      "        1    1.303    1.303    4.041    4.041 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_25768/2801194547.py:35(<listcomp>)\n",
      "   206763    2.738    0.000    2.738    0.000 {built-in method numpy.array}\n",
      "   285488    0.966    0.000    2.629    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py:775(_clean_thread_parent_frames)\n",
      "        1    1.879    1.879    1.879    1.879 {method 'add_nodes_from' of 'rustworkx.PyGraph' objects}\n",
      "   142744    0.579    0.000    0.912    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py:790(<setcomp>)\n",
      "   142744    0.527    0.000    0.629    0.000 /Users/willlangdale/.pyenv/versions/3.11.8/lib/python3.11/threading.py:1501(enumerate)\n",
      "  1141952    0.333    0.000    0.333    0.000 /Users/willlangdale/.pyenv/versions/3.11.8/lib/python3.11/threading.py:1168(ident)\n",
      "        1    0.001    0.001    0.167    0.167 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:467(repeat)\n",
      "        1    0.000    0.000    0.166    0.166 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:51(_wrapfunc)\n",
      "        1    0.166    0.166    0.166    0.166 {method 'repeat' of 'numpy.ndarray' objects}\n",
      "   570976    0.082    0.000    0.082    0.000 {method 'keys' of 'dict' objects}\n",
      "        1    0.053    0.053    0.073    0.073 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_25768/2801194547.py:36(<listcomp>)\n",
      "   285488    0.053    0.000    0.053    0.000 {method 'values' of 'dict' objects}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x16d1a8190>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "from pstats import SortKey\n",
    "\n",
    "# Profile the function\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "_ = attach_independent_components(table)\n",
    "pr.disable()\n",
    "\n",
    "# Print stats sorted by cumulative time\n",
    "ps = pstats.Stats(pr).sort_stats(SortKey.CUMULATIVE)\n",
    "ps.print_stats(20)  # Show top 20 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process a single component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def combine_integers(*n: int) -> int:\n",
    "    \"\"\"\n",
    "    Combine n integers into a single negative integer.\n",
    "\n",
    "    Used to create a symmetric deterministic hash of two integers that populates the\n",
    "    range of integers efficiently and reduces the likelihood of collisions.\n",
    "\n",
    "    Aims to vectorise amazingly when used in Arrow.\n",
    "\n",
    "    Does this by:\n",
    "\n",
    "    * Using a Mersenne prime as a modulus\n",
    "    * Making negative integers positive with modulo, sped up with bitwise operations\n",
    "    * Combining using symmetric operations with coprime multipliers\n",
    "\n",
    "    Args:\n",
    "        *args: Variable number of integers to combine\n",
    "\n",
    "    Returns:\n",
    "        A negative integer\n",
    "    \"\"\"\n",
    "    P = 2147483647\n",
    "\n",
    "    total = 0\n",
    "    product = 1\n",
    "\n",
    "    for x in sorted(n):\n",
    "        x_pos = (x ^ (x >> 31)) - (x >> 31)\n",
    "        total = (total + x_pos) % P\n",
    "        product = (product * x_pos) % P\n",
    "\n",
    "    result = (31 * total + 37 * product) % P\n",
    "\n",
    "    return -result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def combine_strings(*n: str) -> str:\n",
    "    \"\"\"\n",
    "    Combine n strings into a single string.\n",
    "\n",
    "    Args:\n",
    "        *args: Variable number of strings to combine\n",
    "        \n",
    "    Returns:\n",
    "        A single string\n",
    "    \"\"\"\n",
    "    letters = set(chain.from_iterable(n))\n",
    "    return \"\".join(sorted(list(letters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "      <th>probability</th>\n",
       "      <th>component</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7013</th>\n",
       "      <td>81762</td>\n",
       "      <td>389632</td>\n",
       "      <td>1.00</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7014</th>\n",
       "      <td>5063</td>\n",
       "      <td>289249</td>\n",
       "      <td>1.00</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7015</th>\n",
       "      <td>84647</td>\n",
       "      <td>212390</td>\n",
       "      <td>1.00</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7016</th>\n",
       "      <td>175160</td>\n",
       "      <td>262073</td>\n",
       "      <td>1.00</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7017</th>\n",
       "      <td>178519</td>\n",
       "      <td>348267</td>\n",
       "      <td>1.00</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7509</th>\n",
       "      <td>12792</td>\n",
       "      <td>227750</td>\n",
       "      <td>0.70</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7510</th>\n",
       "      <td>71547</td>\n",
       "      <td>331214</td>\n",
       "      <td>0.70</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7511</th>\n",
       "      <td>66430</td>\n",
       "      <td>329216</td>\n",
       "      <td>0.70</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7512</th>\n",
       "      <td>114817</td>\n",
       "      <td>358133</td>\n",
       "      <td>0.70</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7513</th>\n",
       "      <td>88893</td>\n",
       "      <td>202314</td>\n",
       "      <td>0.70</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>501 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        left   right probability  component\n",
       "7013   81762  389632        1.00         15\n",
       "7014    5063  289249        1.00         15\n",
       "7015   84647  212390        1.00         15\n",
       "7016  175160  262073        1.00         15\n",
       "7017  178519  348267        1.00         15\n",
       "...      ...     ...         ...        ...\n",
       "7509   12792  227750        0.70         15\n",
       "7510   71547  331214        0.70         15\n",
       "7511   66430  329216        0.70         15\n",
       "7512  114817  358133        0.70         15\n",
       "7513   88893  202314        0.70         15\n",
       "\n",
       "[501 rows x 4 columns]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc\n",
    "# Convert the Arrow table to a Pandas DataFrame\n",
    "df = cc.to_pandas()\n",
    "\n",
    "# Filter the DataFrame where component is 15\n",
    "cc_15 = df[df['component'] == 15]\n",
    "cc_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import TypeVar, Generic, Hashable, Iterator\n",
    "import pandas as pd\n",
    "\n",
    "T = TypeVar('T', bound=Hashable)\n",
    "\n",
    "class UnionFindWithDiff(Generic[T]):\n",
    "    def __init__(self):\n",
    "        self.parent: dict[T, T] = {}\n",
    "        self.rank: dict[T, int] = {}\n",
    "        self._shadow_parent: dict[T, T] = {}\n",
    "        self._shadow_rank: dict[T, int] = {}\n",
    "        self._pending_pairs: list[tuple[T, T]] = []\n",
    "        \n",
    "    def make_set(self, x: T) -> None:\n",
    "        if x not in self.parent:\n",
    "            self.parent[x] = x\n",
    "            self.rank[x] = 0\n",
    "    \n",
    "    def find(self, x: T, parent_dict: dict[T, T] | None = None) -> T:\n",
    "        if parent_dict is None:\n",
    "            parent_dict = self.parent\n",
    "            \n",
    "        if x not in parent_dict:\n",
    "            self.make_set(x)\n",
    "            if parent_dict is self._shadow_parent:\n",
    "                self._shadow_parent[x] = x\n",
    "                self._shadow_rank[x] = 0\n",
    "        \n",
    "        while parent_dict[x] != x:\n",
    "            parent_dict[x] = parent_dict[parent_dict[x]]\n",
    "            x = parent_dict[x]\n",
    "        return x\n",
    "    \n",
    "    def union(self, x: T, y: T) -> None:\n",
    "        root_x = self.find(x)\n",
    "        root_y = self.find(y)\n",
    "        \n",
    "        if root_x != root_y:\n",
    "            self._pending_pairs.append((x, y))\n",
    "            \n",
    "            if self.rank[root_x] < self.rank[root_y]:\n",
    "                root_x, root_y = root_y, root_x\n",
    "            self.parent[root_y] = root_x\n",
    "            if self.rank[root_x] == self.rank[root_y]:\n",
    "                self.rank[root_x] += 1\n",
    "    \n",
    "    def get_component(self, x: T, parent_dict: dict[T, T] | None = None) -> set[T]:\n",
    "        if parent_dict is None:\n",
    "            parent_dict = self.parent\n",
    "        \n",
    "        root = self.find(x, parent_dict)\n",
    "        return {y for y in parent_dict if self.find(y, parent_dict) == root}\n",
    "    \n",
    "    def get_components(self, parent_dict: dict[T, T] | None = None) -> list[set[T]]:\n",
    "        if parent_dict is None:\n",
    "            parent_dict = self.parent\n",
    "            \n",
    "        components = defaultdict(set)\n",
    "        for x in parent_dict:\n",
    "            root = self.find(x, parent_dict)\n",
    "            components[root].add(x)\n",
    "        return list(components.values())\n",
    "    \n",
    "    def diff(self) -> Iterator[Tuple[set[T], set[T]]]:\n",
    "        \"\"\"\n",
    "        Returns differences including all pairwise merges that occurred since last diff,\n",
    "        excluding cases where old_comp == new_comp.\n",
    "        \"\"\"\n",
    "        # Get current state before processing pairs\n",
    "        current_components = self.get_components()\n",
    "        reported_pairs = set()\n",
    "        \n",
    "        # Process pending pairs\n",
    "        for x, y in self._pending_pairs:\n",
    "            # Find the final component containing the pair\n",
    "            final_component = next(comp for comp in current_components \n",
    "                                 if x in comp and y in comp)\n",
    "            \n",
    "            # Only report if the pair forms a proper subset of the final component\n",
    "            pair_component = {x, y}\n",
    "            if (pair_component != final_component and \n",
    "                frozenset((frozenset(pair_component), frozenset(final_component))) not in reported_pairs):\n",
    "                reported_pairs.add(frozenset((frozenset(pair_component), frozenset(final_component))))\n",
    "                yield (pair_component, final_component)\n",
    "        \n",
    "        self._pending_pairs.clear()\n",
    "        \n",
    "        # Handle initial state\n",
    "        if not self._shadow_parent:\n",
    "            self._shadow_parent = self.parent.copy()\n",
    "            self._shadow_rank = self.rank.copy()\n",
    "            return\n",
    "        \n",
    "        # Get old components\n",
    "        old_components = self.get_components(self._shadow_parent)\n",
    "        \n",
    "        # Report changes between old and new states\n",
    "        for old_comp in old_components:\n",
    "            if len(old_comp) > 1:  # Only consider non-singleton old components\n",
    "                sample_elem = next(iter(old_comp))\n",
    "                new_comp = next(comp for comp in current_components if sample_elem in comp)\n",
    "                \n",
    "                # Only yield if the components are different and this pair hasn't been reported\n",
    "                if (old_comp != new_comp and \n",
    "                    frozenset((frozenset(old_comp), frozenset(new_comp))) not in reported_pairs):\n",
    "                    reported_pairs.add(frozenset((frozenset(old_comp), frozenset(new_comp))))\n",
    "                    yield (old_comp, new_comp)\n",
    "        \n",
    "        # Update shadow copy\n",
    "        self._shadow_parent = self.parent.copy()\n",
    "        self._shadow_rank = self.rank.copy()\n",
    "\n",
    "\n",
    "def component_to_hierarchy(key: str | int | tuple, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert pairwise probabilities into a hierarchical representation.\n",
    "    Assumes data is pre-sorted by probability descending.\n",
    "    \n",
    "    Args:\n",
    "        key: Group key (ignored in this implementation)\n",
    "        df: DataFrame with columns ['left', 'right', 'probability']\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns ['parent', 'child', 'probability'] representing hierarchical merges\n",
    "    \"\"\"\n",
    "    hierarchy: list[tuple[int, int, float]] = []\n",
    "    uf = UnionFindWithDiff[int]()\n",
    "\n",
    "    for threshold in df[\"probability\"].unique():\n",
    "        current_probs = df[df[\"probability\"] == threshold]\n",
    "\n",
    "        for _, row in current_probs.iterrows():\n",
    "            uf.union(row[\"left\"], row[\"right\"])\n",
    "            parent = combine_integers(row[\"left\"], row[\"right\"])\n",
    "            hierarchy.extend([\n",
    "                (parent, row[\"left\"], threshold),\n",
    "                (parent, row[\"right\"], threshold)\n",
    "            ])\n",
    "\n",
    "        for old_comp, new_comp in uf.diff():\n",
    "            if len(old_comp) > 1:\n",
    "                parent = combine_integers(*new_comp)\n",
    "                child = combine_integers(*old_comp)\n",
    "                hierarchy.extend([\n",
    "                    (parent, child, threshold)\n",
    "                ])\n",
    "            else:\n",
    "                parent = combine_integers(*new_comp)\n",
    "                hierarchy.extend([\n",
    "                    (parent, old_comp.pop(), threshold)\n",
    "                ])\n",
    "\n",
    "    return pd.DataFrame(hierarchy, columns=['parent', 'child', 'probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         142325 function calls (139696 primitive calls) in 0.130 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 281 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        2    0.000    0.000    0.130    0.065 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3541(run_code)\n",
      "        2    0.000    0.000    0.130    0.065 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    0.130    0.130 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_25768/2520359126.py:1(<module>)\n",
      "        1    0.006    0.006    0.130    0.130 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_25768/1157819543.py:115(component_to_hierarchy)\n",
      "      532    0.002    0.000    0.058    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/frame.py:1505(iterrows)\n",
      "      532    0.007    0.000    0.052    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/series.py:389(__init__)\n",
      "     3006    0.007    0.000    0.026    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/series.py:1095(__getitem__)\n",
      "      536    0.004    0.000    0.018    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/construction.py:517(sanitize_array)\n",
      "      298    0.002    0.000    0.016    0.000 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_25768/1157819543.py:65(diff)\n",
      "       63    0.000    0.000    0.014    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4062(__getitem__)\n",
      "       61    0.006    0.000    0.013    0.000 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_25768/1157819543.py:55(get_components)\n",
      "       31    0.000    0.000    0.012    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4130(_getitem_bool_array)\n",
      "     3006    0.004    0.000    0.011    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/series.py:1220(_get_value)\n",
      "32335/32273    0.006    0.000    0.008    0.000 {built-in method builtins.isinstance}\n",
      "      504    0.005    0.000    0.008    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/dtypes/cast.py:1156(maybe_infer_to_datetimelike)\n",
      "       31    0.000    0.000    0.007    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/ops/common.py:62(new_method)\n",
      "      594    0.002    0.000    0.007    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/generic.py:6301(__setattr__)\n",
      "       31    0.000    0.000    0.007    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/arraylike.py:38(__eq__)\n",
      "       31    0.000    0.000    0.007    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/series.py:6110(_cmp_method)\n",
      "      532    0.002    0.000    0.007    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/internals/managers.py:1863(from_array)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x162b58110>"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "from pstats import SortKey\n",
    "\n",
    "# Profile the function\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "_ = component_to_hierarchy(0, cc_15)\n",
    "pr.disable()\n",
    "\n",
    "# Print stats sorted by cumulative time\n",
    "ps = pstats.Stats(pr).sort_stats(SortKey.CUMULATIVE)\n",
    "ps.print_stats(20)  # Show top 20 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running test case 1...\n",
      "✓ Test case 1 passed\n",
      "\n",
      "Running test case 2...\n",
      "✓ Test case 2 passed\n",
      "\n",
      "Running test case 3...\n",
      "✓ Test case 3 passed\n",
      "\n",
      "Running test case 4...\n",
      "✓ Test case 4 passed\n"
     ]
    }
   ],
   "source": [
    "from unittest.mock import patch\n",
    "\n",
    "test_cases = [\n",
    "    # Test case 1: Equal probabilities\n",
    "    (\n",
    "        {\n",
    "            \"left\": [\"a\", \"b\", \"c\"],\n",
    "            \"right\": [\"b\", \"c\", \"d\"],\n",
    "            \"probability\": [1.0, 1.0, 1.0],\n",
    "        },\n",
    "        {\n",
    "            (\"ab\", \"a\", 1.0),\n",
    "            (\"ab\", \"b\", 1.0),\n",
    "            (\"bc\", \"b\", 1.0),\n",
    "            (\"bc\", \"c\", 1.0),\n",
    "            (\"cd\", \"c\", 1.0),\n",
    "            (\"cd\", \"d\", 1.0),\n",
    "            (\"abcd\", \"ab\", 1.0),\n",
    "            (\"abcd\", \"bc\", 1.0),\n",
    "            (\"abcd\", \"cd\", 1.0),\n",
    "        },\n",
    "    ),\n",
    "    # Test case 2: Asymmetric probabilities\n",
    "    (\n",
    "        {\n",
    "            \"left\": [\"w\", \"x\", \"y\"],\n",
    "            \"right\": [\"x\", \"y\", \"z\"],\n",
    "            \"probability\": [0.9, 0.85, 0.8],\n",
    "        },\n",
    "        {\n",
    "            (\"wx\", \"w\", 0.9),\n",
    "            (\"wx\", \"x\", 0.9),\n",
    "            (\"xy\", \"x\", 0.85),\n",
    "            (\"xy\", \"y\", 0.85),\n",
    "            (\"wxy\", \"wx\", 0.85),\n",
    "            (\"wxy\", \"xy\", 0.85),\n",
    "            (\"yz\", \"y\", 0.8),\n",
    "            (\"yz\", \"z\", 0.8),\n",
    "            (\"wxyz\", \"wxy\", 0.8),\n",
    "            (\"wxyz\", \"yz\", 0.8),\n",
    "        },\n",
    "    ),\n",
    "    # Test case 3: Empty input\n",
    "    (\n",
    "        {\n",
    "            \"left\": [],\n",
    "            \"right\": [],\n",
    "            \"probability\": [],\n",
    "        },\n",
    "        {\n",
    "            (None, None, None)\n",
    "        },\n",
    "    ),\n",
    "    # Test case 4: Single two-item component\n",
    "    (\n",
    "        {\n",
    "            \"left\": [\"x\"],\n",
    "            \"right\": [\"y\"],\n",
    "            \"probability\": [0.9],\n",
    "        },\n",
    "        {\n",
    "            (\"xy\", \"x\", 0.9),\n",
    "            (\"xy\", \"y\", 0.9),\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "for i, (prob_data, expected_relations) in enumerate(test_cases, 1):\n",
    "    print(f\"\\nRunning test case {i}...\")\n",
    "    \n",
    "    with patch('__main__.combine_integers', side_effect=combine_strings):\n",
    "        probabilities = (\n",
    "            pd.DataFrame.from_dict(prob_data)\n",
    "            .assign(probability=lambda df: df['probability'].astype(float))\n",
    "            .sort_values(by=\"probability\", ascending=False)\n",
    "        )\n",
    "        hierarchy_true = (\n",
    "                pd.DataFrame.from_records(\n",
    "                list(expected_relations), \n",
    "                columns=[\"parent\", \"child\", \"probability\"]\n",
    "            )\n",
    "            .sort_values(by=[\"probability\", \"parent\", \"child\"], ascending=[False, True, True])\n",
    "            .dropna(how='all')\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        hierarchy = component_to_hierarchy(0, probabilities)\n",
    "\n",
    "        hierarchy = hierarchy.sort_values(\n",
    "            by=[\"probability\", \"parent\", \"child\"],\n",
    "            ascending=[False, True, True]\n",
    "        ).reset_index(drop=True)\n",
    "        \n",
    "        try:\n",
    "            assert hierarchy.equals(hierarchy_true)\n",
    "            print(f\"✓ Test case {i} passed\")\n",
    "        except AssertionError:\n",
    "            print(f\"✗ Test case {i} failed\")\n",
    "            print(\"\\nExpected DataFrame:\")\n",
    "            print(hierarchy_true)\n",
    "            print(\"\\nActual DataFrame:\")\n",
    "            print(hierarchy)\n",
    "            print(\"\\nDifferences:\")\n",
    "            if hierarchy.shape != hierarchy_true.shape:\n",
    "                print(f\"Shape mismatch: Expected {hierarchy_true.shape}, got {hierarchy.shape}\")\n",
    "            else:\n",
    "                # Show where values differ\n",
    "                differences = (hierarchy != hierarchy_true).any(axis=1)\n",
    "                if differences.any():\n",
    "                    print(\"\\nMismatched rows:\")\n",
    "                    print(\"Expected:\")\n",
    "                    print(hierarchy_true[differences])\n",
    "                    print(\"\\nGot:\")\n",
    "                    print(hierarchy[differences])\n",
    "            # Continue to next test case instead of raising\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all components in parallel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
