{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical algorithm optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rustworkx as rx\n",
    "from collections import Counter\n",
    "\n",
    "def verify_components(table) -> dict:\n",
    "    \"\"\"\n",
    "    Fast verification of connected components using rustworkx.\n",
    "    \n",
    "    Args:\n",
    "        table: PyArrow table with 'left', 'right' columns\n",
    "    \n",
    "    Returns:\n",
    "        dictionary containing basic component statistics\n",
    "    \"\"\"\n",
    "    # Create graph directly from arrays\n",
    "    graph = rx.PyDiGraph()\n",
    "    \n",
    "    # Add all unique nodes at once\n",
    "    unique_nodes = set(table['left'].to_numpy()) | set(table['right'].to_numpy())\n",
    "    graph.add_nodes_from(range(len(unique_nodes)))\n",
    "    \n",
    "    # Create node mapping and edges in one pass\n",
    "    node_to_idx = {node: idx for idx, node in enumerate(unique_nodes)}\n",
    "    edges = [(node_to_idx[left], node_to_idx[right], prob) \n",
    "            for left, right, prob in zip(table['left'].to_numpy(), \n",
    "                                       table['right'].to_numpy(),\n",
    "                                       table['probability'].to_numpy())]\n",
    "    \n",
    "    # Add all edges at once\n",
    "    graph.add_edges_from(edges)\n",
    "    \n",
    "    # Get components and their sizes\n",
    "    components = rx.weakly_connected_components(graph)\n",
    "    component_sizes = Counter(len(component) for component in components)\n",
    "    \n",
    "    return {\n",
    "        'num_components': len(components),\n",
    "        'total_nodes': len(unique_nodes),\n",
    "        'total_edges': len(edges),\n",
    "        'component_sizes': component_sizes,\n",
    "        'min_component_size': min(component_sizes.keys()),\n",
    "        'max_component_size': max(component_sizes.keys())\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_max_possible_edges(n_nodes: int, num_components: int) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the maximum possible number of edges given n nodes split into k components.\n",
    "    \n",
    "    Args:\n",
    "        n_nodes: Total number of nodes\n",
    "        num_components: Number of components to split into\n",
    "        \n",
    "    Returns:\n",
    "        Maximum possible number of edges\n",
    "    \"\"\"\n",
    "    nodes_per_component = n_nodes // num_components\n",
    "    max_edges_per_component = nodes_per_component * nodes_per_component  # Complete bipartite graph\n",
    "    return max_edges_per_component * num_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import rustworkx as rx\n",
    "from typing import List, Tuple\n",
    "from decimal import Decimal\n",
    "\n",
    "def split_values_into_components(values: List[int], num_components: int) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Split values into non-overlapping groups for each component.\n",
    "    \n",
    "    Args:\n",
    "        values: List of values to split\n",
    "        num_components: Number of components to create\n",
    "        \n",
    "    Returns:\n",
    "        List of arrays, one for each component\n",
    "    \"\"\"\n",
    "    values = np.array(values)\n",
    "    np.random.shuffle(values)\n",
    "    return np.array_split(values, num_components)\n",
    "\n",
    "\n",
    "def generate_arrow_data(\n",
    "    left_values: List[int],\n",
    "    right_values: List[int],\n",
    "    prob_range: Tuple[float, float],\n",
    "    num_components: int,\n",
    "    total_rows: int\n",
    ") -> pa.Table:\n",
    "    \"\"\"\n",
    "    Generate dummy arrow data with guaranteed isolated components.\n",
    "    \n",
    "    Args:\n",
    "        left_values: List of integers to use for left column\n",
    "        right_values: List of integers to use for right column\n",
    "        prob_range: Tuple of (min_prob, max_prob) to constrain probabilities\n",
    "        num_components: Number of distinct connected components to generate\n",
    "        total_rows: Total number of rows to generate\n",
    "    \n",
    "    Returns:\n",
    "        PyArrow Table with 'left', 'right', and 'probability' columns\n",
    "    \"\"\"\n",
    "    if len(left_values) < 2 or len(right_values) < 2:\n",
    "        raise ValueError(\"Need at least 2 possible values for both left and right\")\n",
    "    if num_components > min(len(left_values), len(right_values)):\n",
    "        raise ValueError(\"Cannot have more components than minimum of left/right values\")\n",
    "    \n",
    "    # Calculate maximum possible edges\n",
    "    min_nodes = min(len(left_values), len(right_values))\n",
    "    max_possible_edges = calculate_max_possible_edges(min_nodes, num_components)\n",
    "    \n",
    "    if total_rows > max_possible_edges:\n",
    "        raise ValueError(\n",
    "            f\"Cannot generate {total_rows:,} edges with {num_components:,} components. \"\n",
    "            f\"Maximum possible edges is {max_possible_edges:,} given {min_nodes:,} nodes. \"\n",
    "            \"Either increase the number of nodes, decrease the number of components, \"\n",
    "            \"or decrease the total edges requested.\"\n",
    "        )\n",
    "    \n",
    "    # Convert probability range to integers (60-80 for 0.60-0.80)\n",
    "    prob_min = int(prob_range[0] * 100)\n",
    "    prob_max = int(prob_range[1] * 100)\n",
    "    \n",
    "    # Split values into completely separate groups for each component\n",
    "    left_components = split_values_into_components(left_values, num_components)\n",
    "    right_components = split_values_into_components(right_values, num_components)\n",
    "    \n",
    "    # Calculate base number of edges per component\n",
    "    base_edges_per_component = total_rows // num_components\n",
    "    remaining_edges = total_rows % num_components\n",
    "    \n",
    "    all_edges = []\n",
    "    \n",
    "    # Generate edges for each component\n",
    "    for comp_idx in range(num_components):\n",
    "        comp_left_values = left_components[comp_idx]\n",
    "        comp_right_values = right_components[comp_idx]\n",
    "        \n",
    "        # Calculate edges for this component\n",
    "        edges_in_component = base_edges_per_component\n",
    "        if comp_idx < remaining_edges:  # Distribute remaining edges\n",
    "            edges_in_component += 1\n",
    "            \n",
    "        # Ensure basic connectivity within the component\n",
    "        base_edges = []\n",
    "        \n",
    "        # Create a spanning tree-like structure\n",
    "        for i in range(len(comp_left_values)):\n",
    "            base_edges.append((\n",
    "                comp_left_values[i],\n",
    "                comp_right_values[i % len(comp_right_values)],\n",
    "                np.random.randint(prob_min, prob_max + 1)\n",
    "            ))\n",
    "        \n",
    "        # Generate remaining random edges strictly within this component\n",
    "        remaining_edges = edges_in_component - len(base_edges)\n",
    "        if remaining_edges > 0:\n",
    "            random_lefts = np.random.choice(comp_left_values, size=remaining_edges)\n",
    "            random_rights = np.random.choice(comp_right_values, size=remaining_edges)\n",
    "            random_probs = np.random.randint(prob_min, prob_max + 1, size=remaining_edges)\n",
    "            \n",
    "            component_edges = base_edges + list(zip(random_lefts, random_rights, random_probs))\n",
    "        else:\n",
    "            component_edges = base_edges\n",
    "            \n",
    "        all_edges.extend(component_edges)\n",
    "    \n",
    "    # Convert to arrays\n",
    "    lefts, rights, probs = zip(*all_edges)\n",
    "    \n",
    "    # Create PyArrow arrays\n",
    "    left_array = pa.array(lefts, type=pa.int64())\n",
    "    right_array = pa.array(rights, type=pa.int64())\n",
    "    decimal_probs = [Decimal(str(p/100)) for p in probs]\n",
    "    prob_array = pa.array(decimal_probs, type=pa.decimal128(precision=3, scale=2))\n",
    "    \n",
    "    return pa.table([left_array, right_array, prob_array],\n",
    "                   names=['left', 'right', 'probability'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x10d274c90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components found: 10\n",
      "Total nodes: 20000\n",
      "Total edges: 1000009\n",
      "\n",
      "Component sizes:\n",
      "Size 2000: 10 components\n"
     ]
    }
   ],
   "source": [
    "left_values = list(range(10_000))\n",
    "right_values = list(range(10_000, 20_000))\n",
    "prob_range = (0.6, 0.8)\n",
    "num_components = 10\n",
    "total_rows = 1_000_000\n",
    "\n",
    "table = generate_arrow_data(\n",
    "    left_values=left_values,\n",
    "    right_values=right_values,\n",
    "    prob_range=prob_range,\n",
    "    num_components=num_components,\n",
    "    total_rows=total_rows\n",
    ")\n",
    "\n",
    "results = verify_components(table)\n",
    "print(f\"Number of components found: {results['num_components']}\")\n",
    "print(f\"Total nodes: {results['total_nodes']}\")\n",
    "print(f\"Total edges: {results['total_edges']}\")\n",
    "print(\"\\nComponent sizes:\")\n",
    "for size, count in sorted(results['component_sizes'].items()):\n",
    "    print(f\"Size {size}: {count} components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_values = list(range(int(2e7)))\n",
    "right_values = list(range(int(2e7), int(4e7)))\n",
    "prob_range = (0.7, 1.0)\n",
    "num_components = 200_000\n",
    "total_rows = int(1e8)\n",
    "\n",
    "table = generate_arrow_data(\n",
    "    left_values=left_values,\n",
    "    right_values=right_values,\n",
    "    prob_range=prob_range,\n",
    "    num_components=num_components,\n",
    "    total_rows=total_rows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components found: 206763\n",
      "Total nodes: 40000000\n",
      "Total edges: 100000400\n",
      "\n",
      "Component sizes:\n",
      "Size 2: 6755 components\n",
      "Size 4: 8 components\n",
      "Size 194: 1 components\n",
      "Size 196: 124 components\n",
      "Size 198: 6520 components\n",
      "Size 200: 193355 components\n"
     ]
    }
   ],
   "source": [
    "results = verify_components(table)\n",
    "print(f\"Number of components found: {results['num_components']}\")\n",
    "print(f\"Total nodes: {results['total_nodes']}\")\n",
    "print(f\"Total edges: {results['total_edges']}\")\n",
    "print(\"\\nComponent sizes:\")\n",
    "for size, count in sorted(results['component_sizes'].items()):\n",
    "    print(f\"Size {size}: {count} components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of components found: 206763\n",
    "* Total nodes: 40000000\n",
    "* Total edges: 100000400\n",
    "\n",
    "Component sizes:\n",
    "* Size 2: 6755 components\n",
    "* Size 4: 8 components\n",
    "* Size 194: 1 components\n",
    "* Size 196: 124 components\n",
    "* Size 198: 6520 components\n",
    "* Size 200: 193355 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "pq.write_table(table, Path.cwd() / 'hierarchical_cc200k.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_values = list(range(int(2e5)))\n",
    "right_values = list(range(int(2e5), int(4e5)))\n",
    "prob_range = (0.7, 1.0)\n",
    "num_components = 2_000\n",
    "total_rows = int(1e6)\n",
    "\n",
    "table2 = generate_arrow_data(\n",
    "    left_values=left_values,\n",
    "    right_values=right_values,\n",
    "    prob_range=prob_range,\n",
    "    num_components=num_components,\n",
    "    total_rows=total_rows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components found: 2,080\n",
      "Total nodes: 400,000\n",
      "Total edges: 1,000,400\n",
      "\n",
      "Component sizes:\n",
      "Size 2: 80 components\n",
      "Size 196: 1 components\n",
      "Size 198: 78 components\n",
      "Size 200: 1,921 components\n"
     ]
    }
   ],
   "source": [
    "results2 = verify_components(table2)\n",
    "print(f\"Number of components found: {results2['num_components']:,}\")\n",
    "print(f\"Total nodes: {results2['total_nodes']:,}\")\n",
    "print(f\"Total edges: {results2['total_edges']:,}\")\n",
    "print(\"\\nComponent sizes:\")\n",
    "for size, count in sorted(results2['component_sizes'].items()):\n",
    "    print(f\"Size {size:,}: {count:,} components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of components found: 2,080\n",
    "* Total nodes: 400,000\n",
    "* Total edges: 1,000,400\n",
    "\n",
    "Component sizes:\n",
    "* Size 2: 80 components\n",
    "* Size 196: 1 components\n",
    "* Size 198: 78 components\n",
    "* Size 200: 1,921 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "pq.write_table(table2, Path.cwd() / 'hierarchical_cc2k.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical vs cluster representation\n",
    "\n",
    "Dejan has suggested we store clusters in in a format that minimises recursion. \n",
    "\n",
    "Consider that bc forms at a lower threshold than ab, so we'll need to hold both.\n",
    "\n",
    "Representation A -- proposed in the notebook, used in repo.\n",
    "\n",
    "| parent | child |\n",
    "| --- | --- |\n",
    "| ab | a |\n",
    "| ab | b |\n",
    "| bc | b |\n",
    "| bc | c |\n",
    "|abc | ab |\n",
    "|abc | bc |\n",
    "\n",
    "Representation B -- Dejan's suggestion.\n",
    "\n",
    "| parent | child |\n",
    "| --- | --- |\n",
    "| ab | a |\n",
    "| ab | b |\n",
    "| bc | b |\n",
    "| bc | c |\n",
    "|abc | a |\n",
    "|abc | b |\n",
    "|abc | c |\n",
    "\n",
    "How does the space required to do this change in the \"happy path\" version of this data, where there's lots of components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Representation A count of 2,000 connected components over 1m rows: 2,533,537'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nice and easy to calc for cc2k -- it's what we've been building through the notebook\n",
    "\n",
    "f\"Representation A count of 2,000 connected components over 1m rows: {len(h_out):,}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's adapt `to_clusters()` to get the answer for representation B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rustworkx as rx\n",
    "\n",
    "def to_clusters(results: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Converts probabilities into a list of connected components formed at each threshold.\n",
    "\n",
    "    Returns:\n",
    "        Probabilities sorted by threshold descending.\n",
    "    \"\"\"\n",
    "    G = rx.PyGraph()\n",
    "    added: dict[bytes, int] = {}\n",
    "    components: dict[str, list] = {\"parent\": [], \"child\": [], \"threshold\": []}\n",
    "\n",
    "    # Sort probabilities descending and group by probability\n",
    "    edges_df = results.select(['left', 'right', 'probability']).sort_by([(\"probability\", \"descending\")])\n",
    "    \n",
    "    # Get unique probability thresholds, sorted\n",
    "    thresholds = pa.compute.unique(edges_df.column('probability'))\n",
    "\n",
    "    # Process edges grouped by probability threshold\n",
    "    for prob in thresholds.to_pylist():\n",
    "        mask = pa.compute.equal(edges_df.column('probability'), prob)\n",
    "        threshold_edges = edges_df.filter(mask)\n",
    "        # Get state before adding this batch of edges\n",
    "        old_components = {frozenset(comp) for comp in rx.connected_components(G)}\n",
    "\n",
    "        # Add all nodes and edges at this probability threshold\n",
    "        edge_values = zip(\n",
    "            threshold_edges.column('left').to_pylist(),\n",
    "            threshold_edges.column('right').to_pylist()\n",
    "        )\n",
    "\n",
    "        for left, right in edge_values:\n",
    "            for hash_val in (left, right):\n",
    "                if hash_val not in added:\n",
    "                    idx = G.add_node(hash_val)\n",
    "                    added[hash_val] = idx\n",
    "\n",
    "            G.add_edge(added[left], added[right], None)\n",
    "\n",
    "        new_components = {frozenset(comp) for comp in rx.connected_components(G)}\n",
    "        changed_components = new_components - old_components\n",
    "\n",
    "        # For each changed component, add ALL members at current threshold\n",
    "        for comp in changed_components:\n",
    "            children = [G.get_node_data(n) for n in comp]\n",
    "            parent = combine_integers(*children)\n",
    "\n",
    "            components[\"parent\"].extend([parent] * len(children))\n",
    "            components[\"child\"].extend(children)\n",
    "            components[\"threshold\"].extend([prob] * len(children))\n",
    "\n",
    "    return pa.Table.from_pydict(components)\n",
    "\n",
    "hout_b = to_clusters(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Representation B count of 2,000 connected components over 1m rows: 6,385,847'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Representation B count of 2,000 connected components over 1m rows: {len(hout_b):,}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "h2 = pq.read_table('hierarchical_cc2k.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "left: int64\n",
       "right: int64\n",
       "probability: decimal128(3, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plan.\n",
    "\n",
    "* Find components and their sizes at lowest threshold (rustworkx)\n",
    "* Use this to dask.groupby the data for parallel per-component processing\n",
    "* Ensure we implement early stopping!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2080,\n",
       " pyarrow.Table\n",
       " left: int64\n",
       " right: int64\n",
       " probability: decimal128(3, 2)\n",
       " component: int64\n",
       " ----\n",
       " left: [[197413,116407,114551,160857,6412,...,39429,156175,197197,48177,121674]]\n",
       " right: [[326505,384163,344025,248700,258884,...,311452,278755,204144,357956,378111]]\n",
       " probability: [[1.00,1.00,1.00,1.00,1.00,...,0.70,0.70,0.70,0.70,0.70]]\n",
       " component: [[0,0,0,0,0,...,2079,2079,2079,2079,2079]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import rustworkx as rx\n",
    "import numpy as np\n",
    "\n",
    "def attach_independent_components(table: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Returns the original table with an additional 'component' column indicating\n",
    "    which connected component each edge belongs to.\n",
    "    \"\"\"\n",
    "    # Create dictionary array from sorted unique values\n",
    "    unique = pc.unique(\n",
    "        pa.concat_arrays([\n",
    "            table['left'].combine_chunks(),\n",
    "            table['right'].combine_chunks()\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    # Get indices into unique array for graph construction\n",
    "    left_indices = pc.index_in(table['left'], unique)\n",
    "    right_indices = pc.index_in(table['right'], unique)\n",
    "    \n",
    "    # Create and process graph\n",
    "    n_nodes = len(unique)\n",
    "    n_edges = len(table)\n",
    "    \n",
    "    graph = rx.PyGraph(\n",
    "        node_count_hint=n_nodes,\n",
    "        edge_count_hint=n_edges\n",
    "    )\n",
    "    graph.add_nodes_from(range(n_nodes))\n",
    "\n",
    "    edges = tuple(zip(left_indices.to_numpy(), right_indices.to_numpy()))\n",
    "    graph.add_edges_from_no_data(edges)\n",
    "    \n",
    "    # Get components and create mapping array\n",
    "    components = rx.connected_components(graph)\n",
    "    \n",
    "    # Convert components to numpy arrays\n",
    "    component_indices = np.concatenate([np.array(list(c)) for c in components])\n",
    "    component_labels = np.repeat(np.arange(len(components)), [len(c) for c in components])\n",
    "    \n",
    "    # Create mapping array and fill with component labels\n",
    "    node_to_component = np.zeros(len(unique), dtype=np.int64)\n",
    "    node_to_component[component_indices] = component_labels\n",
    "    \n",
    "    # Use the indices we already have to map back to components  \n",
    "    edge_components = pa.array(node_to_component[left_indices.to_numpy()])\n",
    "    \n",
    "    return table.append_column('component', edge_components).sort_by(\n",
    "        [('component', 'ascending'), ('probability', 'descending')]\n",
    "    )\n",
    "\n",
    "cc2 = attach_independent_components(h2)\n",
    "len(pc.unique(cc2.column('component'))), cc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         34170 function calls in 1.108 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 49 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        2    0.000    0.000    1.108    0.554 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3541(run_code)\n",
      "        2    0.000    0.000    1.108    0.554 {built-in method builtins.exec}\n",
      "        1    0.058    0.058    1.108    1.108 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_25768/655470289.py:1(<module>)\n",
      "        1    0.397    0.397    1.050    1.050 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_25768/2783446239.py:5(attach_independent_components)\n",
      "        3    0.296    0.099    0.296    0.099 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/compute.py:249(wrapper)\n",
      "        1    0.118    0.118    0.118    0.118 {connected_components}\n",
      "        1    0.088    0.088    0.088    0.088 {method 'add_edges_from_no_data' of 'rustworkx.PyGraph' objects}\n",
      "        1    0.066    0.066    0.066    0.066 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/compute.py:239(wrapper)\n",
      "        1    0.007    0.007    0.033    0.033 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_25768/2783446239.py:39(<listcomp>)\n",
      "     2080    0.026    0.000    0.026    0.000 {built-in method numpy.array}\n",
      "     2850    0.008    0.000    0.021    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py:775(_clean_thread_parent_frames)\n",
      "        1    0.017    0.017    0.017    0.017 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/compute.py:446(take)\n",
      "        1    0.013    0.013    0.013    0.013 {method 'add_nodes_from' of 'rustworkx.PyGraph' objects}\n",
      "     1425    0.005    0.000    0.008    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py:790(<setcomp>)\n",
      "     1425    0.004    0.000    0.005    0.000 /Users/willlangdale/.pyenv/versions/3.11.8/lib/python3.11/threading.py:1501(enumerate)\n",
      "    11400    0.003    0.000    0.003    0.000 /Users/willlangdale/.pyenv/versions/3.11.8/lib/python3.11/threading.py:1168(ident)\n",
      "     5700    0.001    0.000    0.001    0.000 {method 'keys' of 'dict' objects}\n",
      "        1    0.000    0.000    0.001    0.001 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_25768/2783446239.py:40(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:467(repeat)\n",
      "        1    0.000    0.000    0.000    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:51(_wrapfunc)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x17d84cdd0>"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "from pstats import SortKey\n",
    "\n",
    "# Profile the function\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "components = attach_independent_components(table2)\n",
    "pr.disable()\n",
    "\n",
    "# Print stats sorted by cumulative time\n",
    "ps = pstats.Stats(pr).sort_stats(SortKey.CUMULATIVE)\n",
    "ps.print_stats(20)  # Show top 20 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         3411228 function calls in 121.198 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 48 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        2    0.000    0.000  139.163   69.581 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3541(run_code)\n",
      "        2    0.001    0.000  139.162   69.581 {built-in method builtins.exec}\n",
      "        1   47.532   47.532  121.195  121.195 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_25768/2801194547.py:1(find_independent_components)\n",
      "        1   24.123   24.123   24.123   24.123 {method 'add_edges_from_no_data' of 'rustworkx.PyGraph' objects}\n",
      "        1   18.713   18.713   18.731   18.731 {connected_components}\n",
      "        2   14.375    7.187   14.375    7.187 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/compute.py:249(wrapper)\n",
      "        1    7.661    7.661    7.661    7.661 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/compute.py:239(wrapper)\n",
      "        1    1.303    1.303    4.041    4.041 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_25768/2801194547.py:35(<listcomp>)\n",
      "   206763    2.738    0.000    2.738    0.000 {built-in method numpy.array}\n",
      "   285488    0.966    0.000    2.629    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py:775(_clean_thread_parent_frames)\n",
      "        1    1.879    1.879    1.879    1.879 {method 'add_nodes_from' of 'rustworkx.PyGraph' objects}\n",
      "   142744    0.579    0.000    0.912    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py:790(<setcomp>)\n",
      "   142744    0.527    0.000    0.629    0.000 /Users/willlangdale/.pyenv/versions/3.11.8/lib/python3.11/threading.py:1501(enumerate)\n",
      "  1141952    0.333    0.000    0.333    0.000 /Users/willlangdale/.pyenv/versions/3.11.8/lib/python3.11/threading.py:1168(ident)\n",
      "        1    0.001    0.001    0.167    0.167 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:467(repeat)\n",
      "        1    0.000    0.000    0.166    0.166 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:51(_wrapfunc)\n",
      "        1    0.166    0.166    0.166    0.166 {method 'repeat' of 'numpy.ndarray' objects}\n",
      "   570976    0.082    0.000    0.082    0.000 {method 'keys' of 'dict' objects}\n",
      "        1    0.053    0.053    0.073    0.073 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_25768/2801194547.py:36(<listcomp>)\n",
      "   285488    0.053    0.000    0.053    0.000 {method 'values' of 'dict' objects}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x16d1a8190>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "from pstats import SortKey\n",
    "\n",
    "# Profile the function\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "_ = attach_independent_components(table)\n",
    "pr.disable()\n",
    "\n",
    "# Print stats sorted by cumulative time\n",
    "ps = pstats.Stats(pr).sort_stats(SortKey.CUMULATIVE)\n",
    "ps.print_stats(20)  # Show top 20 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process a single component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def combine_integers(*n: int) -> int:\n",
    "    \"\"\"\n",
    "    Combine n integers into a single negative integer.\n",
    "\n",
    "    Used to create a symmetric deterministic hash of two integers that populates the\n",
    "    range of integers efficiently and reduces the likelihood of collisions.\n",
    "\n",
    "    Aims to vectorise amazingly when used in Arrow.\n",
    "\n",
    "    Does this by:\n",
    "\n",
    "    * Using a Mersenne prime as a modulus\n",
    "    * Making negative integers positive with modulo, sped up with bitwise operations\n",
    "    * Combining using symmetric operations with coprime multipliers\n",
    "\n",
    "    Args:\n",
    "        *args: Variable number of integers to combine\n",
    "\n",
    "    Returns:\n",
    "        A negative integer\n",
    "    \"\"\"\n",
    "    P = 2147483647\n",
    "\n",
    "    total = 0\n",
    "    product = 1\n",
    "\n",
    "    for x in sorted(n):\n",
    "        x_pos = (x ^ (x >> 31)) - (x >> 31)\n",
    "        total = (total + x_pos) % P\n",
    "        product = (product * x_pos) % P\n",
    "\n",
    "    result = (31 * total + 37 * product) % P\n",
    "\n",
    "    return -result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized results: [-930]\n",
      "Original results: [-930]\n",
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def combine_integers(*n: int) -> int:\n",
    "    \"\"\"Original function for single tuple of integers\"\"\"\n",
    "    P = 2147483647\n",
    "\n",
    "    total = 0\n",
    "    product = 1\n",
    "\n",
    "    for x in sorted(n):\n",
    "        x_pos = (x ^ (x >> 31)) - (x >> 31)\n",
    "        total = (total + x_pos) % P\n",
    "        product = (product * x_pos) % P\n",
    "\n",
    "    result = (31 * total + 37 * product) % P\n",
    "\n",
    "    return -result\n",
    "\n",
    "def vectorized_combine_integers(arrays: list[list[int]] | np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Vectorized version of combine_integers that works on arrays of integer arrays.\n",
    "    \n",
    "    Args:\n",
    "        arrays: List of integer arrays or 2D numpy array where each row is a set of integers\n",
    "               to be combined\n",
    "               \n",
    "    Returns:\n",
    "        1D numpy array of combined negative integers, one for each input array\n",
    "    \"\"\"\n",
    "    P = 2147483647\n",
    "    arrays = np.asarray(arrays)\n",
    "    arrays = np.sort(arrays, axis=1)\n",
    "    \n",
    "    signs = arrays >> 31\n",
    "    x_pos = (arrays ^ signs) - signs\n",
    "    \n",
    "    totals = np.sum(x_pos, axis=1) % P\n",
    "    products = np.prod(x_pos, axis=1) % P\n",
    "    \n",
    "    results = (31 * totals + 37 * products) % P\n",
    "    \n",
    "    return -results\n",
    "\n",
    "test_arrays = [\n",
    "    # [1, 2, 3],\n",
    "    # [-1, -2, -3],\n",
    "    # [10, 20, 0],\n",
    "    [10, 20, 0]\n",
    "]\n",
    "\n",
    "vectorized_results = vectorized_combine_integers(test_arrays)\n",
    "original_results = np.array([combine_integers(*arr) for arr in test_arrays])\n",
    "\n",
    "print(\"Vectorized results:\", vectorized_results)\n",
    "print(\"Original results:\", original_results)\n",
    "print(\"Match:\", np.array_equal(vectorized_results, original_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Results:\n",
      "------------------------------------------------------------\n",
      "        Size | Original (s) | Vectorized (s) |  Speedup\n",
      "------------------------------------------------------------\n",
      "    100x3    |       0.0002 |       0.0003 |     0.75x\n",
      "   1000x3    |       0.0024 |       0.0016 |     1.46x\n",
      "  10000x3    |       0.0218 |       0.0057 |     3.81x\n",
      " 100000x3    |       0.2714 |       0.0585 |     4.64x\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "import random\n",
    "\n",
    "def generate_test_data(num_arrays: int, array_size: int) -> List[List[int]]:\n",
    "    \"\"\"Generate random test arrays\"\"\"\n",
    "    return [\n",
    "        [random.randint(-1000000, 1000000) for _ in range(array_size)]\n",
    "        for _ in range(num_arrays)\n",
    "    ]\n",
    "\n",
    "def run_benchmark(arrays: List[List[int]]) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Run speed comparison between original and vectorized versions\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (original_time, vectorized_time) in seconds\n",
    "    \"\"\"\n",
    "    # Time original version\n",
    "    start = time.time()\n",
    "    original_results = [combine_integers(*arr) for arr in arrays]\n",
    "    original_time = time.time() - start\n",
    "    \n",
    "    # Time vectorized version\n",
    "    start = time.time()\n",
    "    vectorized_results = vectorized_combine_integers(arrays)\n",
    "    vectorized_time = time.time() - start\n",
    "    \n",
    "    # Verify results match\n",
    "    assert np.array_equal(original_results, vectorized_results), \"Results don't match!\"\n",
    "    \n",
    "    return original_time, vectorized_time\n",
    "\n",
    "# Run benchmarks with different sizes\n",
    "test_sizes = [\n",
    "    (100, 3),     # 100 arrays of size 3\n",
    "    (1000, 3),    # 1000 arrays of size 3\n",
    "    (10000, 3),   # 10000 arrays of size 3\n",
    "    (100000, 3),  # 100000 arrays of size 3\n",
    "]\n",
    "\n",
    "print(\"Benchmark Results:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Size':>12} | {'Original (s)':>12} | {'Vectorized (s)':>12} | {'Speedup':>8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for num_arrays, array_size in test_sizes:\n",
    "    # Generate test data\n",
    "    arrays = generate_test_data(num_arrays, array_size)\n",
    "    \n",
    "    # Run benchmark\n",
    "    orig_time, vec_time = run_benchmark(arrays)\n",
    "    speedup = orig_time / vec_time\n",
    "    \n",
    "    print(f\"{num_arrays:>7}x{array_size:<4} | {orig_time:>12.4f} | {vec_time:>12.4f} | {speedup:>8.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized results: ['abcdef' 'xyz' 'dehlorw']\n",
      "Original results: ['abcdef' 'xyz' 'dehlorw']\n",
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def combine_strings(*n: str) -> str:\n",
    "    \"\"\"\n",
    "    Combine n strings into a single string.\n",
    "\n",
    "    Args:\n",
    "        *args: Variable number of strings to combine\n",
    "        \n",
    "    Returns:\n",
    "        A single string\n",
    "    \"\"\"\n",
    "    letters = set(chain.from_iterable(n))\n",
    "    return \"\".join(sorted(letters))\n",
    "\n",
    "\n",
    "def vectorized_combine_strings(arrays: list[list[str]] | np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Vectorized version of combine_strings that works on arrays of string arrays.\n",
    "    \n",
    "    Args:\n",
    "        arrays: List of string arrays or 2D numpy array where each row is a set of \n",
    "               strings to be combined\n",
    "               \n",
    "    Returns:\n",
    "        1D numpy array of combined strings, one for each input array\n",
    "    \"\"\"\n",
    "    arrays = np.asarray(arrays, dtype=object)\n",
    "    \n",
    "    def process_row(row):\n",
    "        return combine_strings(*row)\n",
    "    \n",
    "    vfunc = np.vectorize(process_row, signature='(n)->()', otypes=[object])\n",
    "    return vfunc(arrays)\n",
    "\n",
    "test_arrays = [\n",
    "    [\"abc\", \"def\", \"abd\"],\n",
    "    [\"xyz\", \"xyy\", \"xzz\"],\n",
    "    [\"hello\", \"world\", \"hello\"]\n",
    "]\n",
    "\n",
    "vectorized_results = vectorized_combine_strings(test_arrays)\n",
    "original_results = np.array([combine_strings(*arr) for arr in test_arrays])\n",
    "\n",
    "print(\"Vectorized results:\", vectorized_results)\n",
    "print(\"Original results:\", original_results)\n",
    "print(\"Match:\", np.array_equal(vectorized_results, original_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import TypeVar, Generic, Hashable, Iterator\n",
    "import pandas as pd\n",
    "\n",
    "T = TypeVar('T', bound=Hashable)\n",
    "\n",
    "class UnionFindWithDiff(Generic[T]):\n",
    "    def __init__(self):\n",
    "        self.parent: dict[T, T] = {}\n",
    "        self.rank: dict[T, int] = {}\n",
    "        self._shadow_parent: dict[T, T] = {}\n",
    "        self._shadow_rank: dict[T, int] = {}\n",
    "        self._pending_pairs: list[tuple[T, T]] = []\n",
    "        \n",
    "    def make_set(self, x: T) -> None:\n",
    "        if x not in self.parent:\n",
    "            self.parent[x] = x\n",
    "            self.rank[x] = 0\n",
    "    \n",
    "    def find(self, x: T, parent_dict: dict[T, T] | None = None) -> T:\n",
    "        if parent_dict is None:\n",
    "            parent_dict = self.parent\n",
    "            \n",
    "        if x not in parent_dict:\n",
    "            self.make_set(x)\n",
    "            if parent_dict is self._shadow_parent:\n",
    "                self._shadow_parent[x] = x\n",
    "                self._shadow_rank[x] = 0\n",
    "        \n",
    "        while parent_dict[x] != x:\n",
    "            parent_dict[x] = parent_dict[parent_dict[x]]\n",
    "            x = parent_dict[x]\n",
    "        return x\n",
    "    \n",
    "    def union(self, x: T, y: T) -> None:\n",
    "        root_x = self.find(x)\n",
    "        root_y = self.find(y)\n",
    "        \n",
    "        if root_x != root_y:\n",
    "            self._pending_pairs.append((x, y))\n",
    "            \n",
    "            if self.rank[root_x] < self.rank[root_y]:\n",
    "                root_x, root_y = root_y, root_x\n",
    "            self.parent[root_y] = root_x\n",
    "            if self.rank[root_x] == self.rank[root_y]:\n",
    "                self.rank[root_x] += 1\n",
    "    \n",
    "    def get_component(self, x: T, parent_dict: dict[T, T] | None = None) -> set[T]:\n",
    "        if parent_dict is None:\n",
    "            parent_dict = self.parent\n",
    "        \n",
    "        root = self.find(x, parent_dict)\n",
    "        return {y for y in parent_dict if self.find(y, parent_dict) == root}\n",
    "    \n",
    "    def get_components(self, parent_dict: dict[T, T] | None = None) -> list[set[T]]:\n",
    "        if parent_dict is None:\n",
    "            parent_dict = self.parent\n",
    "            \n",
    "        components = defaultdict(set)\n",
    "        for x in parent_dict:\n",
    "            root = self.find(x, parent_dict)\n",
    "            components[root].add(x)\n",
    "        return list(components.values())\n",
    "    \n",
    "    def diff(self) -> Iterator[tuple[set[T], set[T]]]:\n",
    "        \"\"\"\n",
    "        Returns differences including all pairwise merges that occurred since last diff,\n",
    "        excluding cases where old_comp == new_comp.\n",
    "        \"\"\"\n",
    "        # Get current state before processing pairs\n",
    "        current_components = self.get_components()\n",
    "        reported_pairs = set()\n",
    "        \n",
    "        # Process pending pairs\n",
    "        for x, y in self._pending_pairs:\n",
    "            # Find the final component containing the pair\n",
    "            final_component = next(comp for comp in current_components \n",
    "                                 if x in comp and y in comp)\n",
    "            \n",
    "            # Only report if the pair forms a proper subset of the final component\n",
    "            pair_component = {x, y}\n",
    "            if (pair_component != final_component and \n",
    "                frozenset((frozenset(pair_component), frozenset(final_component))) not in reported_pairs):\n",
    "                reported_pairs.add(frozenset((frozenset(pair_component), frozenset(final_component))))\n",
    "                yield (pair_component, final_component)\n",
    "        \n",
    "        self._pending_pairs.clear()\n",
    "        \n",
    "        # Handle initial state\n",
    "        if not self._shadow_parent:\n",
    "            self._shadow_parent = self.parent.copy()\n",
    "            self._shadow_rank = self.rank.copy()\n",
    "            return\n",
    "        \n",
    "        # Get old components\n",
    "        old_components = self.get_components(self._shadow_parent)\n",
    "        \n",
    "        # Report changes between old and new states\n",
    "        for old_comp in old_components:\n",
    "            if len(old_comp) > 1:  # Only consider non-singleton old components\n",
    "                sample_elem = next(iter(old_comp))\n",
    "                new_comp = next(comp for comp in current_components if sample_elem in comp)\n",
    "                \n",
    "                # Only yield if the components are different and this pair hasn't been reported\n",
    "                if (old_comp != new_comp and \n",
    "                    frozenset((frozenset(old_comp), frozenset(new_comp))) not in reported_pairs):\n",
    "                    reported_pairs.add(frozenset((frozenset(old_comp), frozenset(new_comp))))\n",
    "                    yield (old_comp, new_comp)\n",
    "        \n",
    "        # Update shadow copy\n",
    "        self._shadow_parent = self.parent.copy()\n",
    "        self._shadow_rank = self.rank.copy()\n",
    "\n",
    "\n",
    "def component_to_hierarchy(key: str | int | tuple, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert pairwise probabilities into a hierarchical representation.\n",
    "    Assumes data is pre-sorted by probability descending.\n",
    "    \n",
    "    Args:\n",
    "        key: Group key (ignored in this implementation)\n",
    "        df: DataFrame with columns ['left', 'right', 'probability']\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns ['parent', 'child', 'probability'] representing hierarchical merges\n",
    "    \"\"\"\n",
    "    hierarchy: list[tuple[int, int, float]] = []\n",
    "    uf = UnionFindWithDiff[int]()\n",
    "\n",
    "    for threshold in df[\"probability\"].unique():\n",
    "        current_probs = df[df[\"probability\"] == threshold]\n",
    "\n",
    "        for _, row in current_probs.iterrows():\n",
    "            uf.union(row[\"left\"], row[\"right\"])\n",
    "            parent = combine_integers(row[\"left\"], row[\"right\"])\n",
    "            hierarchy.extend([\n",
    "                (parent, row[\"left\"], threshold),\n",
    "                (parent, row[\"right\"], threshold)\n",
    "            ])\n",
    "\n",
    "        for old_comp, new_comp in uf.diff():\n",
    "            if len(old_comp) > 1:\n",
    "                parent = combine_integers(*new_comp)\n",
    "                child = combine_integers(*old_comp)\n",
    "                hierarchy.extend([\n",
    "                    (parent, child, threshold)\n",
    "                ])\n",
    "            else:\n",
    "                parent = combine_integers(*new_comp)\n",
    "                hierarchy.extend([\n",
    "                    (parent, old_comp.pop(), threshold)\n",
    "                ])\n",
    "\n",
    "    return pd.DataFrame(hierarchy, columns=['parent', 'child', 'probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cc2.to_pandas()\n",
    "cc_15_pd = df[df['component'] == 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         142304 function calls (139675 primitive calls) in 0.136 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 276 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        2    0.000    0.000    0.136    0.068 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3541(run_code)\n",
      "        2    0.000    0.000    0.136    0.068 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    0.136    0.136 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_68435/3996032216.py:1(<module>)\n",
      "        1    0.007    0.007    0.136    0.136 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_68435/807932433.py:115(component_to_hierarchy)\n",
      "      532    0.002    0.000    0.059    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/frame.py:1505(iterrows)\n",
      "      532    0.007    0.000    0.054    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/series.py:389(__init__)\n",
      "     3006    0.007    0.000    0.027    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/series.py:1095(__getitem__)\n",
      "      536    0.004    0.000    0.018    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/construction.py:517(sanitize_array)\n",
      "      298    0.003    0.000    0.017    0.000 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_68435/807932433.py:65(diff)\n",
      "       63    0.000    0.000    0.014    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4062(__getitem__)\n",
      "       61    0.007    0.000    0.013    0.000 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_68435/807932433.py:55(get_components)\n",
      "       31    0.000    0.000    0.012    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4130(_getitem_bool_array)\n",
      "     3006    0.004    0.000    0.012    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/series.py:1220(_get_value)\n",
      "32333/32271    0.006    0.000    0.008    0.000 {built-in method builtins.isinstance}\n",
      "      504    0.005    0.000    0.008    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/dtypes/cast.py:1156(maybe_infer_to_datetimelike)\n",
      "       31    0.000    0.000    0.008    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/ops/common.py:62(new_method)\n",
      "      594    0.002    0.000    0.007    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/generic.py:6301(__setattr__)\n",
      "       31    0.000    0.000    0.007    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/arraylike.py:38(__eq__)\n",
      "       31    0.000    0.000    0.007    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/generic.py:4142(_take_with_is_copy)\n",
      "       31    0.000    0.000    0.007    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pandas/core/series.py:6110(_cmp_method)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x12219f390>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "from pstats import SortKey\n",
    "\n",
    "# Profile the function\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "_ = component_to_hierarchy(15, cc_15_pd)\n",
    "pr.disable()\n",
    "\n",
    "# Print stats sorted by cumulative time\n",
    "ps = pstats.Stats(pr).sort_stats(SortKey.CUMULATIVE)\n",
    "ps.print_stats(20)  # Show top 20 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Got unexpected argument type <class 'polars.series.series.Series'> for compute function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 211\u001b[0m\n\u001b[1;32m    204\u001b[0m     parents, children, probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mhierarchy)\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mtable({\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparent\u001b[39m\u001b[38;5;124m'\u001b[39m: pa\u001b[38;5;241m.\u001b[39marray(parents, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mpa\u001b[38;5;241m.\u001b[39mint64()),\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchild\u001b[39m\u001b[38;5;124m'\u001b[39m: pa\u001b[38;5;241m.\u001b[39marray(children, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mpa\u001b[38;5;241m.\u001b[39mint64()),\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprobability\u001b[39m\u001b[38;5;124m'\u001b[39m: pa\u001b[38;5;241m.\u001b[39marray(probs, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mpa\u001b[38;5;241m.\u001b[39mfloat64())\n\u001b[1;32m    209\u001b[0m     })\n\u001b[0;32m--> 211\u001b[0m \u001b[43mcomponent_to_hierarchy_pa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcc_15\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[128], line 168\u001b[0m, in \u001b[0;36mcomponent_to_hierarchy_pa\u001b[0;34m(table)\u001b[0m\n\u001b[1;32m    165\u001b[0m uf \u001b[38;5;241m=\u001b[39m NumpyUnionFindWithDiff[\u001b[38;5;28mint\u001b[39m]()\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# Get unique probabilities\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[43mpc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprobability\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m threshold \u001b[38;5;129;01min\u001b[39;00m probs:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# Get current probability rows\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     mask \u001b[38;5;241m=\u001b[39m pc\u001b[38;5;241m.\u001b[39mequal(table[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprobability\u001b[39m\u001b[38;5;124m'\u001b[39m], threshold)\n",
      "File \u001b[0;32m~/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/compute.py:247\u001b[0m, in \u001b[0;36m_make_generic_wrapper.<locals>.wrapper\u001b[0;34m(memory_pool, *args)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], Expression):\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Expression\u001b[38;5;241m.\u001b[39m_call(func_name, \u001b[38;5;28mlist\u001b[39m(args))\n\u001b[0;32m--> 247\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_pool\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/_compute.pyx:372\u001b[0m, in \u001b[0;36mpyarrow._compute.Function.call\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/_compute.pyx:505\u001b[0m, in \u001b[0;36mpyarrow._compute._pack_compute_args\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Got unexpected argument type <class 'polars.series.series.Series'> for compute function"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import TypeVar, Generic, Hashable, Iterator, Optional, Dict\n",
    "from collections import defaultdict\n",
    "\n",
    "T = TypeVar('T', bound=Hashable)\n",
    "\n",
    "class NumpyUnionFindWithDiff(Generic[T]):\n",
    "    def __init__(self, max_size: int = 1000000):\n",
    "        # Core numpy arrays for fast operations\n",
    "        self.parent = np.arange(max_size, dtype=np.int32)\n",
    "        self.rank = np.zeros(max_size, dtype=np.int32)\n",
    "        self.size = 0\n",
    "        \n",
    "        # Mapping between external IDs and array indices\n",
    "        self.id_to_idx: Dict[T, int] = {}\n",
    "        self.idx_to_id: Dict[int, T] = {}\n",
    "        \n",
    "        # Shadow state\n",
    "        self._shadow_parent = np.arange(max_size, dtype=np.int32)\n",
    "        self._shadow_rank = np.zeros(max_size, dtype=np.int32)\n",
    "        self._pending_x = np.array([], dtype=np.int32)\n",
    "        self._pending_y = np.array([], dtype=np.int32)\n",
    "        \n",
    "    def _get_idx(self, x: T) -> int:\n",
    "        if x not in self.id_to_idx:\n",
    "            idx = self.size\n",
    "            self.id_to_idx[x] = idx\n",
    "            self.idx_to_id[idx] = x\n",
    "            self.size += 1\n",
    "        return self.id_to_idx[x]\n",
    "    \n",
    "    def find_vec(self, indices: np.ndarray, parent_array: Optional[np.ndarray] = None) -> np.ndarray:\n",
    "        if parent_array is None:\n",
    "            parent_array = self.parent\n",
    "            \n",
    "        paths = indices.reshape(-1, 1)\n",
    "        positions = np.zeros(len(indices), dtype=np.int32)\n",
    "        max_length = 32\n",
    "        full_paths = np.zeros((len(indices), max_length), dtype=np.int32)\n",
    "        full_paths[:, 0] = indices\n",
    "        \n",
    "        while True:\n",
    "            next_parent = parent_array[paths[:, -1]]\n",
    "            if np.all(next_parent == paths[:, -1]):\n",
    "                break\n",
    "            positions += 1\n",
    "            full_paths[:, positions] = next_parent\n",
    "            paths = np.hstack([paths, next_parent.reshape(-1, 1)])\n",
    "            \n",
    "        roots = paths[:, -1]\n",
    "        mask = full_paths != 0\n",
    "        parent_array[full_paths[mask]] = np.repeat(roots, mask.sum(axis=1))\n",
    "        \n",
    "        return roots\n",
    "    \n",
    "    def union_vectorized(self, x_vec: np.ndarray, y_vec: np.ndarray) -> None:\n",
    "        x_idx = np.array([self._get_idx(x) for x in x_vec])\n",
    "        y_idx = np.array([self._get_idx(y) for y in y_vec])\n",
    "        \n",
    "        for i in range(len(x_idx)):\n",
    "            root_x = self.find_vec(np.array([x_idx[i]]))[0]\n",
    "            root_y = self.find_vec(np.array([y_idx[i]]))[0]\n",
    "            \n",
    "            if root_x != root_y:\n",
    "                self._pending_x = np.append(self._pending_x, x_idx[i])\n",
    "                self._pending_y = np.append(self._pending_y, y_idx[i])\n",
    "                \n",
    "                if self.rank[root_x] < self.rank[root_y]:\n",
    "                    root_x, root_y = root_y, root_x\n",
    "                    \n",
    "                self.parent[root_y] = root_x\n",
    "                \n",
    "                if self.rank[root_x] == self.rank[root_y]:\n",
    "                    self.rank[root_x] += 1\n",
    "\n",
    "    def get_components(self, parent_array: Optional[np.ndarray] = None) -> list[set[int]]:\n",
    "        if parent_array is None:\n",
    "            parent_array = self.parent\n",
    "            \n",
    "        all_indices = np.arange(self.size)\n",
    "        roots = self.find_vec(all_indices, parent_array)\n",
    "        unique_roots = np.unique(roots)\n",
    "        \n",
    "        components = []\n",
    "        for root in unique_roots:\n",
    "            comp_indices = all_indices[roots == root]\n",
    "            if len(comp_indices) > 1:  # Only include non-singleton components\n",
    "                components.append({self.idx_to_id[i] for i in comp_indices})\n",
    "        return components\n",
    "\n",
    "    def diff(self) -> tuple[np.ndarray, np.ndarray]:\n",
    "        if len(self._pending_x) == 0:\n",
    "            if not hasattr(self, '_initialized_shadow'):\n",
    "                self._shadow_parent[:] = self.parent\n",
    "                self._shadow_rank[:] = self.rank\n",
    "                self._initialized_shadow = True\n",
    "            return np.array([], dtype=object), np.array([], dtype=object)\n",
    "        \n",
    "        # Get current final state\n",
    "        final_components = self.get_components()\n",
    "        changes = set()\n",
    "        \n",
    "        # Process pending pairs\n",
    "        all_indices = np.arange(self.size)\n",
    "        current_roots = self.find_vec(all_indices)\n",
    "        \n",
    "        for x_idx, y_idx in zip(self._pending_x, self._pending_y):\n",
    "            # Get the final component containing this pair\n",
    "            final_root = self.find_vec(np.array([x_idx]))[0]\n",
    "            final_comp = next(comp for comp in final_components \n",
    "                            if self.idx_to_id[x_idx] in comp)\n",
    "            \n",
    "            # Add the direct pair change\n",
    "            pair_comp = frozenset({self.idx_to_id[x_idx], self.idx_to_id[y_idx]})\n",
    "            if pair_comp != final_comp:\n",
    "                changes.add((pair_comp, frozenset(final_comp)))\n",
    "\n",
    "        # Process changes from old state\n",
    "        if hasattr(self, '_initialized_shadow'):\n",
    "            old_components = self.get_components(self._shadow_parent)\n",
    "            \n",
    "            # For each old component\n",
    "            for old_comp in old_components:\n",
    "                # Find which final component contains this old component\n",
    "                sample_elem = next(iter(old_comp))\n",
    "                sample_idx = self._get_idx(sample_elem)\n",
    "                final_root = self.find_vec(np.array([sample_idx]))[0]\n",
    "                final_comp = next(comp for comp in final_components \n",
    "                                if self.idx_to_id[sample_idx] in comp)\n",
    "                \n",
    "                if old_comp != final_comp:\n",
    "                    changes.add((frozenset(old_comp), frozenset(final_comp)))\n",
    "        \n",
    "        # Update shadow state and clear pending\n",
    "        self._shadow_parent[:] = self.parent\n",
    "        self._shadow_rank[:] = self.rank\n",
    "        self._initialized_shadow = True\n",
    "        self._pending_x = np.array([], dtype=np.int32)\n",
    "        self._pending_y = np.array([], dtype=np.int32)\n",
    "        \n",
    "        if not changes:\n",
    "            return np.array([], dtype=object), np.array([], dtype=object)\n",
    "        \n",
    "        # Convert changes to numpy arrays\n",
    "        changes_list = list(changes)\n",
    "        old_comps = np.array([np.array(list(old_comp)) for old_comp, _ in changes_list], dtype=object)\n",
    "        new_comps = np.array([np.array(list(new_comp)) for _, new_comp in changes_list], dtype=object)\n",
    "        \n",
    "        return old_comps, new_comps\n",
    "\n",
    "\n",
    "def component_to_hierarchy_pa(table: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Convert pairwise probabilities into a hierarchical representation.\n",
    "    Assumes data is pre-sorted by probability descending.\n",
    "    \n",
    "    Args:\n",
    "        key: Group key (ignored in this implementation)\n",
    "        table: Arrow Table with columns ['left', 'right', 'probability']\n",
    "    \n",
    "    Returns:\n",
    "        Arrow Table with columns ['parent', 'child', 'probability'] representing hierarchical merges\n",
    "    \"\"\"\n",
    "    hierarchy: list[tuple[int, int, float]] = []\n",
    "    uf = NumpyUnionFindWithDiff[int]()\n",
    "\n",
    "    # Get unique probabilities\n",
    "    probs = pc.unique(table['probability'])\n",
    "\n",
    "    for threshold in probs:\n",
    "        # Get current probability rows\n",
    "        mask = pc.equal(table['probability'], threshold)\n",
    "        current_probs = table.filter(mask)\n",
    "        threshold_float = float(threshold.as_py())\n",
    "        \n",
    "        # Process each row\n",
    "        left = current_probs['left'].to_numpy()\n",
    "        right = current_probs['right'].to_numpy()\n",
    "        uf.union_vectorized(left, right)\n",
    "        parent = vectorized_combine_integers([left, right])\n",
    "\n",
    "        hierarchy.extend(\n",
    "            list(zip(parent, left, [threshold_float] * len(parent))) +\n",
    "            list(zip(parent, right, [threshold_float] * len(parent)))\n",
    "        )\n",
    "        \n",
    "        # Process UnionFind diffs\n",
    "        old_comps, new_comps = uf.diff()\n",
    "        if len(old_comps) > 0:  # only if there are changes\n",
    "            singles = np.array([len(comp) == 1 for comp in old_comps])\n",
    "            multi_indices = ~singles\n",
    "            single_indices = singles\n",
    "\n",
    "            if multi_indices.any():\n",
    "                parents = vectorized_combine_integers(new_comps[multi_indices])\n",
    "                children = vectorized_combine_integers(old_comps[multi_indices])\n",
    "                hierarchy.extend(list(zip(parents, children, [threshold_float] * len(parents))))\n",
    "\n",
    "            if single_indices.any():\n",
    "                parents = vectorized_combine_integers(new_comps[single_indices])\n",
    "                children = old_comps[single_indices].ravel()  # automatically flattens single-element arrays\n",
    "                hierarchy.extend(list(zip(parents, children, [threshold_float] * len(parents))))\n",
    "\n",
    "    parents, children, probs = zip(*hierarchy)\n",
    "    return pa.table({\n",
    "        'parent': pa.array(parents, type=pa.int64()),\n",
    "        'child': pa.array(children, type=pa.int64()),\n",
    "        'probability': pa.array(probs, type=pa.float64())\n",
    "    })\n",
    "\n",
    "component_to_hierarchy_pa(cc_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1: Chain merge\n",
      "Dict version changes: [({1, 2}, {1, 2, 3}), ({2, 3}, {1, 2, 3})]\n",
      "Numpy version changes: [({2, 3}, {1, 2, 3}), ({1, 2}, {1, 2, 3})]\n",
      "Match: True\n",
      "\n",
      "Test Case 2: Star merge\n",
      "Dict version changes: [({1, 2}, {1, 2, 3, 4}), ({1, 3}, {1, 2, 3, 4}), ({1, 4}, {1, 2, 3, 4})]\n",
      "Numpy version changes: [({1, 2}, {1, 2, 3, 4}), ({1, 4}, {1, 2, 3, 4}), ({1, 3}, {1, 2, 3, 4})]\n",
      "Match: True\n",
      "\n",
      "Test Case 3: Merging existing components\n",
      "Dict version changes: [({2, 3}, {1, 2, 3, 4}), ({1, 2}, {1, 2, 3, 4}), ({3, 4}, {1, 2, 3, 4})]\n",
      "Numpy version changes: [({1, 2}, {1, 2, 3, 4}), ({3, 4}, {1, 2, 3, 4}), ({2, 3}, {1, 2, 3, 4})]\n",
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Set, List, Tuple\n",
    "\n",
    "# Helper function to convert numpy arrays to sets for comparison\n",
    "def numpy_components_to_sets(old_comps: np.ndarray, new_comps: np.ndarray) -> List[Tuple[Set[int], Set[int]]]:\n",
    "    if len(old_comps) == 0:\n",
    "        return []\n",
    "    return [(set(old.tolist()), set(new.tolist())) for old, new in zip(old_comps, new_comps)]\n",
    "\n",
    "def test_implementations():\n",
    "    # Initialize both implementations\n",
    "    dict_uf = UnionFindWithDiff()\n",
    "    numpy_uf = NumpyUnionFindWithDiff()\n",
    "    \n",
    "    # Test Case 1: Simple chain merge\n",
    "    print(\"Test Case 1: Chain merge\")\n",
    "    # Dict version\n",
    "    dict_uf = UnionFindWithDiff()\n",
    "    dict_uf.union(1, 2)\n",
    "    dict_uf.union(2, 3)\n",
    "    dict_changes1 = list(dict_uf.diff())\n",
    "    \n",
    "    # Numpy version\n",
    "    numpy_uf = NumpyUnionFindWithDiff()\n",
    "    numpy_uf.union_vectorized(np.array([1, 2]), np.array([2, 3]))\n",
    "    old_comps, new_comps = numpy_uf.diff()\n",
    "    numpy_changes1 = numpy_components_to_sets(old_comps, new_comps)\n",
    "    \n",
    "    print(\"Dict version changes:\", dict_changes1)\n",
    "    print(\"Numpy version changes:\", numpy_changes1)\n",
    "    print(\"Match:\", set(map(str, dict_changes1)) == set(map(str, numpy_changes1)))\n",
    "    print()\n",
    "    \n",
    "    # Test Case 2: Star merge\n",
    "    print(\"Test Case 2: Star merge\")\n",
    "    # Dict version\n",
    "    dict_uf = UnionFindWithDiff()\n",
    "    dict_uf.union(1, 2)\n",
    "    dict_uf.union(1, 3)\n",
    "    dict_uf.union(1, 4)\n",
    "    dict_changes2 = list(dict_uf.diff())\n",
    "    \n",
    "    # Numpy version\n",
    "    numpy_uf = NumpyUnionFindWithDiff()\n",
    "    numpy_uf.union_vectorized(np.array([1, 1, 1]), np.array([2, 3, 4]))\n",
    "    old_comps, new_comps = numpy_uf.diff()\n",
    "    numpy_changes2 = numpy_components_to_sets(old_comps, new_comps)\n",
    "    \n",
    "    print(\"Dict version changes:\", dict_changes2)\n",
    "    print(\"Numpy version changes:\", numpy_changes2)\n",
    "    print(\"Match:\", set(map(str, dict_changes2)) == set(map(str, numpy_changes2)))\n",
    "    print()\n",
    "    \n",
    "    # Test Case 3: Merging existing components\n",
    "    print(\"Test Case 3: Merging existing components\")\n",
    "    # Dict version\n",
    "    dict_uf = UnionFindWithDiff()\n",
    "    dict_uf.union(1, 2)\n",
    "    dict_uf.union(3, 4)\n",
    "    list(dict_uf.diff())  # Clear initial changes\n",
    "    dict_uf.union(2, 3)\n",
    "    dict_changes3 = list(dict_uf.diff())\n",
    "    \n",
    "    # Numpy version\n",
    "    numpy_uf = NumpyUnionFindWithDiff()\n",
    "    numpy_uf.union_vectorized(np.array([1, 3]), np.array([2, 4]))\n",
    "    numpy_uf.diff()  # Clear initial changes\n",
    "    numpy_uf.union_vectorized(np.array([2]), np.array([3]))\n",
    "    old_comps, new_comps = numpy_uf.diff()\n",
    "    numpy_changes3 = numpy_components_to_sets(old_comps, new_comps)\n",
    "    \n",
    "    print(\"Dict version changes:\", dict_changes3)\n",
    "    print(\"Numpy version changes:\", numpy_changes3)\n",
    "    print(\"Match:\", set(map(str, dict_changes3)) == set(map(str, numpy_changes3)))\n",
    "\n",
    "test_implementations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Size</th>\n",
       "      <th>Num_Pairs</th>\n",
       "      <th>Dict_Time</th>\n",
       "      <th>Numpy_Time</th>\n",
       "      <th>Dict_Std</th>\n",
       "      <th>Numpy_Std</th>\n",
       "      <th>Speedup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.002420</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.030523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.018620</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.036309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.002496</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.029668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.015933</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.036586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.027857</td>\n",
       "      <td>0.232283</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>0.016529</td>\n",
       "      <td>0.119926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.002457</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.029138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.014807</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>0.040527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.018490</td>\n",
       "      <td>0.185388</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.099738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Size  Num_Pairs  Dict_Time  Numpy_Time  Dict_Std  Numpy_Std   Speedup\n",
       "0    100         10   0.000074    0.002420  0.000010   0.000339  0.030523\n",
       "1    100        100   0.000676    0.018620  0.000165   0.000740  0.036309\n",
       "2   1000         10   0.000074    0.002496  0.000004   0.000256  0.029668\n",
       "3   1000        100   0.000583    0.015933  0.000005   0.001204  0.036586\n",
       "4   1000       1000   0.027857    0.232283  0.003230   0.016529  0.119926\n",
       "5  10000         10   0.000072    0.002457  0.000007   0.000193  0.029138\n",
       "6  10000        100   0.000600    0.014807  0.000026   0.000860  0.040527\n",
       "7  10000       1000   0.018490    0.185388  0.001771   0.001559  0.099738"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from typing import Tuple, List\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_random_pairs(size: int, num_pairs: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Generate random pairs of integers for testing\"\"\"\n",
    "    x = np.random.randint(0, size, num_pairs)\n",
    "    y = np.random.randint(0, size, num_pairs)\n",
    "    return x, y\n",
    "\n",
    "def benchmark_dict_version(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    \"\"\"Benchmark dictionary-based implementation\"\"\"\n",
    "    uf = UnionFindWithDiff()\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    for xi, yi in zip(x, y):\n",
    "        uf.union(int(xi), int(yi))\n",
    "    list(uf.diff())  # Materialize the changes\n",
    "    \n",
    "    return time.perf_counter() - start\n",
    "\n",
    "def benchmark_numpy_version(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    \"\"\"Benchmark numpy-based implementation\"\"\"\n",
    "    uf = NumpyUnionFindWithDiff()\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    uf.union_vectorized(x, y)\n",
    "    uf.diff()  # Get the changes\n",
    "    \n",
    "    return time.perf_counter() - start\n",
    "\n",
    "def run_benchmarks() -> pd.DataFrame:\n",
    "    \"\"\"Run benchmarks with different input sizes\"\"\"\n",
    "    # Test parameters\n",
    "    sizes = [100, 1000, 10000]\n",
    "    pairs_per_size = [10, 100, 1000]\n",
    "    num_trials = 5\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        for num_pairs in pairs_per_size:\n",
    "            if num_pairs > size:\n",
    "                continue\n",
    "                \n",
    "            dict_times = []\n",
    "            numpy_times = []\n",
    "            \n",
    "            for _ in range(num_trials):\n",
    "                # Generate random pairs\n",
    "                x, y = generate_random_pairs(size, num_pairs)\n",
    "                \n",
    "                # Run benchmarks\n",
    "                dict_time = benchmark_dict_version(x, y)\n",
    "                numpy_time = benchmark_numpy_version(x, y)\n",
    "                \n",
    "                dict_times.append(dict_time)\n",
    "                numpy_times.append(numpy_time)\n",
    "            \n",
    "            # Record average results\n",
    "            results.append({\n",
    "                'Size': size,\n",
    "                'Num_Pairs': num_pairs,\n",
    "                'Dict_Time': np.mean(dict_times),\n",
    "                'Numpy_Time': np.mean(numpy_times),\n",
    "                'Dict_Std': np.std(dict_times),\n",
    "                'Numpy_Std': np.std(numpy_times),\n",
    "                'Speedup': np.mean(dict_times) / np.mean(numpy_times)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "run_benchmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_15_pa = cc2.filter(pc.equal(cc2['component'], 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         26701 function calls in 0.033 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 54 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        2    0.000    0.000    0.033    0.016 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3541(run_code)\n",
      "        2    0.000    0.000    0.033    0.016 {built-in method builtins.exec}\n",
      "        1    0.006    0.006    0.033    0.033 /Users/willlangdale/DS/matchbox/notebooks/cluster.py:208(component_to_hierarchy_pa)\n",
      "      298    0.003    0.000    0.021    0.000 /Users/willlangdale/DS/matchbox/notebooks/cluster.py:107(diff)\n",
      "       61    0.007    0.000    0.017    0.000 /Users/willlangdale/DS/matchbox/notebooks/cluster.py:97(get_components)\n",
      "    11218    0.009    0.000    0.009    0.000 /Users/willlangdale/DS/matchbox/notebooks/cluster.py:62(find)\n",
      "      501    0.001    0.000    0.002    0.000 /Users/willlangdale/DS/matchbox/notebooks/cluster.py:77(union)\n",
      "    10483    0.002    0.000    0.002    0.000 {method 'add' of 'set' objects}\n",
      "       32    0.002    0.000    0.002    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/compute.py:239(wrapper)\n",
      "       31    0.001    0.000    0.001    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/compute.py:249(wrapper)\n",
      "      745    0.000    0.000    0.001    0.000 {built-in method builtins.next}\n",
      "      542    0.000    0.000    0.000    0.000 /Users/willlangdale/DS/matchbox/notebooks/cluster.py:150(<genexpr>)\n",
      "      398    0.000    0.000    0.000    0.000 /Users/willlangdale/DS/matchbox/notebooks/cluster.py:119(<genexpr>)\n",
      "      200    0.000    0.000    0.000    0.000 /Users/willlangdale/DS/matchbox/notebooks/cluster.py:57(make_set)\n",
      "      768    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}\n",
      "      601    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "       31    0.000    0.000    0.000    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/compute.py:216(_handle_options)\n",
      "       62    0.000    0.000    0.000    0.000 {method 'copy' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 /Users/willlangdale/.pyenv/versions/3.11.8/lib/python3.11/codeop.py:120(__call__)\n",
      "        4    0.000    0.000    0.000    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py:775(_clean_thread_parent_frames)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x127c54310>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "from pstats import SortKey\n",
    "\n",
    "from cluster import component_to_hierarchy_pa\n",
    "\n",
    "# Profile the function\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "_ = component_to_hierarchy_pa(cc_15_pa)\n",
    "pr.disable()\n",
    "\n",
    "# Print stats sorted by cumulative time\n",
    "ps = pstats.Stats(pr).sort_stats(SortKey.CUMULATIVE)\n",
    "ps.print_stats(20)  # Show top 20 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (501, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>left</th><th>right</th><th>parent</th></tr><tr><td>i32</td><td>i32</td><td>i32</td></tr></thead><tbody><tr><td>81762</td><td>389632</td><td>-1905962718</td></tr><tr><td>5063</td><td>289249</td><td>-507336890</td></tr><tr><td>84647</td><td>212390</td><td>-1629285124</td></tr><tr><td>175160</td><td>262073</td><td>-1975619425</td></tr><tr><td>178519</td><td>348267</td><td>-435576558</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>12792</td><td>227750</td><td>-428260364</td></tr><tr><td>71547</td><td>331214</td><td>-641775315</td></tr><tr><td>66430</td><td>329216</td><td>-1741711938</td></tr><tr><td>114817</td><td>358133</td><td>-1027235085</td></tr><tr><td>88893</td><td>202314</td><td>-1855621058</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (501, 3)\n",
       "┌────────┬────────┬─────────────┐\n",
       "│ left   ┆ right  ┆ parent      │\n",
       "│ ---    ┆ ---    ┆ ---         │\n",
       "│ i32    ┆ i32    ┆ i32         │\n",
       "╞════════╪════════╪═════════════╡\n",
       "│ 81762  ┆ 389632 ┆ -1905962718 │\n",
       "│ 5063   ┆ 289249 ┆ -507336890  │\n",
       "│ 84647  ┆ 212390 ┆ -1629285124 │\n",
       "│ 175160 ┆ 262073 ┆ -1975619425 │\n",
       "│ 178519 ┆ 348267 ┆ -435576558  │\n",
       "│ …      ┆ …      ┆ …           │\n",
       "│ 12792  ┆ 227750 ┆ -428260364  │\n",
       "│ 71547  ┆ 331214 ┆ -641775315  │\n",
       "│ 66430  ┆ 329216 ┆ -1741711938 │\n",
       "│ 114817 ┆ 358133 ┆ -1027235085 │\n",
       "│ 88893  ┆ 202314 ┆ -1855621058 │\n",
       "└────────┴────────┴─────────────┘"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = 2147483647\n",
    "\n",
    "cc_15.with_columns([\n",
    "    pl.min_horizontal(['left', 'right']).abs().alias('smaller'),\n",
    "    pl.max_horizontal(['left', 'right']).abs().alias('larger'),\n",
    "]).with_columns([\n",
    "    pl.col('smaller').add(pl.col('larger')).mod(P).alias('total2'),\n",
    "    pl.col('smaller').mul(pl.col('larger')).mod(P).alias('prod2'),\n",
    "]).with_columns([\n",
    "    pl.col('total2').mul(31).add(pl.col('prod2').mul(37)).mod(P).neg().alias('parent')\n",
    "]).select([\"left\", \"right\", \"parent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (501, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>map</th></tr><tr><td>i64</td></tr></thead><tbody><tr><td>-1905963266</td></tr><tr><td>-507336916</td></tr><tr><td>-1629285434</td></tr><tr><td>-1975620253</td></tr><tr><td>-435577630</td></tr><tr><td>&hellip;</td></tr><tr><td>-428260452</td></tr><tr><td>-641775761</td></tr><tr><td>-1741712314</td></tr><tr><td>-1027235831</td></tr><tr><td>-1855621368</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (501, 1)\n",
       "┌─────────────┐\n",
       "│ map         │\n",
       "│ ---         │\n",
       "│ i64         │\n",
       "╞═════════════╡\n",
       "│ -1905963266 │\n",
       "│ -507336916  │\n",
       "│ -1629285434 │\n",
       "│ -1975620253 │\n",
       "│ -435577630  │\n",
       "│ …           │\n",
       "│ -428260452  │\n",
       "│ -641775761  │\n",
       "│ -1741712314 │\n",
       "│ -1027235831 │\n",
       "│ -1855621368 │\n",
       "└─────────────┘"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_15.map_rows(lambda t: combine_integers(t[0], t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "def component_to_hierarchy_pl(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert pairwise probabilities into a hierarchical representation using Polars.\n",
    "    Assumes data is pre-sorted by probability descending.\n",
    "    \n",
    "    Args:\n",
    "        key: Group key (ignored in this implementation)\n",
    "        df: Polars DataFrame with columns ['left', 'right', 'probability']\n",
    "    \n",
    "    Returns:\n",
    "        Polars DataFrame with columns ['parent', 'child', 'probability'] representing hierarchical merges\n",
    "    \"\"\"\n",
    "    hierarchy: list[tuple[int, int, float]] = []\n",
    "    uf = UnionFindWithDiff[int]()\n",
    "\n",
    "    thresholds = df[\"probability\"].unique(maintain_order=True)\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        # Filter current probability rows and convert to Python objects for processing\n",
    "        current_probs = df.filter(pl.col('probability') == threshold).select([\"left\", \"right\", \"probability\"])\n",
    "        \n",
    "        # Process each row\n",
    "        for row in current_probs.iter_rows():\n",
    "            parent = combine_integers(row[0], row[1])\n",
    "            uf.union(row[0], row[1])\n",
    "            hierarchy.extend([\n",
    "                (parent, row[1], threshold),\n",
    "                (parent, row[0], threshold)\n",
    "            ])\n",
    "\n",
    "        # Process union-find differences\n",
    "        for old_comp, new_comp in uf.diff():\n",
    "            if len(old_comp) > 1:\n",
    "                parent = combine_integers(*new_comp)\n",
    "                child = combine_integers(*old_comp)\n",
    "                hierarchy.extend([\n",
    "                    (parent, child, threshold)\n",
    "                ])\n",
    "            else:\n",
    "                parent = combine_integers(*new_comp)\n",
    "                hierarchy.extend([\n",
    "                    (parent, old_comp.pop(), threshold)\n",
    "                ])\n",
    "\n",
    "    # Convert the results to a Polars DataFrame\n",
    "    return pl.DataFrame(\n",
    "        hierarchy,\n",
    "        schema={\n",
    "            'parent': pl.Int32,\n",
    "            'child': pl.Int32,\n",
    "            'probability': pl.UInt8\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (501, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>left</th><th>right</th><th>probability</th><th>component</th></tr><tr><td>i32</td><td>i32</td><td>u8</td><td>u32</td></tr></thead><tbody><tr><td>81762</td><td>389632</td><td>100</td><td>15</td></tr><tr><td>5063</td><td>289249</td><td>100</td><td>15</td></tr><tr><td>84647</td><td>212390</td><td>100</td><td>15</td></tr><tr><td>175160</td><td>262073</td><td>100</td><td>15</td></tr><tr><td>178519</td><td>348267</td><td>100</td><td>15</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>12792</td><td>227750</td><td>70</td><td>15</td></tr><tr><td>71547</td><td>331214</td><td>70</td><td>15</td></tr><tr><td>66430</td><td>329216</td><td>70</td><td>15</td></tr><tr><td>114817</td><td>358133</td><td>70</td><td>15</td></tr><tr><td>88893</td><td>202314</td><td>70</td><td>15</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (501, 4)\n",
       "┌────────┬────────┬─────────────┬───────────┐\n",
       "│ left   ┆ right  ┆ probability ┆ component │\n",
       "│ ---    ┆ ---    ┆ ---         ┆ ---       │\n",
       "│ i32    ┆ i32    ┆ u8          ┆ u32       │\n",
       "╞════════╪════════╪═════════════╪═══════════╡\n",
       "│ 81762  ┆ 389632 ┆ 100         ┆ 15        │\n",
       "│ 5063   ┆ 289249 ┆ 100         ┆ 15        │\n",
       "│ 84647  ┆ 212390 ┆ 100         ┆ 15        │\n",
       "│ 175160 ┆ 262073 ┆ 100         ┆ 15        │\n",
       "│ 178519 ┆ 348267 ┆ 100         ┆ 15        │\n",
       "│ …      ┆ …      ┆ …           ┆ …         │\n",
       "│ 12792  ┆ 227750 ┆ 70          ┆ 15        │\n",
       "│ 71547  ┆ 331214 ┆ 70          ┆ 15        │\n",
       "│ 66430  ┆ 329216 ┆ 70          ┆ 15        │\n",
       "│ 114817 ┆ 358133 ┆ 70          ┆ 15        │\n",
       "│ 88893  ┆ 202314 ┆ 70          ┆ 15        │\n",
       "└────────┴────────┴─────────────┴───────────┘"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "cc_15 = pl.from_arrow(cc2.filter(pc.equal(cc2['component'], 15)))\n",
    "cc_15 = cc_15.with_columns(\n",
    "    pl.col(\"probability\").cast(pl.Float32).mul(100).cast(pl.UInt8).alias(\"probability\"),\n",
    "    pl.col(\"left\").cast(pl.Int32),\n",
    "    pl.col(\"right\").cast(pl.Int32),\n",
    "    pl.col(\"component\").cast(pl.UInt32),\n",
    ")\n",
    "cc_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         32850 function calls in 0.040 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 196 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        2    0.000    0.000    0.040    0.020 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3541(run_code)\n",
      "        2    0.000    0.000    0.040    0.020 {built-in method builtins.exec}\n",
      "        1    0.004    0.004    0.040    0.040 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_72030/483903679.py:3(component_to_hierarchy_pl)\n",
      "      298    0.003    0.000    0.019    0.000 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_72030/807932433.py:65(diff)\n",
      "       61    0.008    0.000    0.015    0.000 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_72030/807932433.py:55(get_components)\n",
      "       31    0.000    0.000    0.007    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/polars/dataframe/frame.py:4614(filter)\n",
      "    11218    0.007    0.000    0.007    0.000 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_72030/807932433.py:20(find)\n",
      "       63    0.000    0.000    0.006    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/polars/lazyframe/frame.py:1824(collect)\n",
      "       63    0.005    0.000    0.005    0.000 {method 'collect' of 'builtins.PyLazyFrame' objects}\n",
      "       31    0.000    0.000    0.004    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/polars/dataframe/frame.py:8921(select)\n",
      "       31    0.000    0.000    0.003    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/polars/lazyframe/frame.py:3323(select)\n",
      "       63    0.000    0.000    0.002    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/polars/_utils/parse/expr.py:78(parse_into_list_of_expressions)\n",
      "      156    0.000    0.000    0.002    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/polars/_utils/parse/expr.py:20(parse_into_expression)\n",
      "      501    0.001    0.000    0.002    0.000 /var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_72030/807932433.py:35(union)\n",
      "       63    0.000    0.000    0.002    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/polars/_utils/parse/expr.py:108(_parse_positional_inputs)\n",
      "       31    0.000    0.000    0.002    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/polars/lazyframe/frame.py:3104(filter)\n",
      "    10483    0.002    0.000    0.002    0.000 {method 'add' of 'set' objects}\n",
      "      532    0.001    0.000    0.001    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/polars/dataframe/frame.py:10512(iter_rows)\n",
      "       63    0.000    0.000    0.001    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/polars/_utils/parse/expr.py:114(<listcomp>)\n",
      "       31    0.000    0.000    0.001    0.000 /Users/willlangdale/DS/matchbox/.venv/lib/python3.11/site-packages/polars/expr/expr.py:170(__eq__)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_72030/483903679.py:48: DataOrientationWarning: Row orientation inferred during DataFrame construction. Explicitly specify the orientation by passing `orient=\"row\"` to silence this warning.\n",
      "  return pl.DataFrame(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x126f54a10>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "from pstats import SortKey\n",
    "\n",
    "# Profile the function\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "_ = component_to_hierarchy_pl(cc_15)\n",
    "pr.disable()\n",
    "\n",
    "# Print stats sorted by cumulative time\n",
    "ps = pstats.Stats(pr).sort_stats(SortKey.CUMULATIVE)\n",
    "ps.print_stats(20)  # Show top 20 lines\n",
    "\n",
    "\n",
    "# Pure polars was 0.06, with iter_rows, 0.03-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running test case 1...\n",
      "✓ Test case 1 passed\n",
      "\n",
      "Running test case 2...\n",
      "✓ Test case 2 passed\n",
      "\n",
      "Running test case 3...\n",
      "✓ Test case 3 passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_72030/483903679.py:48: DataOrientationWarning: Row orientation inferred during DataFrame construction. Explicitly specify the orientation by passing `orient=\"row\"` to silence this warning.\n",
      "  return pl.DataFrame(\n",
      "/var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_72030/483903679.py:48: DataOrientationWarning: Row orientation inferred during DataFrame construction. Explicitly specify the orientation by passing `orient=\"row\"` to silence this warning.\n",
      "  return pl.DataFrame(\n",
      "/var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_72030/483903679.py:48: DataOrientationWarning: Row orientation inferred during DataFrame construction. Explicitly specify the orientation by passing `orient=\"row\"` to silence this warning.\n",
      "  return pl.DataFrame(\n"
     ]
    }
   ],
   "source": [
    "from unittest.mock import patch\n",
    "\n",
    "test_cases = [\n",
    "    # Test case 1: Equal probabilities\n",
    "    (\n",
    "        {\n",
    "            \"left\": [\"a\", \"b\", \"c\"],\n",
    "            \"right\": [\"b\", \"c\", \"d\"],\n",
    "            \"probability\": [1.0, 1.0, 1.0],\n",
    "        },\n",
    "        {\n",
    "            (\"ab\", \"a\", 1.0),\n",
    "            (\"ab\", \"b\", 1.0),\n",
    "            (\"bc\", \"b\", 1.0),\n",
    "            (\"bc\", \"c\", 1.0),\n",
    "            (\"cd\", \"c\", 1.0),\n",
    "            (\"cd\", \"d\", 1.0),\n",
    "            (\"abcd\", \"ab\", 1.0),\n",
    "            (\"abcd\", \"bc\", 1.0),\n",
    "            (\"abcd\", \"cd\", 1.0),\n",
    "        },\n",
    "    ),\n",
    "    # Test case 2: Asymmetric probabilities\n",
    "    (\n",
    "        {\n",
    "            \"left\": [\"w\", \"x\", \"y\"],\n",
    "            \"right\": [\"x\", \"y\", \"z\"],\n",
    "            \"probability\": [0.9, 0.85, 0.8],\n",
    "        },\n",
    "        {\n",
    "            (\"wx\", \"w\", 0.9),\n",
    "            (\"wx\", \"x\", 0.9),\n",
    "            (\"xy\", \"x\", 0.85),\n",
    "            (\"xy\", \"y\", 0.85),\n",
    "            (\"wxy\", \"wx\", 0.85),\n",
    "            (\"wxy\", \"xy\", 0.85),\n",
    "            (\"yz\", \"y\", 0.8),\n",
    "            (\"yz\", \"z\", 0.8),\n",
    "            (\"wxyz\", \"wxy\", 0.8),\n",
    "            (\"wxyz\", \"yz\", 0.8),\n",
    "        },\n",
    "    ),\n",
    "    # Test case 3: Single two-item component\n",
    "    (\n",
    "        {\n",
    "            \"left\": [\"x\"],\n",
    "            \"right\": [\"y\"],\n",
    "            \"probability\": [0.9],\n",
    "        },\n",
    "        {\n",
    "            (\"xy\", \"x\", 0.9),\n",
    "            (\"xy\", \"y\", 0.9),\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "for i, (prob_data, expected_relations) in enumerate(test_cases, 1):\n",
    "    print(f\"\\nRunning test case {i}...\")\n",
    "    \n",
    "    with patch('__main__.combine_integers', side_effect=combine_strings):\n",
    "        # Pandas\n",
    "        # probabilities = (\n",
    "        #     pd.DataFrame.from_dict(prob_data)\n",
    "        #     .assign(probability=lambda df: df['probability'].astype(float))\n",
    "        #     .sort_values(by=\"probability\", ascending=False)\n",
    "        # )\n",
    "        # hierarchy_true = (\n",
    "        #         pd.DataFrame.from_records(\n",
    "        #         list(expected_relations), \n",
    "        #         columns=[\"parent\", \"child\", \"probability\"]\n",
    "        #     )\n",
    "        #     .sort_values(by=[\"probability\", \"parent\", \"child\"], ascending=[False, True, True])\n",
    "        #     .dropna(how='all')\n",
    "        #     .reset_index(drop=True)\n",
    "        # )\n",
    "\n",
    "        # hierarchy = component_to_hierarchy(0, probabilities)\n",
    "\n",
    "        # hierarchy = hierarchy.sort_values(\n",
    "        #     by=[\"probability\", \"parent\", \"child\"],\n",
    "        #     ascending=[False, True, True]\n",
    "        # ).reset_index(drop=True)\n",
    "\n",
    "        # Arrow\n",
    "        # probabilities = (\n",
    "        #     pa.Table.from_pydict(prob_data)\n",
    "        #     .cast(pa.schema([\n",
    "        #         ('left', pa.string()),\n",
    "        #         ('right', pa.string()),\n",
    "        #         ('probability', pa.float64()),\n",
    "        #     ]))\n",
    "        #     .sort_by([('probability', 'descending')])\n",
    "        # )\n",
    "\n",
    "        # parents, children, probs = zip(*expected_relations)\n",
    "\n",
    "        # hierarchy_true = (\n",
    "        #     pa.table(\n",
    "        #         [parents, children, probs], \n",
    "        #         names=['parent', 'child', 'probability']\n",
    "        #     )\n",
    "        #     .sort_by([\n",
    "        #         ('probability', 'descending'),\n",
    "        #         ('parent', 'ascending'),\n",
    "        #         ('child', 'ascending')\n",
    "        #     ])\n",
    "        #     .filter(pc.is_valid(pc.field('parent')))\n",
    "        # )\n",
    "\n",
    "        # hierarchy = (\n",
    "        #     component_to_hierarchy_pa(probabilities)\n",
    "        #     .sort_by([\n",
    "        #         ('probability', 'descending'),\n",
    "        #         ('parent', 'ascending'),\n",
    "        #         ('child', 'ascending')\n",
    "        #     ])\n",
    "        # )\n",
    "\n",
    "        # Polars\n",
    "        probabilities = (\n",
    "            pl.DataFrame(prob_data)\n",
    "            .with_columns(\n",
    "                pl.col(\"probability\").cast(pl.Float32).mul(100).cast(pl.UInt8).alias(\"probability\"),\n",
    "                pl.col(\"left\").cast(pl.String),\n",
    "                pl.col(\"right\").cast(pl.String),\n",
    "            )\n",
    "            .sort(\"probability\", descending=True)\n",
    "        )\n",
    "\n",
    "        # Convert hierarchy_true DataFrame\n",
    "        hierarchy_true = (\n",
    "                pd.DataFrame.from_records(\n",
    "                list(expected_relations), \n",
    "                columns=[\"parent\", \"child\", \"probability\"]\n",
    "            )\n",
    "            .sort_values(by=[\"probability\", \"parent\", \"child\"], ascending=[False, True, True])\n",
    "            .dropna(how='all')\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # Assuming component_to_hierarchy() function exists and works with polars\n",
    "        hierarchy = component_to_hierarchy_pl(probabilities)\n",
    "\n",
    "        # Sort hierarchy, turning back to pandas\n",
    "        hierarchy = (\n",
    "            hierarchy\n",
    "            .with_columns(\n",
    "                pl.col(\"probability\").cast(pl.Float64).truediv(100).alias(\"probability\")\n",
    "            )\n",
    "            .to_pandas()\n",
    "            .sort_values(\n",
    "                by=[\"probability\", \"parent\", \"child\"],\n",
    "                ascending=[False, True, True]\n",
    "            )\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            assert hierarchy.equals(hierarchy_true)\n",
    "            print(f\"✓ Test case {i} passed\")\n",
    "        except AssertionError:\n",
    "            print(f\"✗ Test case {i} failed\")\n",
    "            print(\"\\nExpected DataFrame:\")\n",
    "            print(hierarchy_true)\n",
    "            print(\"\\nActual DataFrame:\")\n",
    "            print(hierarchy)\n",
    "            print(\"\\nDifferences:\")\n",
    "            if hierarchy.shape != hierarchy_true.shape:\n",
    "                print(f\"Shape mismatch: Expected {hierarchy_true.shape}, got {hierarchy.shape}\")\n",
    "            else:\n",
    "                # Show where values differ\n",
    "                differences = (hierarchy != hierarchy_true).any(axis=1)\n",
    "                if differences.any():\n",
    "                    print(\"\\nMismatched rows:\")\n",
    "                    print(\"Expected:\")\n",
    "                    print(hierarchy_true[differences])\n",
    "                    print(\"\\nGot:\")\n",
    "                    print(hierarchy[differences])\n",
    "            # Continue to next test case instead of raising\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all components in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "h2 = pq.read_table('hierarchical_cc2k.parquet')\n",
    "cc2 = attach_independent_components(h2)\n",
    "\n",
    "h200 = pq.read_table('hierarchical_cc200k.parquet')\n",
    "cc200 = attach_independent_components(h200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from multiprocessing import cpu_count\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "def setup_client(port: int = 8787) -> Client:\n",
    "    \"\"\"\n",
    "    Creates a Dask client optimized for local computation.\n",
    "    Returns the client object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = Client(f'tcp://localhost:{port}')\n",
    "    except Exception:\n",
    "        n_workers = cpu_count()\n",
    "        cluster = LocalCluster(\n",
    "            dashboard_address=f':{port}',\n",
    "            n_workers=n_workers,\n",
    "            threads_per_worker=1,  # One thread per worker for CPU-bound tasks\n",
    "            memory_limit='auto'    # Automatically determine memory limits\n",
    "        )\n",
    "        client = Client(cluster)\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m component_to_hierarchy\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_hierarchical_clusters\u001b[39m(client: \u001b[43mClient\u001b[49m, table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    Perform hierarchical clustering on an Arrow table of pairwise probabilities.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mto_pandas()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Client' is not defined"
     ]
    }
   ],
   "source": [
    "from cluster import component_to_hierarchy\n",
    "\n",
    "def to_hierarchical_clusters(client: Client, table: pa.Table) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform hierarchical clustering on an Arrow table of pairwise probabilities.\n",
    "    \"\"\"\n",
    "    pdf = table.to_pandas()\n",
    "    \n",
    "    ddf = dd.from_pandas(\n",
    "        pdf, \n",
    "        npartitions=len(client.cluster.worker_spec)\n",
    "    )\n",
    "    \n",
    "    result_ddf = ddf.groupby('component').apply(\n",
    "        component_to_hierarchy, \n",
    "        meta={\n",
    "            'parent': int, \n",
    "            'child': int, \n",
    "            'probability': float\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return result_ddf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msetup_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8787\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m client:\n\u001b[1;32m      2\u001b[0m     h_out \u001b[38;5;241m=\u001b[39m to_hierarchical_clusters(client, cc2)\n\u001b[1;32m      4\u001b[0m h_out\n",
      "Cell \u001b[0;32mIn[15], line 13\u001b[0m, in \u001b[0;36msetup_client\u001b[0;34m(port)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mCreates a Dask client optimized for local computation.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03mReturns the client object.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     client \u001b[38;5;241m=\u001b[39m \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtcp://localhost:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mport\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     n_workers \u001b[38;5;241m=\u001b[39m cpu_count()\n",
      "File \u001b[0;32m~/DS/matchbox/.venv/lib/python3.11/site-packages/distributed/client.py:1231\u001b[0m, in \u001b[0;36mClient.__init__\u001b[0;34m(self, address, loop, timeout, set_as_default, scheduler_file, security, asynchronous, name, heartbeat_interval, serializers, deserializers, extensions, direct_to_workers, connection_limit, **kwargs)\u001b[0m\n\u001b[1;32m   1228\u001b[0m preload_argv \u001b[38;5;241m=\u001b[39m dask\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistributed.client.preload-argv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreloads \u001b[38;5;241m=\u001b[39m preloading\u001b[38;5;241m.\u001b[39mprocess_preloads(\u001b[38;5;28mself\u001b[39m, preload, preload_argv)\n\u001b[0;32m-> 1231\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1232\u001b[0m Client\u001b[38;5;241m.\u001b[39m_instances\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecreate_tasks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReplayTaskClient\n",
      "File \u001b[0;32m~/DS/matchbox/.venv/lib/python3.11/site-packages/distributed/client.py:1433\u001b[0m, in \u001b[0;36mClient.start\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1431\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_started \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m   1432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1433\u001b[0m     \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DS/matchbox/.venv/lib/python3.11/site-packages/distributed/utils.py:436\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m e\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[0;32m--> 436\u001b[0m         \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "File \u001b[0;32m~/DS/matchbox/.venv/lib/python3.11/site-packages/distributed/utils.py:425\u001b[0m, in \u001b[0;36msync.<locals>.wait\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 425\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    427\u001b[0m         loop\u001b[38;5;241m.\u001b[39madd_callback(cancel)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with setup_client(port=8787) as client:\n",
    "    h_out = to_hierarchical_clusters(client, cc2)\n",
    "\n",
    "h_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'pyarrow.lib.Table' object has no attribute 'partition_by'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcc2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartition_by\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomponent\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'pyarrow.lib.Table' object has no attribute 'partition_by'"
     ]
    }
   ],
   "source": [
    "for i in cc2.partitioning(\"component\"):\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing\n",
    "from cluster import component_to_hierarchy_pa\n",
    "import logging\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "\n",
    "# def to_hierarchical_clusters_pa(table: pa.Table) -> pa.Table:\n",
    "#     \"\"\"\n",
    "#     Parallel processing of components using Arrow with optimized submission.\n",
    "#     \"\"\"\n",
    "#     logging.info(f\"Starting processing with table size: {len(table)}\")\n",
    "#     components = pc.unique(table['component'])\n",
    "#     logging.info(f\"Found {len(components)} unique components\")\n",
    "#     n_cores = multiprocessing.cpu_count() + 4\n",
    "    \n",
    "#     # Pre-filter all components at once\n",
    "#     component_tables = []\n",
    "#     t_start = time.time()\n",
    "#     for comp in components:\n",
    "#         mask = pc.equal(table[\"component\"], comp)\n",
    "#         component_tables.append(table.filter(mask))\n",
    "#     logging.info(f\"Bulk filtering took {time.time() - t_start:.2f}s\")\n",
    "    \n",
    "#     results = []\n",
    "#     with ProcessPoolExecutor(max_workers=n_cores) as executor:\n",
    "#         # Submit all tasks at once\n",
    "#         t_start = time.time()\n",
    "#         futures = [executor.submit(component_to_hierarchy_pa, component_table) \n",
    "#                   for component_table in component_tables]\n",
    "#         logging.info(f\"Bulk submission took {time.time() - t_start:.2f}s\")\n",
    "        \n",
    "#         # Process results as they complete\n",
    "#         for i, future in enumerate(futures):\n",
    "#             t_start = time.time()\n",
    "#             results.append(future.result())\n",
    "#             logging.info(f\"Component {i}: processing took {time.time() - t_start:.2f}s\")\n",
    "\n",
    "#     return pa.concat_tables(results) if results else pa.table({\n",
    "#         'parent': pa.array([], type=pa.int64()),\n",
    "#         'child': pa.array([], type=pa.int64()),\n",
    "#         'probability': pa.array([], type=pa.float64())\n",
    "#     })\n",
    "\n",
    "\n",
    "# def to_hierarchical_clusters_pa(table: pa.Table) -> pa.Table:\n",
    "#     \"\"\"\n",
    "#     Parallel processing of components using Arrow with optimized submission.\n",
    "#     \"\"\"\n",
    "#     components = pc.unique(table['component'])\n",
    "#     n_cores = multiprocessing.cpu_count() + 4\n",
    "    \n",
    "#     component_tables = []\n",
    "#     for comp in components:\n",
    "#         mask = pc.equal(table[\"component\"], comp)\n",
    "#         component_tables.append(table.filter(mask))\n",
    "    \n",
    "#     results = []\n",
    "#     with ProcessPoolExecutor(max_workers=n_cores) as executor:\n",
    "#         futures = [\n",
    "#             executor.submit(component_to_hierarchy_pa, component_table)\n",
    "#             for component_table in component_tables\n",
    "#         ]\n",
    "        \n",
    "#         for future in futures:\n",
    "#             results.append(future.result())\n",
    "\n",
    "#     return pa.concat_tables(results) if results else pa.table({\n",
    "#         'parent': pa.array([], type=pa.int64()),\n",
    "#         'child': pa.array([], type=pa.int64()),\n",
    "#         'probability': pa.array([], type=pa.float64())\n",
    "#     })\n",
    "\n",
    "# def split_table(table: pa.Table) -> list[pa.Table]:\n",
    "#     \"\"\"\n",
    "#     Split a table into component chunks efficiently.\n",
    "#     \"\"\"\n",
    "#     sorted_table = table.sort_by(\"component\")\n",
    "\n",
    "#     components = []\n",
    "#     current_component = None\n",
    "#     start_idx = 0\n",
    "\n",
    "#     component_array = sorted_table.column(\"component\").to_numpy()\n",
    "    \n",
    "#     for idx, component in enumerate(component_array):\n",
    "#         if component != current_component:\n",
    "#             if current_component is not None:\n",
    "#                 components.append(sorted_table.slice(start_idx, idx - start_idx).combine_chunks())\n",
    "#             current_component = component\n",
    "#             start_idx = idx\n",
    "    \n",
    "#     if current_component is not None:\n",
    "#         components.append(sorted_table.slice(start_idx, len(component_array) - start_idx).combine_chunks())\n",
    "    \n",
    "#     return components\n",
    "\n",
    "def to_hierarchical_clusters_pa(table: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Parallel processing of components using Arrow with optimized submission and progress tracking.\n",
    "    \"\"\"\n",
    "    # Set up logging\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    table = table.sort_by([(\"component\", \"ascending\")])\n",
    "    components = pc.unique(table['component'])\n",
    "    n_cores = multiprocessing.cpu_count() + 4\n",
    "    n_components = len(components)\n",
    "    \n",
    "    logging.info(f\"Processing {n_components} components using {n_cores} workers\")\n",
    "\n",
    "    # 1) Filtering (2k @ 11s) \n",
    "    component_tables = []\n",
    "    for comp in tqdm(components, desc=\"Preparing components\", leave=False):\n",
    "        mask = pc.equal(table[\"component\"], comp)\n",
    "        component_tables.append(table.filter(mask))\n",
    "\n",
    "    # 2) Sort slicing (2k in like 10 minutes)\n",
    "    # component_tables = split_table(table)\n",
    "    \n",
    "    results = []\n",
    "    with ProcessPoolExecutor(max_workers=n_cores) as executor:\n",
    "        # Submit all tasks\n",
    "        futures = [\n",
    "            executor.submit(component_to_hierarchy_pa, component_table)\n",
    "            for component_table in component_tables\n",
    "        ]\n",
    "        \n",
    "        # Process results with progress bar\n",
    "        for future in tqdm(futures, desc=\"Processing components\", total=len(futures)):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing component: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    logging.info(f\"Completed processing {len(results)} components successfully\")\n",
    "    \n",
    "    # Create empty table if no results\n",
    "    if not results:\n",
    "        logging.warning(\"No results to concatenate\")\n",
    "        return pa.table({\n",
    "            'parent': pa.array([], type=pa.int64()),\n",
    "            'child': pa.array([], type=pa.int64()),\n",
    "            'probability': pa.array([], type=pa.float64())\n",
    "        })\n",
    "\n",
    "    return pa.concat_tables(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "left: int64\n",
       "right: int64\n",
       "probability: decimal128(3, 2)\n",
       "component: int64\n",
       "----\n",
       "left: [[197413,116407,114551,160857,6412,...,39429,156175,197197,48177,121674]]\n",
       "right: [[326505,384163,344025,248700,258884,...,311452,278755,204144,357956,378111]]\n",
       "probability: [[1.00,1.00,1.00,1.00,1.00,...,0.70,0.70,0.70,0.70,0.70]]\n",
       "component: [[0,0,0,0,0,...,2079,2079,2079,2079,2079]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear existing handlers\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 14:01:07,543 - INFO - Processing 2080 components using 20 workers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f94be190dfa4392bbdf5c03f067474b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing components:   0%|          | 0/2080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03fb5beee3c4cb9923d13b450405bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing components:   0%|          | 0/2080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 14:01:19,547 - INFO - Completed processing 2080 components successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "parent: int64\n",
       "child: int64\n",
       "probability: double\n",
       "----\n",
       "parent: [[-1193661193,-1193661193,-1065816097,-1065816097,-2131390865,...,-1988605072,-384900360,-384900360,-1450541918,-1450541918],[-1998262778,-1998262778,-352384079,-352384079,-1971847649,...,-1780983856,-1515770754,-1515770754,-1282464668,-1282464668],...,[-549228011,-549228011,-785366439,-785366439,-1013741990,...,-1759852711,-1842716946,-1842716946,-1756929996,-1756929996],[-75333889,-75333889,-1183850366,-1183850366,-95546434,...,-1299895816,-284056808,-284056808,-1440724029,-1440724029]]\n",
       "child: [[197413,326505,116407,384163,114551,...,212599,19131,294811,193341,224447],[64284,340314,170279,378400,152470,...,246051,14448,296071,85173,322723],...,[72846,345192,90867,360477,2449,...,324026,51072,220303,113552,257515],[168741,288248,29190,339106,179769,...,204144,48177,357956,121674,378111]]\n",
       "probability: [[1,1,1,1,1,...,0.7,0.7,0.7,0.7,0.7],[1,1,1,1,1,...,0.7,0.7,0.7,0.7,0.7],...,[1,1,1,1,1,...,0.7,0.7,0.7,0.7,0.7],[1,1,1,1,1,...,0.7,0.7,0.7,0.7,0.7]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_out = to_hierarchical_clusters_pa(cc2)\n",
    "\n",
    "h_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 14:01:51,445 - INFO - Processing 206763 components using 20 workers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f743aea82c404253a106d4501b8767ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing components:   0%|          | 0/206763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m h_out \u001b[38;5;241m=\u001b[39m \u001b[43mto_hierarchical_clusters_pa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcc200\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m h_out\n",
      "Cell \u001b[0;32mIn[29], line 118\u001b[0m, in \u001b[0;36mto_hierarchical_clusters_pa\u001b[0;34m(table)\u001b[0m\n\u001b[1;32m    116\u001b[0m component_tables \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m comp \u001b[38;5;129;01min\u001b[39;00m tqdm(components, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreparing components\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 118\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[43mpc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mequal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomponent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     component_tables\u001b[38;5;241m.\u001b[39mappend(table\u001b[38;5;241m.\u001b[39mfilter(mask))\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# 2) Sort slicing (2k in like 10 minutes)\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# component_tables = split_table(table)\u001b[39;00m\n",
      "File \u001b[0;32m~/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/compute.py:247\u001b[0m, in \u001b[0;36m_make_generic_wrapper.<locals>.wrapper\u001b[0;34m(memory_pool, *args)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], Expression):\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Expression\u001b[38;5;241m.\u001b[39m_call(func_name, \u001b[38;5;28mlist\u001b[39m(args))\n\u001b[0;32m--> 247\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_pool\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "h_out = to_hierarchical_clusters_pa(cc200)\n",
    "\n",
    "h_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_72030/1496664110.py:48: DataOrientationWarning: Row orientation inferred during DataFrame construction. Explicitly specify the orientation by passing `orient=\"row\"` to silence this warning.\n",
      "  return pl.DataFrame(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2_533_537, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>parent</th><th>child</th><th>probability</th></tr><tr><td>i32</td><td>i32</td><td>u8</td></tr></thead><tbody><tr><td>-283593755</td><td>337091</td><td>100</td></tr><tr><td>-283593755</td><td>15518</td><td>100</td></tr><tr><td>-126938103</td><td>378438</td><td>100</td></tr><tr><td>-126938103</td><td>41264</td><td>100</td></tr><tr><td>-1702119885</td><td>234149</td><td>100</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>-1028611184</td><td>182587</td><td>70</td></tr><tr><td>-1183984842</td><td>323264</td><td>70</td></tr><tr><td>-1183984842</td><td>13923</td><td>70</td></tr><tr><td>-1046343640</td><td>268429</td><td>70</td></tr><tr><td>-1046343640</td><td>42700</td><td>70</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2_533_537, 3)\n",
       "┌─────────────┬────────┬─────────────┐\n",
       "│ parent      ┆ child  ┆ probability │\n",
       "│ ---         ┆ ---    ┆ ---         │\n",
       "│ i32         ┆ i32    ┆ u8          │\n",
       "╞═════════════╪════════╪═════════════╡\n",
       "│ -283593755  ┆ 337091 ┆ 100         │\n",
       "│ -283593755  ┆ 15518  ┆ 100         │\n",
       "│ -126938103  ┆ 378438 ┆ 100         │\n",
       "│ -126938103  ┆ 41264  ┆ 100         │\n",
       "│ -1702119885 ┆ 234149 ┆ 100         │\n",
       "│ …           ┆ …      ┆ …           │\n",
       "│ -1028611184 ┆ 182587 ┆ 70          │\n",
       "│ -1183984842 ┆ 323264 ┆ 70          │\n",
       "│ -1183984842 ┆ 13923  ┆ 70          │\n",
       "│ -1046343640 ┆ 268429 ┆ 70          │\n",
       "│ -1046343640 ┆ 42700  ┆ 70          │\n",
       "└─────────────┴────────┴─────────────┘"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "def to_hierarchical_clusters_pl(table: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Parallel processing of components using Arrow.\n",
    "    \"\"\"\n",
    "    table_pl = (\n",
    "        pl.from_arrow(table)\n",
    "        .with_columns(\n",
    "            pl.col(\"probability\").cast(pl.Float32).mul(100).cast(pl.UInt8).alias(\"probability\"),\n",
    "            pl.col(\"left\").cast(pl.Int32),\n",
    "            pl.col(\"right\").cast(pl.Int32),\n",
    "            pl.col(\"component\").cast(pl.UInt32),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return table_pl.group_by(\"component\").map_groups(component_to_hierarchy_pl)\n",
    "\n",
    "to_hierarchical_clusters_pl(cc2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
