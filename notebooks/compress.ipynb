{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compression notes\n",
    "\n",
    "* 100m SHA-1 hashes, binary, binary, binary, float\n",
    "    * Standard: 7.6Gb, written in 23 seconds\n",
    "    * ZSTD 8: 6.85Gb, written in 3 mins\n",
    "    * ZSTD 15: 6.85Gb, written in 13 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import sha1\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import random\n",
    "\n",
    "def fast_generate_hashes(n: int = int(2e7)) -> pa.Array:\n",
    "    \"\"\"Generate n SHA1 hashes using PyArrow arrays.\"\"\"\n",
    "    rand = random.randint(0, int(1e5))\n",
    "    hashes = [\n",
    "        sha1((i + rand).to_bytes(8, 'big')).digest() \n",
    "        for i in range(int(n))\n",
    "    ]\n",
    "    return pa.array(hashes, type=pa.binary())\n",
    "\n",
    "def fast_sample_pairs(hashes: pa.Array, n: int = int(1e8)) -> pa.Table:\n",
    "    \"\"\"Generate hash pairs with random new hashes.\"\"\"\n",
    "    hash_count = len(hashes)\n",
    "    \n",
    "    # Generate indices\n",
    "    left = np.random.randint(0, hash_count, n)\n",
    "    right = np.random.randint(0, hash_count - 1, n)\n",
    "    right += (right >= left)\n",
    "    \n",
    "    # Take values using PyArrow\n",
    "    left_hashes = hashes.take(pa.array(left))\n",
    "    right_hashes = hashes.take(pa.array(right))\n",
    "    \n",
    "    # Generate probabilities as PyArrow array (between 0.7 and 1.0 to 2 DP)\n",
    "    probs = pa.array(np.round(0.7 + 0.3 * np.random.random(n), 2), type=pa.float64())\n",
    "    \n",
    "    # Generate completely new random hashes instead of combining\n",
    "    new_hashes = [sha1(i.to_bytes(8, 'big')).digest() \n",
    "                  for i in range(n)]\n",
    "    combined_arr = pa.array(new_hashes, type=pa.binary())\n",
    "    \n",
    "    # Create table directly with PyArrow\n",
    "    return pa.table({\n",
    "        'hash': combined_arr,\n",
    "        'left': left_hashes,\n",
    "        'right': right_hashes,\n",
    "        'probability': probs\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rustworkx as rx\n",
    "from matchbox.common.hash import list_to_value_ordered_hash\n",
    "\n",
    "def to_clusters(results: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Converts probabilities into a list of connected components formed at each threshold.\n",
    "\n",
    "    Returns:\n",
    "        Probabilities sorted by threshold descending.\n",
    "    \"\"\"\n",
    "    G = rx.PyGraph()\n",
    "    added: dict[bytes, int] = {}\n",
    "    components: dict[str, list] = {\"parent\": [], \"child\": [], \"threshold\": []}\n",
    "\n",
    "    # Sort probabilities descending and group by probability\n",
    "    edges_df = results.select(['left', 'right', 'probability']).sort_by([(\"probability\", \"descending\")])\n",
    "    \n",
    "    # Get unique probability thresholds, sorted\n",
    "    thresholds = pa.compute.unique(edges_df.column('probability'))\n",
    "\n",
    "    # Process edges grouped by probability threshold\n",
    "    for prob in thresholds.to_pylist():\n",
    "        mask = pa.compute.equal(edges_df.column('probability'), prob)\n",
    "        threshold_edges = edges_df.filter(mask)\n",
    "        # Get state before adding this batch of edges\n",
    "        old_components = {frozenset(comp) for comp in rx.connected_components(G)}\n",
    "\n",
    "        # Add all nodes and edges at this probability threshold\n",
    "        edge_values = zip(\n",
    "            threshold_edges.column('left').to_pylist(),\n",
    "            threshold_edges.column('right').to_pylist()\n",
    "        )\n",
    "\n",
    "        for left, right in edge_values:\n",
    "            for hash_val in (left, right):\n",
    "                if hash_val not in added:\n",
    "                    idx = G.add_node(hash_val)\n",
    "                    added[hash_val] = idx\n",
    "\n",
    "            G.add_edge(added[left], added[right], None)\n",
    "\n",
    "        new_components = {frozenset(comp) for comp in rx.connected_components(G)}\n",
    "        changed_components = new_components - old_components\n",
    "\n",
    "        # For each changed component, add ALL members at current threshold\n",
    "        for comp in changed_components:\n",
    "            children = sorted([G.get_node_data(n) for n in comp])\n",
    "            parent = list_to_value_ordered_hash(children)\n",
    "\n",
    "            components[\"parent\"].extend([parent] * len(children))\n",
    "            components[\"child\"].extend(children)\n",
    "            components[\"threshold\"].extend([prob] * len(children))\n",
    "\n",
    "    return pa.Table.from_pydict(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from collections import defaultdict\n",
    "\n",
    "def _cluster_results_to_hierarchical_pa(\n",
    "    probabilities: pa.Table,\n",
    "    clusters: pa.Table,\n",
    ") -> pa.Table:\n",
    "    \"\"\"\n",
    "    Converts results to a hierarchical structure by processing thresholds sequentially,\n",
    "    maintaining ultimate parent tracking to avoid graph traversal.\n",
    "    \n",
    "    Args:\n",
    "        probabilities: Original pairwise probabilities containing base components\n",
    "        clusters: Connected components at each threshold\n",
    "        \n",
    "    Returns:\n",
    "        PyArrow Table with schema:\n",
    "            parent: binary\n",
    "            child: binary\n",
    "            threshold: double\n",
    "    \"\"\"\n",
    "    # Sort thresholds in descending order\n",
    "    thresholds = pa.compute.unique(clusters['threshold']).sort(order='descending')\n",
    "    \n",
    "    # Initialize data structures\n",
    "    hierarchy: list[tuple[bytes, bytes, float]] = []\n",
    "    ultimate_parents: dict[bytes, set[bytes]] = defaultdict(set)\n",
    "    \n",
    "    # Process each threshold level\n",
    "    for threshold in thresholds:\n",
    "        threshold_float = float(threshold.as_py())\n",
    "        \n",
    "        # Filter and process pairwise probabilities at this threshold\n",
    "        prob_mask = pa.compute.equal(probabilities['probability'], threshold)\n",
    "        current_probs = probabilities.filter(prob_mask)\n",
    "        \n",
    "        # Add new pairwise relationships at this threshold\n",
    "        for batch in current_probs.to_batches():\n",
    "            parent_array = batch.column('hash')\n",
    "            left_array = batch.column('left')\n",
    "            right_array = batch.column('right')\n",
    "            \n",
    "            for i in range(len(batch)):\n",
    "                parent = parent_array[i].as_py()\n",
    "                left_id = left_array[i].as_py()\n",
    "                right_id = right_array[i].as_py()\n",
    "                \n",
    "                # Add to hierarchy\n",
    "                hierarchy.extend([\n",
    "                    (parent, left_id, threshold_float),\n",
    "                    (parent, right_id, threshold_float)\n",
    "                ])\n",
    "                \n",
    "                # Update ultimate parents\n",
    "                ultimate_parents[left_id].add(parent)\n",
    "                ultimate_parents[right_id].add(parent)\n",
    "        \n",
    "        # Process clusters at this threshold\n",
    "        cluster_mask = pa.compute.equal(clusters['threshold'], threshold)\n",
    "        current_clusters = clusters.filter(cluster_mask)\n",
    "        \n",
    "        # Group by parent to process components together\n",
    "        for batch in current_clusters.to_batches():\n",
    "            parent_col = batch.column('parent')\n",
    "            child_col = batch.column('child')\n",
    "            \n",
    "            parent_groups: dict[bytes, set[bytes]] = defaultdict(set)\n",
    "            for i in range(len(batch)):\n",
    "                parent = parent_col[i].as_py()\n",
    "                child = child_col[i].as_py()\n",
    "                parent_groups[parent].add(child)\n",
    "            \n",
    "            # Process each component\n",
    "            for new_parent, children in parent_groups.items():\n",
    "                if len(children) <= 2:\n",
    "                    continue  # Skip pairs already handled by pairwise probabilities\n",
    "                \n",
    "                # Collect all current ultimate parents for children in this component\n",
    "                current_ultimate_parents: set[bytes] = set()\n",
    "                for child in children:\n",
    "                    current_ultimate_parents.update(ultimate_parents[child])\n",
    "                \n",
    "                # Add edges from ultimate parents to new parent\n",
    "                for up in current_ultimate_parents:\n",
    "                    hierarchy.append((new_parent, up, threshold_float))\n",
    "                \n",
    "                # Update ultimate parents for all children in the component\n",
    "                for child in children:\n",
    "                    ultimate_parents[child] = {new_parent}\n",
    "    \n",
    "    # Sort hierarchy by threshold (descending), then parent, then child\n",
    "    hierarchy.sort(key=lambda x: (x[2], x[0], x[1]), reverse=True)\n",
    "    \n",
    "    # Convert to PyArrow Table\n",
    "    return pa.Table.from_arrays(\n",
    "        [\n",
    "            pa.array([h[0] for h in hierarchy], type=pa.binary()),\n",
    "            pa.array([h[1] for h in hierarchy], type=pa.binary()),\n",
    "            pa.array([h[2] for h in hierarchy], type=pa.float64())\n",
    "        ],\n",
    "        names=['parent', 'child', 'threshold']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "\n",
    "def create_test_tables(\n",
    "    size: int,\n",
    "    name: bytes,\n",
    "    id_gen: Generator[str, None, None]\n",
    ") -> tuple[pa.Table, pa.Table]:\n",
    "    \"\"\"\n",
    "    Create two Arrow tables: a standard version and an update version with controlled changes.\n",
    "    The update version maintains some hashes, reassigns some IDs, and adds new entries.\n",
    "    \n",
    "    Args:\n",
    "        size: Number of rows in each table\n",
    "        name: Dataset name as bytes (will be used as prefix for both versions)\n",
    "        id_gen: Generator that yields string IDs\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[pa.Table, pa.Table]: Standard and update tables\n",
    "    \"\"\"\n",
    "    # Generate standard table data\n",
    "    standard_hashes = fast_generate_hashes(size)\n",
    "    standard_ids = [[str(next(id_gen))] for _ in range(size)]\n",
    "    \n",
    "    # Create standard table\n",
    "    standard_ids_nested = pa.array(standard_ids, type=pa.list_(pa.string()))\n",
    "    standard_dataset = pa.array([name] * len(standard_hashes), type=pa.binary())\n",
    "    \n",
    "    standard_table = pa.Table.from_arrays(\n",
    "        [standard_hashes, standard_dataset, standard_ids_nested],\n",
    "        names=['hash', 'dataset', 'id']\n",
    "    )\n",
    "\n",
    "    # Calculate sizes for different types of changes\n",
    "    keep_count = int(size * 0.3)  # 30% remain same\n",
    "    reuse_count = int(size * 0.2)  # 20% same hash, new ID\n",
    "    move_count = int(size * 0.2)   # 20% same ID, new hash\n",
    "    new_count = size - (keep_count + reuse_count + move_count)  # 30% new entries\n",
    "\n",
    "    # Build update version\n",
    "    update_hashes = []\n",
    "    update_ids = []\n",
    "\n",
    "    # 1. Keep some entries exactly the same\n",
    "    for i in range(keep_count):\n",
    "        update_hashes.append(standard_hashes[i])\n",
    "        update_ids.append(standard_ids[i])\n",
    "    \n",
    "    # 2. Same hash, new IDs\n",
    "    for i in range(keep_count, keep_count + reuse_count):\n",
    "        update_hashes.append(standard_hashes[i])\n",
    "        update_ids.append(\n",
    "            [str(next(id_gen))]\n",
    "        )\n",
    "    \n",
    "    # 3. Same IDs, new hashes\n",
    "    new_hashes = fast_generate_hashes(move_count)\n",
    "    for i, hash in enumerate(new_hashes):\n",
    "        idx = keep_count + reuse_count + i\n",
    "        update_hashes.append(hash)\n",
    "        update_ids.append(standard_ids[idx])\n",
    "    \n",
    "    # 4. Completely new entries\n",
    "    final_new_hashes = fast_generate_hashes(new_count)\n",
    "    for hash in final_new_hashes:\n",
    "        update_hashes.append(hash)\n",
    "        update_ids.append(\n",
    "            [str(next(id_gen))]\n",
    "        )\n",
    "\n",
    "    # Create update table\n",
    "    update_ids_nested = pa.array(update_ids, type=pa.list_(pa.string()))\n",
    "    update_dataset = pa.array([name] * len(update_hashes), type=pa.binary())\n",
    "\n",
    "    update_table = pa.Table.from_arrays(\n",
    "        [pa.array(update_hashes), update_dataset, update_ids_nested],\n",
    "        names=['hash', 'dataset', 'id']\n",
    "    )\n",
    "\n",
    "    return standard_table, update_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_consistent_pairs(\n",
    "   standard_left: pa.Array,\n",
    "   standard_right: pa.Array,\n",
    "   update_left: pa.Array, \n",
    "   update_right: pa.Array,\n",
    "   n: int = int(1e8)\n",
    ") -> tuple[pa.Table, pa.Table]:\n",
    "   \"\"\"Generate partially consistent hash pairs between standard and update tables.\"\"\"\n",
    "   # Calculate sizes and adjust n\n",
    "   standard_total = len(standard_left) * len(standard_right)\n",
    "   update_total = len(update_left) * len(update_right)\n",
    "   n = min(n, min(standard_total, update_total))\n",
    "   \n",
    "   # Generate random permutation of just the first n indices instead of all possible indices\n",
    "   standard_flat = np.random.permutation(n)\n",
    "   # Scale to full range \n",
    "   standard_flat = (standard_flat * (standard_total / n)).astype(np.int64)\n",
    "   std_left_idx = standard_flat // len(standard_right)\n",
    "   std_right_idx = standard_flat % len(standard_right)\n",
    "   \n",
    "   # For update table, make ~30% of pairs match standard pairs\n",
    "   match_count = n // 3\n",
    "   update_flat = np.empty(n, dtype=np.int64)\n",
    "   update_flat[:match_count] = standard_flat[:match_count]  # Copy matching pairs\n",
    "   \n",
    "   # Same optimization for the remaining indices\n",
    "   remaining = np.random.permutation(n - match_count)\n",
    "   remaining = (remaining * (update_total / (n - match_count))).astype(np.int64)\n",
    "   update_flat[match_count:] = remaining\n",
    "   \n",
    "   up_left_idx = update_flat // len(update_right)\n",
    "   up_right_idx = update_flat % len(update_right)\n",
    "   \n",
    "   # Generate hashes using original SHA1 approach\n",
    "   standard_hashes = [\n",
    "       sha1(i.to_bytes(8, 'big')).digest() \n",
    "       for i in range(n)\n",
    "   ]\n",
    "   standard_combined = pa.array(standard_hashes, type=pa.binary())\n",
    "   \n",
    "   # For update table, reuse hashes for matching pairs\n",
    "   update_hashes = standard_hashes[:match_count] + [\n",
    "       sha1((i + n).to_bytes(8, 'big')).digest()\n",
    "       for i in range(n - match_count)\n",
    "   ]\n",
    "   update_combined = pa.array(update_hashes, type=pa.binary())\n",
    "   \n",
    "   # Create tables\n",
    "   standard_table = pa.table({\n",
    "       'hash': standard_combined,\n",
    "       'left': standard_left.take(pa.array(std_left_idx)),\n",
    "       'right': standard_right.take(pa.array(std_right_idx)),\n",
    "       'probability': pa.array(np.round(0.7 + 0.3 * np.random.random(n), 1), type=pa.float64())\n",
    "   })\n",
    "\n",
    "   update_table = pa.table({\n",
    "       'hash': update_combined,\n",
    "       'left': update_left.take(pa.array(up_left_idx)),\n",
    "       'right': update_right.take(pa.array(up_right_idx)),\n",
    "       'probability': pa.array(np.round(0.7 + 0.3 * np.random.random(n), 1), type=pa.float64())\n",
    "   })\n",
    "   \n",
    "   return standard_table, update_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster_tables(\n",
    "    hierarchical_results: pa.Table,\n",
    "    model: bytes,\n",
    ") -> tuple[pa.Table, pa.Table, pa.Table]:\n",
    "    \"\"\"\n",
    "    Creates three PyArrow tables from hierarchical clustering results:\n",
    "    - clusters: unique clusters with their metadata\n",
    "    - contains: parent-child relationships between clusters\n",
    "    - probabilities: probability scores for each cluster\n",
    "\n",
    "    Args:\n",
    "        hierarchical_results: PyArrow Table with columns (parent, child, threshold)\n",
    "        model: bytes identifier for the model\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (clusters, contains, probabilities) PyArrow tables where:\n",
    "        - clusters: schema (hash: binary, dataset: binary, id: list[string])\n",
    "        - contains: schema (parent: binary, child: binary)\n",
    "        - probabilities: schema (model: binary, cluster: binary, probability: double)\n",
    "    \"\"\"\n",
    "    # Extract unique clusters (parents) and create clusters table\n",
    "    unique_clusters = pa.compute.unique(hierarchical_results['parent'])\n",
    "    clusters = pa.Table.from_arrays(\n",
    "        [\n",
    "            unique_clusters,  # hash\n",
    "            pa.array([None] * len(unique_clusters), type=pa.binary()),  # dataset\n",
    "            pa.array([[]] * len(unique_clusters), type=pa.list_(pa.string())),  # id as list[str]\n",
    "        ],\n",
    "        names=['hash', 'dataset', 'id']\n",
    "    )\n",
    "\n",
    "    # Contains table is just parent-child relationships\n",
    "    contains = pa.Table.from_arrays(\n",
    "        [\n",
    "            hierarchical_results['parent'],\n",
    "            hierarchical_results['child']\n",
    "        ],\n",
    "        names=['parent', 'child']\n",
    "    )\n",
    "\n",
    "    # Probabilities table with model reference\n",
    "    probabilities = pa.Table.from_arrays(\n",
    "        [\n",
    "            pa.array([model] * len(hierarchical_results)),\n",
    "            hierarchical_results['parent'],\n",
    "            hierarchical_results['threshold']\n",
    "        ],\n",
    "        names=['model', 'cluster', 'probability']\n",
    "    )\n",
    "\n",
    "    return clusters, contains, probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy data\n",
    "\n",
    "Use the functions and make the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 5\n",
    "size = \"sml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "from uuid import UUID, uuid4\n",
    "from typing import Generator\n",
    "\n",
    "def d1gen() -> Generator[UUID, None, None]:\n",
    "    \"\"\"Generate UUIDs for dataset 1.\"\"\"\n",
    "\n",
    "    for i in count():\n",
    "        yield UUID(int=i, version=4)\n",
    "\n",
    "def d2gen() -> Generator[UUID, None, None]:\n",
    "    \"\"\"Generate UUIDs for dataset 2.\"\"\"\n",
    "    for _ in count():\n",
    "        yield uuid4()\n",
    "\n",
    "d1ids = d1gen()\n",
    "d2ids = d2gen()\n",
    "\n",
    "def num(c: int, e: int) -> int:\n",
    "    return int(c * (10 ** e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyarrow.Table\n",
       " hash: binary\n",
       " dataset: binary\n",
       " id: list<item: string>\n",
       "   child 0, item: string\n",
       " ----\n",
       " hash: [[B10421434DA41B7C94C47718EFDE3CE6CB1FF122,A15B231090CC4807D51B8052B58FF57E4470191E,D29641E62DAD17B8377E950C61CC9808B2066024,2A43B84231A5A07A7AEDF9CFC9CB2FB52F0BCA79,79BB59CF199E1DFB3A9C74B6ACFA93423B9ABE51,...,713E1EF2E53E791A2619DFCA3D97036D8B28C305,5D3B6381F8A484AADDE45CE36CCDD7A5A6F768EE,CBF03FD01FBAD9B3A3DC3C39D94D5D5BAA5486B6,1B98FEB4EADE4E1E2A6F49B053375870CC5DFD35,9C32D1D9A0229E99683FFE4F7ED5E8BF5A438862]]\n",
       " dataset: [[6461746173657431,6461746173657431,6461746173657431,6461746173657431,6461746173657431,...,6461746173657431,6461746173657431,6461746173657431,6461746173657431,6461746173657431]]\n",
       " id: [[[\"00000000-0000-4000-8000-000000000000\"],[\"00000000-0000-4000-8000-000000000001\"],...,[\"00000000-0000-4000-8000-000000030d3e\"],[\"00000000-0000-4000-8000-000000030d3f\"]]],\n",
       " pyarrow.Table\n",
       " hash: binary\n",
       " dataset: binary\n",
       " id: list<item: string>\n",
       "   child 0, item: string\n",
       " ----\n",
       " hash: [[B10421434DA41B7C94C47718EFDE3CE6CB1FF122,A15B231090CC4807D51B8052B58FF57E4470191E,D29641E62DAD17B8377E950C61CC9808B2066024,2A43B84231A5A07A7AEDF9CFC9CB2FB52F0BCA79,79BB59CF199E1DFB3A9C74B6ACFA93423B9ABE51,...,C9279DCBE0894A1C20F84FDC853217A8D2C7530E,29B86742C81BB67B838019A117DEDC9B775AC965,1D480B55D22A2DBB4AF3E52C27F9D759D397E35C,42022739D36F21BB817263D88B34694FF9173AA7,748F1A5F27DA713393BA6EA05F51CE1AA51A86F0]]\n",
       " dataset: [[6461746173657431,6461746173657431,6461746173657431,6461746173657431,6461746173657431,...,6461746173657431,6461746173657431,6461746173657431,6461746173657431,6461746173657431]]\n",
       " id: [[[\"00000000-0000-4000-8000-000000000000\"],[\"00000000-0000-4000-8000-000000000001\"],...,[\"00000000-0000-4000-8000-0000000493de\"],[\"00000000-0000-4000-8000-0000000493df\"]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1_standard, dataset1_update = create_test_tables(num(2, e), b'dataset1', d1ids)\n",
    "\n",
    "dataset1_standard, dataset1_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyarrow.Table\n",
       " hash: binary\n",
       " dataset: binary\n",
       " id: list<item: string>\n",
       "   child 0, item: string\n",
       " ----\n",
       " hash: [[51F3669C17B5570DE5187B35B9019960F13A60BF,72B5B923969D62530CF156CCD3795D73F87BECCB,5C48EAD71D5A3057BFBC3F2511F57E831B59AC76,AC30A70215DEFF28FC75EB1CFC755744E117CE1A,DC1602B6D0768EA9B749DAAAEC4EDD1265002271,...,72FB481854F4C2237FA6B331D48C12A213CE44B3,AB66AD8433006A8F6F14029A9305F9D0B46D3186,D8048A51ADC25D94A374284F01C5E476445D240E,9888525ED6FED103445D75A4097158514F4CC502,C9ADD6CC80AFB498F66A8275E1B639DDB815C7FF]]\n",
       " dataset: [[6461746173657432,6461746173657432,6461746173657432,6461746173657432,6461746173657432,...,6461746173657432,6461746173657432,6461746173657432,6461746173657432,6461746173657432]]\n",
       " id: [[[\"b4e5e10c-8b7c-4790-8bab-fb5dea9eece3\"],[\"7551a529-2e75-4ed4-9a73-3d75e4fe8c45\"],...,[\"ef605c11-5de8-4fbd-8747-cec2e6a7d7be\"],[\"1a763f37-5018-416f-a85a-21ad749227b7\"]]],\n",
       " pyarrow.Table\n",
       " hash: binary\n",
       " dataset: binary\n",
       " id: list<item: string>\n",
       "   child 0, item: string\n",
       " ----\n",
       " hash: [[51F3669C17B5570DE5187B35B9019960F13A60BF,72B5B923969D62530CF156CCD3795D73F87BECCB,5C48EAD71D5A3057BFBC3F2511F57E831B59AC76,AC30A70215DEFF28FC75EB1CFC755744E117CE1A,DC1602B6D0768EA9B749DAAAEC4EDD1265002271,...,27866A0AA37D3E9A710C3B3BC30C27DE79D56D33,A1CEB453A4104117F091B263D71D3F30BD7A5EE3,C3A3F3CA5F3B5DFEA170DAAE76232BDC004A6B7C,E39513FC2DA556C3F48C7546FD329933B7C94A6D,B9FC0DB532C6BDED319901B710621D70D98731D4]]\n",
       " dataset: [[6461746173657432,6461746173657432,6461746173657432,6461746173657432,6461746173657432,...,6461746173657432,6461746173657432,6461746173657432,6461746173657432,6461746173657432]]\n",
       " id: [[[\"b4e5e10c-8b7c-4790-8bab-fb5dea9eece3\"],[\"7551a529-2e75-4ed4-9a73-3d75e4fe8c45\"],...,[\"a7fac124-47f7-4445-8e73-36727b568936\"],[\"fe70a6fc-a016-49a0-bf3a-c9ddf5cebffc\"]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2_standard, dataset2_update = create_test_tables(num(5, e - 1), b'dataset2', d2ids)\n",
    "\n",
    "dataset2_standard, dataset2_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyarrow.Table\n",
       " hash: binary\n",
       " left: binary\n",
       " right: binary\n",
       " probability: double\n",
       " ----\n",
       " hash: [[05FE405753166F125559E7C9AC558654F107C7E9,CB473678976F425D6EC1339838F11011007AD27D,07AAE1B618F604C684EE3189FA1723BEF8656FE4,461D6580E38CCB6DC72699B6C945E53831DCDF03,7F028DDBB42E47AC2CD00E27A37BD191F1C2B925,...,D3425A8AD85C009A7C0207DE64C76CD24A022146,64F3C46ED38031FC857E20E304267CC5E133D292,F5DF0A4361A13D8AF3D6D8382BAE765529C133D7,550E7F8689BFF4E9A8C04DBC7B34684604C8B8C7,A3E65CF5997DDA194FC6B35EF0269CBB2B941EC3]]\n",
       " left: [[CF5E9FB3957C4EC962A4103462C15E7F8FC0DF8E,355BEE9C22137718A46B0CF2CA39BCDBB26BEF8F,5BCE428231451E8FFE4362AD657B220207844A3B,F8801D4165D23A5ED8E719862A952F1C135FA1CC,AD28B61ECB101285A7B2E07120581BD7E7AE1CCB,...,A222F9016CDB31E4E95F1AA322CB0937A909D431,9CD5CF0309C0CE03675D1C285EF7DCA1E89DC2EF,7CCA0B89F7F723839BB32D0E29FF5E28A8AA29EC,1F26288DD88FCCA79EE9E8018FB4E33E3F073191,731920CB3B1CC21C1B909179D4C4E6175EC79172]]\n",
       " right: [[4F85C373359E57A62BD647D55FC7C8521CBD0EF2,AA7695BD81ACF7491866A1BC5626D490F17D012A,AA7695BD81ACF7491866A1BC5626D490F17D012A,4F85C373359E57A62BD647D55FC7C8521CBD0EF2,AA7695BD81ACF7491866A1BC5626D490F17D012A,...,AA7695BD81ACF7491866A1BC5626D490F17D012A,AA7695BD81ACF7491866A1BC5626D490F17D012A,AA7695BD81ACF7491866A1BC5626D490F17D012A,AA7695BD81ACF7491866A1BC5626D490F17D012A,E4E1568D63F363CDDAA4EC2D50CDA1868CDDFC5B]]\n",
       " probability: [[0.7,1,1,0.7,0.7,...,0.9,0.9,0.8,0.8,1]],\n",
       " pyarrow.Table\n",
       " hash: binary\n",
       " left: binary\n",
       " right: binary\n",
       " probability: double\n",
       " ----\n",
       " hash: [[05FE405753166F125559E7C9AC558654F107C7E9,CB473678976F425D6EC1339838F11011007AD27D,07AAE1B618F604C684EE3189FA1723BEF8656FE4,461D6580E38CCB6DC72699B6C945E53831DCDF03,7F028DDBB42E47AC2CD00E27A37BD191F1C2B925,...,7BDCBA5BE8EC3464391FB1CA26BB25CAA45C357A,6BE0275AE916D1B8D3CD6AE41D7D894EA188A437,B9411381CC4F092FB883B53069979BB6689F87E9,38677C34C39F1B4417EF408BF2AC098E88A7FBD2,604F72CA8A5E0F7BA05EC860DAD81C8261986DE6]]\n",
       " left: [[9DB007E32A1EA6064E6EFDACB1877A2D3E08775F,355BEE9C22137718A46B0CF2CA39BCDBB26BEF8F,B809F1BDA0B27AE8C47200E5AF4234245101A6F1,F7C739191E447B9D42C8C5E11AD729C2F89332BF,AD28B61ECB101285A7B2E07120581BD7E7AE1CCB,...,DCEFCA3B9F59E173D45393295AB5CDD0F5AC6B6A,8F5A0FB45E722686D662BF35B2F549918B4C7FC8,A7C0D08FC0C76C680625B88738F38208E0EFA572,BE3093C7A7FC10694C5B61569EF44764F0D5BB91,33E9BBDDC78B863C909E107EB3AD85D5E42FD3A8]]\n",
       " right: [[56609D59090A4B6172F7CAD713AF6F3ABE13E4A6,317F7880019F03C503B6AC2227952ECDE0D41F96,317F7880019F03C503B6AC2227952ECDE0D41F96,56609D59090A4B6172F7CAD713AF6F3ABE13E4A6,317F7880019F03C503B6AC2227952ECDE0D41F96,...,BAF0E7880E92883CDF61F9166BF94E81132D061C,E257AD3B13AC71BA4F457CD8BC254158FE990488,1B4ED779C2A7DF58470C723AC4A2328E6F47E369,A3B304CEBF8B801EEF34E28617256F617150A15F,87CA126A8D5F7784D755D3BD86DF62CB72E819A6]]\n",
       " probability: [[0.8,0.9,0.8,0.7,0.8,...,1,0.9,0.8,1,0.9]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probresults_standard, probresults_update = fast_consistent_pairs(\n",
    "    dataset1_standard['hash'],\n",
    "    dataset2_standard['hash'],\n",
    "    dataset1_update['hash'],\n",
    "    dataset2_update['hash'],\n",
    "    n=num(1, e + 1)\n",
    ")\n",
    "\n",
    "probresults_standard, probresults_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyarrow.Table\n",
       " parent: binary\n",
       " child: binary\n",
       " threshold: double\n",
       " ----\n",
       " parent: [[07F58C1D1D7F7E9B39BB5CD9A65FD254E92AD26E68D29B5B36BA42BD8FE40714,07F58C1D1D7F7E9B39BB5CD9A65FD254E92AD26E68D29B5B36BA42BD8FE40714,07F58C1D1D7F7E9B39BB5CD9A65FD254E92AD26E68D29B5B36BA42BD8FE40714,07F58C1D1D7F7E9B39BB5CD9A65FD254E92AD26E68D29B5B36BA42BD8FE40714,07F58C1D1D7F7E9B39BB5CD9A65FD254E92AD26E68D29B5B36BA42BD8FE40714,...,57CF45A6B8ABC3A40018C3CAB83385F7F3E4EA053888454A6D60224804149075,57CF45A6B8ABC3A40018C3CAB83385F7F3E4EA053888454A6D60224804149075,57CF45A6B8ABC3A40018C3CAB83385F7F3E4EA053888454A6D60224804149075,57CF45A6B8ABC3A40018C3CAB83385F7F3E4EA053888454A6D60224804149075,57CF45A6B8ABC3A40018C3CAB83385F7F3E4EA053888454A6D60224804149075]]\n",
       " child: [[00001658DE925885461A13C4E24CD137E744ED1B,00013143B8E9564450AA2AC631F3208C9791B327,00015E864E65931319EEA1EE51760E7CA7B3C915,000197407B326FD40BE3F52D74261B5227CE9BFE,0001D00DED90766C18B4C6622776DDC405BB321C,...,FFFCC3E75539A6352EA516F0DAD6E28A2CCD9900,FFFD0F3371B95C2DC93EBDE701309ECADA1DBE21,FFFDB881D8578BB3325C91AE6646A2E7C95441A7,FFFE21A2614C1D29A4B572BB8C15C934802FC7DF,FFFE767F38648B640A9BB0A03E245ACB38B738A5]]\n",
       " threshold: [[1,1,1,1,1,...,0.7,0.7,0.7,0.7,0.7]],\n",
       " pyarrow.Table\n",
       " parent: binary\n",
       " child: binary\n",
       " threshold: double\n",
       " ----\n",
       " parent: [[F90F07D03F016B7F0FD56F89BDA280DD0A5BE0E7270C4F90AE6136BB37B381BA,F90F07D03F016B7F0FD56F89BDA280DD0A5BE0E7270C4F90AE6136BB37B381BA,F90F07D03F016B7F0FD56F89BDA280DD0A5BE0E7270C4F90AE6136BB37B381BA,F90F07D03F016B7F0FD56F89BDA280DD0A5BE0E7270C4F90AE6136BB37B381BA,F90F07D03F016B7F0FD56F89BDA280DD0A5BE0E7270C4F90AE6136BB37B381BA,...,7E06FBBD3BD64CD97E9E08B4B6A1236B9EF3B8B0A1E1C0FCF27587D65C6458D3,7E06FBBD3BD64CD97E9E08B4B6A1236B9EF3B8B0A1E1C0FCF27587D65C6458D3,7E06FBBD3BD64CD97E9E08B4B6A1236B9EF3B8B0A1E1C0FCF27587D65C6458D3,7E06FBBD3BD64CD97E9E08B4B6A1236B9EF3B8B0A1E1C0FCF27587D65C6458D3,7E06FBBD3BD64CD97E9E08B4B6A1236B9EF3B8B0A1E1C0FCF27587D65C6458D3]]\n",
       " child: [[011056C4F23F3FDF15CD20B71E0BE83445E5B7C3,57A2C613A09CB58979D65DB1D655261DDBF9C27A,87A8767A05BCB724E6B23F2AC8EA8CD0A9434682,D987E5695169F9D276DC09011F655FD427AED1F3,E7492D7B64DA2D92A76DFC864F0C978D078AD601,...,FFFC05696E3415842E614FCF402EB3609A495FCA,FFFC4C7E568DD8B54AA71A19AF35E521F823877D,FFFC9368B92E60D59727D1DDFC652E61D48CD255,FFFE18EE7325D9E2A0412CB2F44F33D2E198A40E,FFFF38A5B12F9C3E4B819C6172F8FAE6194CBC85]]\n",
       " threshold: [[1,1,1,1,1,...,0.7,0.7,0.7,0.7,0.7]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusresults_standard = to_clusters(probresults_standard)\n",
    "clusresults_update = to_clusters(probresults_update)\n",
    "\n",
    "clusresults_standard, clusresults_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyarrow.Table\n",
       " parent: binary\n",
       " child: binary\n",
       " threshold: double\n",
       " ----\n",
       " parent: [[FFFE461EB0EBF8728C2C0D056E9182990F164456,FFFE461EB0EBF8728C2C0D056E9182990F164456,FFFD38D1675EBEC0848FD0B0D8F162F43E178EEC,FFFD38D1675EBEC0848FD0B0D8F162F43E178EEC,FFFC7CECF46E6A00E591F0932DF14E2E7652713B,...,000197407B326FD40BE3F52D74261B5227CE9BFE,00017F1C323AACA322EE44CC134F3DBF4807B1C5,00017F1C323AACA322EE44CC134F3DBF4807B1C5,00015632977C2E0908883990F2DE9151C6B781A4,00015632977C2E0908883990F2DE9151C6B781A4]]\n",
       " child: [[E4E1568D63F363CDDAA4EC2D50CDA1868CDDFC5B,202E1AF2B1F8C1491B8B4112000529A6594F3283,BCEA5C3427F09219C43F802511222611E810BBE8,73009B330C21FE56910F11AD18A88DC125547CD1,BCEA5C3427F09219C43F802511222611E810BBE8,...,E4E1568D63F363CDDAA4EC2D50CDA1868CDDFC5B,EC7A80600D3C33268CA4FD6A0E7A593C0BB47D8B,E4E1568D63F363CDDAA4EC2D50CDA1868CDDFC5B,BCEA5C3427F09219C43F802511222611E810BBE8,AF2B4CF3F633F3E5C2282E54C42E4DA9B8BBE184]]\n",
       " threshold: [[1,1,1,1,1,...,0.7,0.7,0.7,0.7,0.7]],\n",
       " pyarrow.Table\n",
       " parent: binary\n",
       " child: binary\n",
       " threshold: double\n",
       " ----\n",
       " parent: [[FFFF7D7EB344D5610CE897F6F3DE9B05A3B1E235,FFFF7D7EB344D5610CE897F6F3DE9B05A3B1E235,FFFEF59A828A2D28C6565B9C996898710DE0BAA3,FFFEF59A828A2D28C6565B9C996898710DE0BAA3,FFFE679806033605138F4E4A1B4603937A120B58,...,000045D0E91911FDABC4C793BBB4D8159960E4D7,000041D5A799DF96700CBE89646C241C8B3D7C24,000041D5A799DF96700CBE89646C241C8B3D7C24,00001658DE925885461A13C4E24CD137E744ED1B,00001658DE925885461A13C4E24CD137E744ED1B]]\n",
       " child: [[FFA15301B345E7C2C2FDCC45D090E4CE1A1EA556,4AC84CDCAA2332A6673D347A4AA77611C5F2405A,D66E78749FCF70D7E80AFF55584559A642DFF0B8,54BAC5F5F7A4422DA112D3954071B6087776EA36,36F6C2B1DA197EF680ED2549E926C812727DF347,...,3019431DEA1EBD17F24921C2BBCF9B4308177617,A8EA5FFC14DBF4E47CE1751DD7DC9D7AF1A5CA04,1EBD8F0F2BA4ADE7B0C26E324F44515F4A83D1CC,E4E1568D63F363CDDAA4EC2D50CDA1868CDDFC5B,3ECB7DDC8CD92EF3FB8582EE659D7ABCEA8E3F9E]]\n",
       " threshold: [[1,1,1,1,1,...,0.7,0.7,0.7,0.7,0.7]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hierarchy_standard = _cluster_results_to_hierarchical_pa(probresults_standard, clusresults_standard)\n",
    "hierarchy_update = _cluster_results_to_hierarchical_pa(probresults_update, clusresults_update)\n",
    "\n",
    "hierarchy_standard, hierarchy_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyarrow.Table\n",
       " hash: binary\n",
       " dataset: binary\n",
       " id: list<item: string>\n",
       "   child 0, item: string\n",
       " ----\n",
       " hash: [[FFFE461EB0EBF8728C2C0D056E9182990F164456,FFFD38D1675EBEC0848FD0B0D8F162F43E178EEC,FFFC7CECF46E6A00E591F0932DF14E2E7652713B,FFFC64F1971D751AF36B7532ED88E3C3771CA478,FFFBAC26F6D4A88C2C522FDDDE3ED34FB6343E67,...,00021FDDFE3897E5DBA2E34EB22404F472531CEA,0001D8BFAFB8F202558801A8AF76639E9F21B3FE,000197407B326FD40BE3F52D74261B5227CE9BFE,00017F1C323AACA322EE44CC134F3DBF4807B1C5,00015632977C2E0908883990F2DE9151C6B781A4]]\n",
       " dataset: [[null,null,null,null,null,...,null,null,null,null,null]]\n",
       " id: [[[],[],...,[],[]]],\n",
       " pyarrow.Table\n",
       " parent: binary\n",
       " child: binary\n",
       " ----\n",
       " parent: [[FFFE461EB0EBF8728C2C0D056E9182990F164456,FFFE461EB0EBF8728C2C0D056E9182990F164456,FFFD38D1675EBEC0848FD0B0D8F162F43E178EEC,FFFD38D1675EBEC0848FD0B0D8F162F43E178EEC,FFFC7CECF46E6A00E591F0932DF14E2E7652713B,...,000197407B326FD40BE3F52D74261B5227CE9BFE,00017F1C323AACA322EE44CC134F3DBF4807B1C5,00017F1C323AACA322EE44CC134F3DBF4807B1C5,00015632977C2E0908883990F2DE9151C6B781A4,00015632977C2E0908883990F2DE9151C6B781A4]]\n",
       " child: [[E4E1568D63F363CDDAA4EC2D50CDA1868CDDFC5B,202E1AF2B1F8C1491B8B4112000529A6594F3283,BCEA5C3427F09219C43F802511222611E810BBE8,73009B330C21FE56910F11AD18A88DC125547CD1,BCEA5C3427F09219C43F802511222611E810BBE8,...,E4E1568D63F363CDDAA4EC2D50CDA1868CDDFC5B,EC7A80600D3C33268CA4FD6A0E7A593C0BB47D8B,E4E1568D63F363CDDAA4EC2D50CDA1868CDDFC5B,BCEA5C3427F09219C43F802511222611E810BBE8,AF2B4CF3F633F3E5C2282E54C42E4DA9B8BBE184]],\n",
       " pyarrow.Table\n",
       " model: binary\n",
       " cluster: binary\n",
       " probability: double\n",
       " ----\n",
       " model: [[6D6F64656C31,6D6F64656C31,6D6F64656C31,6D6F64656C31,6D6F64656C31,...,6D6F64656C31,6D6F64656C31,6D6F64656C31,6D6F64656C31,6D6F64656C31]]\n",
       " cluster: [[FFFE461EB0EBF8728C2C0D056E9182990F164456,FFFE461EB0EBF8728C2C0D056E9182990F164456,FFFD38D1675EBEC0848FD0B0D8F162F43E178EEC,FFFD38D1675EBEC0848FD0B0D8F162F43E178EEC,FFFC7CECF46E6A00E591F0932DF14E2E7652713B,...,000197407B326FD40BE3F52D74261B5227CE9BFE,00017F1C323AACA322EE44CC134F3DBF4807B1C5,00017F1C323AACA322EE44CC134F3DBF4807B1C5,00015632977C2E0908883990F2DE9151C6B781A4,00015632977C2E0908883990F2DE9151C6B781A4]]\n",
       " probability: [[1,1,1,1,1,...,0.7,0.7,0.7,0.7,0.7]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters_standard, contains_standard, probabilities_standard = create_cluster_tables(hierarchy_standard, b'model1')\n",
    "\n",
    "clusters_standard, contains_standard, probabilities_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyarrow.Table\n",
       " hash: binary\n",
       " dataset: binary\n",
       " id: list<item: string>\n",
       "   child 0, item: string\n",
       " ----\n",
       " hash: [[FFFF7D7EB344D5610CE897F6F3DE9B05A3B1E235,FFFEF59A828A2D28C6565B9C996898710DE0BAA3,FFFE679806033605138F4E4A1B4603937A120B58,FFFE5361A6785092508A12075294B2F81663F8F4,FFFE18EE7325D9E2A0412CB2F44F33D2E198A40E,...,00015E864E65931319EEA1EE51760E7CA7B3C915,0000A02A1DC9D8C923CDA243A943C32AC2165A2F,000045D0E91911FDABC4C793BBB4D8159960E4D7,000041D5A799DF96700CBE89646C241C8B3D7C24,00001658DE925885461A13C4E24CD137E744ED1B]]\n",
       " dataset: [[null,null,null,null,null,...,null,null,null,null,null]]\n",
       " id: [[[],[],...,[],[]]],\n",
       " pyarrow.Table\n",
       " parent: binary\n",
       " child: binary\n",
       " ----\n",
       " parent: [[FFFF7D7EB344D5610CE897F6F3DE9B05A3B1E235,FFFF7D7EB344D5610CE897F6F3DE9B05A3B1E235,FFFEF59A828A2D28C6565B9C996898710DE0BAA3,FFFEF59A828A2D28C6565B9C996898710DE0BAA3,FFFE679806033605138F4E4A1B4603937A120B58,...,000045D0E91911FDABC4C793BBB4D8159960E4D7,000041D5A799DF96700CBE89646C241C8B3D7C24,000041D5A799DF96700CBE89646C241C8B3D7C24,00001658DE925885461A13C4E24CD137E744ED1B,00001658DE925885461A13C4E24CD137E744ED1B]]\n",
       " child: [[FFA15301B345E7C2C2FDCC45D090E4CE1A1EA556,4AC84CDCAA2332A6673D347A4AA77611C5F2405A,D66E78749FCF70D7E80AFF55584559A642DFF0B8,54BAC5F5F7A4422DA112D3954071B6087776EA36,36F6C2B1DA197EF680ED2549E926C812727DF347,...,3019431DEA1EBD17F24921C2BBCF9B4308177617,A8EA5FFC14DBF4E47CE1751DD7DC9D7AF1A5CA04,1EBD8F0F2BA4ADE7B0C26E324F44515F4A83D1CC,E4E1568D63F363CDDAA4EC2D50CDA1868CDDFC5B,3ECB7DDC8CD92EF3FB8582EE659D7ABCEA8E3F9E]],\n",
       " pyarrow.Table\n",
       " model: binary\n",
       " cluster: binary\n",
       " probability: double\n",
       " ----\n",
       " model: [[6D6F64656C31,6D6F64656C31,6D6F64656C31,6D6F64656C31,6D6F64656C31,...,6D6F64656C31,6D6F64656C31,6D6F64656C31,6D6F64656C31,6D6F64656C31]]\n",
       " cluster: [[FFFF7D7EB344D5610CE897F6F3DE9B05A3B1E235,FFFF7D7EB344D5610CE897F6F3DE9B05A3B1E235,FFFEF59A828A2D28C6565B9C996898710DE0BAA3,FFFEF59A828A2D28C6565B9C996898710DE0BAA3,FFFE679806033605138F4E4A1B4603937A120B58,...,000045D0E91911FDABC4C793BBB4D8159960E4D7,000041D5A799DF96700CBE89646C241C8B3D7C24,000041D5A799DF96700CBE89646C241C8B3D7C24,00001658DE925885461A13C4E24CD137E744ED1B,00001658DE925885461A13C4E24CD137E744ED1B]]\n",
       " probability: [[1,1,1,1,1,...,0.7,0.7,0.7,0.7,0.7]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters_update, contains_update, probabilities_update = create_cluster_tables(hierarchy_update, b'model1')\n",
    "\n",
    "clusters_update, contains_update, probabilities_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've therefore decided to produce three sets of parquets: small, medium and large. I figured this would be useful for you too as you can use small to ensure your plumbing is all set up right before you scale to medium and large.\n",
    " \n",
    "Each set of files includes:\n",
    "\n",
    "* `clusters_dataset1_standard_{size}`\n",
    "* `clusters_dataset1_update_{size}`\n",
    "* `clusters_dataset2_standard_{size}`\n",
    "* `clusters_dataset2_update_{size}`\n",
    "* `clusters_clusters_standard_{size}`\n",
    "* `clusters_clusters_update_{size}`\n",
    "* `contains_standard_{size}`\n",
    "* `contains_update_{size}`\n",
    "* `probabilities_standard_{size}`\n",
    "* `probabilities_update_{size}`\n",
    "\n",
    "The first word of these names is the table to insert into. Final word is whether it's the initial or updated insert. The shape of them should be exactly right for the PostgreSQL table with column names matching the ORM, and datatypes in the paired Arrow format, so `BYTEA == pa.binary`, `ARRAY<VARCHAR> == pa.list_(pa.string)`, `FLOAT == pa.double`. I've used snappy compression to match the post-processed parquet format we're likely to use, rather than the hyper-compressed format we upload.\n",
    " \n",
    "Because the `Clusters` table contains both data and clusters, and these files describe a linking process, there's the inserts for each dataset, then the inserts of the clusters that match those datasets.\n",
    " \n",
    "To achieve proportional scaling with the same functions, my counts are in scientific notation where I've parameterised the exponent. In small it's 5, in medium it's 6, and in large it's 7, which I believe gets us to the 100m probabilities scale for the probabilities table (1 ** 10 ^ e + 1 == 1e8 == 100m).\n",
    " \n",
    "Dataset1 is analogous to HMRC exports: there's 2 ** 10 ^ e rows, or 20m in large.\n",
    " \n",
    "Dataset2 is analogous to Companies House: there's 5 ** 10 ^ e - 1 rows, or 5m in large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "\n",
    "file_path = Path.cwd()\n",
    "\n",
    "dir_path = Path.cwd() / size\n",
    "\n",
    "pq.write_table(dataset1_standard, dir_path / f'clusters_dataset1_standard_{size}.parquet')\n",
    "pq.write_table(dataset1_update, dir_path / f'clusters_dataset1_update_{size}.parquet')\n",
    "pq.write_table(dataset2_standard, dir_path / f'clusters_dataset2_standard_{size}.parquet')\n",
    "pq.write_table(dataset2_update, dir_path / f'clusters_dataset2_update_{size}.parquet')\n",
    "pq.write_table(clusters_standard, dir_path / f'clusters_clusters_standard_{size}.parquet')\n",
    "pq.write_table(clusters_update, dir_path / f'clusters_clusters_update_{size}.parquet')\n",
    "pq.write_table(contains_standard, dir_path / f'contains_standard_{size}.parquet')\n",
    "pq.write_table(contains_update, dir_path / f'contains_update_{size}.parquet')\n",
    "pq.write_table(probabilities_standard, dir_path / f'probabilities_standard_{size}.parquet')\n",
    "pq.write_table(probabilities_update, dir_path / f'probabilities_update_{size}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(hash: binary\n",
       " left: binary\n",
       " right: binary\n",
       " probability: double,\n",
       " '7653.24MB')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_table.schema, f\"{arrow_table.nbytes / (1024 * 1024):.2f}MB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "\n",
    "class HashIndex:\n",
    "    def __init__(self, hash_type: str = 'sha1', hashes: list[bytes] | pa.Array | None = None):\n",
    "        \"\"\"Create a new HashIndex instance.\n",
    "        \n",
    "        Args:\n",
    "            hash_type: Hash algorithm to use (default: 'sha1')\n",
    "            hashes (Optional): Initial list of hashes to insert (default: None)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            hash_obj = hashlib.new(hash_type)\n",
    "            self._hash_size: int = hash_obj.digest_size   # Return the digest size in bytes\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f'Unsupported hash type: {hash_type}') from e\n",
    "        \n",
    "        # Initialize empty table with correct schema\n",
    "        self.schema = pa.schema([\n",
    "            ('hash', pa.binary(self._hash_size)),\n",
    "            ('id', pa.int32())\n",
    "        ])\n",
    "        self.table = pa.Table.from_pydict({\n",
    "            'hash': [],\n",
    "            'id': []\n",
    "        }, schema=self.schema)\n",
    "        self.next_id: int = 0\n",
    "\n",
    "        if hashes is not None:\n",
    "            self.insert_hashes(hashes)\n",
    "\n",
    "    def __eq__(self, other: 'HashIndex') -> bool:\n",
    "        \"\"\"\n",
    "        Compare this HashIndex with another for equality.\n",
    "        \n",
    "        Two HashIndex instances are considered equal if they:\n",
    "            1. Have the same hash size\n",
    "            2. Have the same next_id\n",
    "            3. Have equal tables (same schema and data)\n",
    "        \n",
    "        Args:\n",
    "            other: Another HashIndex instance to compare with\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if the indexes are equal, False otherwise\n",
    "        \"\"\"\n",
    "        if not isinstance(other, HashIndex):\n",
    "            return False\n",
    "            \n",
    "        return (\n",
    "            self._hash_size == other._hash_size and\n",
    "            self.next_id == other.next_id and\n",
    "            self.table.equals(other.table)\n",
    "        )\n",
    "\n",
    "    def _list_to_array(self, hashes: list[bytes] | pa.Array) -> pa.Array:\n",
    "        if isinstance(hashes, list):\n",
    "            return pa.array(hashes, type=pa.binary(self._hash_size))\n",
    "        return hashes\n",
    "\n",
    "    def insert_hashes(self, hashes: list[bytes] | pa.Array) -> pa.Array:\n",
    "        \"\"\"\n",
    "        Insert new hashes and return their indices. For existing hashes, returns their\n",
    "        current indices. For new hashes, assigns and returns new indices.\n",
    "        \n",
    "        Args:\n",
    "            hashes: Array of SHA-1 hashes to insert\n",
    "        Returns:\n",
    "            Array of indices (both existing and newly assigned)\n",
    "        \"\"\"\n",
    "        hashes = self._list_to_array(hashes)\n",
    "        \n",
    "        # If table is empty, fast path to create initial table\n",
    "        if len(self.table) == 0:\n",
    "            ids = pa.array(range(len(hashes)), type=pa.int32())\n",
    "            self.table = pa.Table.from_arrays([hashes, ids], schema=self.schema)\n",
    "            self.next_id = len(hashes)\n",
    "            # Sort table by hash for future binary searches\n",
    "            self.table = self.table.sort_by('hash')\n",
    "            return ids\n",
    "\n",
    "        # Find existing hashes using binary search\n",
    "        indices = pc.index_in(hashes, self.table['hash'])\n",
    "        is_new = pc.is_null(indices)\n",
    "        new_count = pc.sum(pc.cast(is_new, pa.int32())).as_py()\n",
    "        \n",
    "        if new_count > 0:\n",
    "            # Get the new hashes\n",
    "            new_hashes = pc.filter(hashes, is_new)\n",
    "            \n",
    "            # Pre-allocate new IDs array\n",
    "            new_ids = pa.array(\n",
    "                range(self.next_id, self.next_id + new_count), \n",
    "                type=pa.int32()\n",
    "            )\n",
    "            self.next_id += new_count\n",
    "            \n",
    "            # Append in one operation and sort once\n",
    "            new_table = pa.Table.from_arrays([new_hashes, new_ids], schema=self.schema)\n",
    "            self.table = pa.concat_tables([self.table, new_table])\n",
    "            self.table = self.table.sort_by('hash')\n",
    "            \n",
    "            # Final lookup to get all IDs in correct order\n",
    "            indices = pc.index_in(hashes, self.table['hash'])\n",
    "        \n",
    "        return pc.take(self.table['id'], indices)\n",
    "\n",
    "    def get_hashes(self, ids: list[int] | pa.Array) -> pa.Array:\n",
    "        \"\"\"\n",
    "        Look up hashes by their IDs\n",
    "        \n",
    "        Args:\n",
    "            ids: Array of IDs to look up\n",
    "        Returns:\n",
    "            Array of corresponding hashes (null for unknown indices)\n",
    "        \"\"\"\n",
    "        if isinstance(ids, list):\n",
    "            ids = pa.array(ids, type=pa.int32())\n",
    "        \n",
    "        positions = pc.index_in(ids, self.table['id'])\n",
    "        return pc.take(self.table['hash'], positions)\n",
    "\n",
    "    def get_indices(self, hashes: list[bytes] | pa.Array) -> pa.Array:\n",
    "        \"\"\"\n",
    "        Look up IDs for existing hashes. Returns null for unknown hashes.\n",
    "        \n",
    "        Args:\n",
    "            hashes: Array of hashes to look up\n",
    "        Returns:\n",
    "            Array of corresponding IDs (null for unknown hashes)\n",
    "        \"\"\"\n",
    "        hashes = self._list_to_array(hashes)\n",
    "            \n",
    "        indices = pc.index_in(hashes, self.table['hash'])\n",
    "        return pc.take(self.table['id'], indices)\n",
    "    \n",
    "\n",
    "    def to_parquet(self, path: str | Path, compression: str = 'zstd') -> None:\n",
    "        \"\"\"\n",
    "        Save the HashIndex to a Parquet file.\n",
    "        \n",
    "        Args:\n",
    "            path: Path to save the Parquet file\n",
    "            compression: Compression algorithm to use (default: 'zstd')\n",
    "                Options include: 'none', 'snappy', 'gzip', 'brotli', 'lz4', 'zstd'\n",
    "        \n",
    "        Raises:\n",
    "            IOError: If the file cannot be written\n",
    "            ValueError: If the compression algorithm is not supported\n",
    "        \"\"\"\n",
    "        path = Path(path)\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        metadata = {\n",
    "            b'next_id': str(self.next_id).encode(),\n",
    "            b'hash_size': str(self._hash_size).encode()\n",
    "        }\n",
    "\n",
    "        existing_metadata = self.table.schema.metadata or {}\n",
    "        merged_metadata = {**existing_metadata, **metadata}\n",
    "        \n",
    "        try:\n",
    "            pq.write_table(\n",
    "                self.table.replace_schema_metadata(merged_metadata),\n",
    "                path,\n",
    "                compression=compression,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise IOError(f\"Failed to write Parquet file: {e}\") from e\n",
    "\n",
    "    @classmethod\n",
    "    def from_parquet(cls, path: str | Path) -> 'HashIndex':\n",
    "        \"\"\"\n",
    "        Load a HashIndex from a Parquet file.\n",
    "        \n",
    "        Args:\n",
    "            path: Path to the Parquet file\n",
    "        \n",
    "        Returns:\n",
    "            HashIndex: New HashIndex instance loaded from the file\n",
    "        \n",
    "        Raises:\n",
    "            IOError: If the file cannot be read or is invalid\n",
    "            ValueError: If the file format is invalid\n",
    "        \"\"\"\n",
    "        path = Path(path)\n",
    "        \n",
    "        try:\n",
    "            table = pq.read_table(path)\n",
    "            metadata = table.schema.metadata\n",
    "            \n",
    "            if not metadata or b'next_id' not in metadata or b'hash_size' not in metadata:\n",
    "                raise ValueError(\"Invalid Parquet file: missing required metadata\")\n",
    "            \n",
    "            # Create new instance\n",
    "            instance = cls.__new__(cls)\n",
    "            \n",
    "            instance._hash_size = int(metadata[b'hash_size'].decode())\n",
    "            instance.next_id = int(metadata[b'next_id'].decode())\n",
    "            \n",
    "            instance.schema = table.schema\n",
    "            instance.table = table\n",
    "            \n",
    "            return instance\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise IOError(f\"Failed to load Parquet file: {e}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<pyarrow.lib.BinaryArray object at 0x118363fa0>\n",
       " [\n",
       "   7BDB9431F1E8DDBF1ACCF691D4B8661CF9B43A25,\n",
       "   392BCA5B7AFC18C6727FC30EC57B718DDAAFF953,\n",
       "   F884626B7A54AEFFECB3CA3DDB1493613CC0C7A9,\n",
       "   C94022C8F551879B31D10F99BCC9A3561BAE7612,\n",
       "   08458C06AD784FCDC7809825056654AD5F119C4C\n",
       " ],\n",
       " <pyarrow.lib.BinaryArray object at 0x11f06ada0>\n",
       " [\n",
       "   31E7C33F498E4B7E95B1C52ECACAF348CEFFAE71,\n",
       "   CC141AB85B40C152864B1A24689072D718332BDD,\n",
       "   C769376877CFA78C46210E9B91D4446E82A6D7CE,\n",
       "   9263614A6B6C53967ABA62FA2859F785C58B1016,\n",
       "   15B8ABB98A8ED7D0F008C1234583DCE9DC34AFFF\n",
       " ])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hashes = fast_generate_hashes(int(2e7))\n",
    "test_hashes2 = pa.concat_arrays([fast_generate_hashes(int(2e7) / 2), test_hashes[:int(2e7 / 2)]])\n",
    "hidx_5 = HashIndex(hash_type='sha1', hashes=test_hashes)\n",
    "test_hashes[:5], test_hashes2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pa.array(random.sample(range(len(test_hashes2)), k=len(test_hashes2)))\n",
    "test_hashes3 = test_hashes2.take(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.ChunkedArray object at 0x13dc3c8b0>\n",
       "[\n",
       "  [\n",
       "    8997172,\n",
       "    6063643,\n",
       "    6260234,\n",
       "    3255430,\n",
       "    7485144,\n",
       "    ...\n",
       "    9917674,\n",
       "    5612100,\n",
       "    2723141,\n",
       "    201414,\n",
       "    4680984\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidx_5.insert_hashes(test_hashes3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<pyarrow.lib.BinaryArray object at 0x1187140a0>\n",
       " [\n",
       "   1F21ABB3185DA8D5340A8298C6CBC9C21F8AF9CD,\n",
       "   813583A225BA22DE65149EC502FA79A082F93D8C,\n",
       "   C4587910358DC70CD81B6B106FD724685D8AE971,\n",
       "   79924525EAF98B48E03E00213E0FFBFD17AD8778,\n",
       "   5E668D4D878659EE35DC8F3E7EFE5B19D68890FA\n",
       " ],\n",
       " <pyarrow.lib.BinaryArray object at 0x11ec591e0>\n",
       " [\n",
       "   BDBD93052D2290495857C2A46936C468CA4A7FD6,\n",
       "   AC0FFE1E2F41A6D12D5F56E32A00F8B48D47EF66,\n",
       "   A3DD64FC8F69428EA0693218AF31FEA22F8BCDDE,\n",
       "   0C7FF3A1FB8441369A80059DC2FA08F678A05D86,\n",
       "   BD939DD6F750A40D0C3CFCB3F1C3B202A8BEADE2\n",
       " ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hashes = fast_generate_hashes(40)\n",
    "test_hashes2 = fast_generate_hashes(40)\n",
    "hidx = HashIndex(hash_type='sha1')\n",
    "test_hashes[:5], test_hashes2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path.cwd()\n",
    "\n",
    "hidx3 = HashIndex(hash_type='sha1', hashes=fast_generate_hashes(int(2e7)))\n",
    "\n",
    "hidx3.to_parquet(file_path / 'hash_index.parquet')\n",
    "\n",
    "# del hidx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidx3 = HashIndex.from_parquet(file_path / 'hash_index.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.ChunkedArray object at 0x11ec7e6b0>\n",
       "[\n",
       "  [\n",
       "    3F0B311BB3EF9EAEB211378D538A67491E43429F,\n",
       "    1234FE9CB4C1413559FD09FBBAF512DD26DF7CF6,\n",
       "    C163CF811B2021CE8D17AFE5720F9DED2D602543,\n",
       "    D151C016C7C94022B0E8A7826EE698A0B5FB947E,\n",
       "    30EE3D4D4A6497F0C1A85158F895398C9E901F63\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidx3.get_hashes([random.randint(0, int(2e7)) for _ in range(int(2e5))])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<pyarrow.lib.Int32Array object at 0x11ecc1f60>\n",
       " [\n",
       "   0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4\n",
       " ],\n",
       " <pyarrow.lib.ChunkedArray object at 0x11ef1f600>\n",
       " [\n",
       "   [\n",
       "     40,\n",
       "     41,\n",
       "     42,\n",
       "     43,\n",
       "     44\n",
       "   ]\n",
       " ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidx.insert_hashes(test_hashes)[:5], hidx.insert_hashes(test_hashes2)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = hidx.get_hashes([0])[0]\n",
    "i = hidx.get_indices([h])[0]\n",
    "h2 = hidx.get_hashes([i])[0]\n",
    "\n",
    "h == h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = Path.cwd()\n",
    "\n",
    "hidx.to_parquet(file_path / 'hash_index.parquet')\n",
    "hidx2 = HashIndex.from_parquet(file_path / 'hash_index.parquet')\n",
    "\n",
    "hidx == hidx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.ChunkedArray object at 0x15a223d80>\n",
       "[\n",
       "  [\n",
       "    9E24FC1BC65D8A134990D02A6C4A40E3AAB20FAA,\n",
       "    09C61C3BE151A112228E61AC7C7C1B81DC625CDF,\n",
       "    CEED556D7FE741F9BC0FA5A464DEF839989F3E74,\n",
       "    76506960448B8BB6CB2AF337FBE9852E1ABB1486,\n",
       "    null\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidx.get_hashes([0, 40, 7, 42, 190])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<pyarrow.lib.ChunkedArray object at 0x120b61e90>\n",
       " [\n",
       "   [\n",
       "     0,\n",
       "     1,\n",
       "     2,\n",
       "     3,\n",
       "     4\n",
       "   ]\n",
       " ],\n",
       " <pyarrow.lib.ChunkedArray object at 0x15a2d1800>\n",
       " [\n",
       "   [\n",
       "     40,\n",
       "     41,\n",
       "     42,\n",
       "     43,\n",
       "     44\n",
       "   ]\n",
       " ])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidx.get_indices(test_hashes[:5]), hidx.get_indices(test_hashes2[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashes = fast_generate_hashes(int(2e7))\n",
    "t1 = fast_sample_pairs(hashes, int(1e8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('7653.24MB', '4135.36MB', '1907.35MB', '6042.71MB')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def index_probability_table(table: pa.Table) -> tuple[HashIndex, pa.Table]:\n",
    "\n",
    "    hidx = HashIndex(hash_type='sha1')\n",
    "\n",
    "    table = table.set_column(\n",
    "        table.column_names.index(\"left\"),\n",
    "        \"left\",\n",
    "        hidx.insert_hashes(table['left'])\n",
    "    )\n",
    "    table = table.set_column(\n",
    "        table.column_names.index(\"right\"),\n",
    "        \"right\",\n",
    "        hidx.insert_hashes(table['right'])\n",
    "    )\n",
    "    table = table.set_column(\n",
    "        table.column_names.index(\"hash\"),\n",
    "        \"hash\",\n",
    "        hidx.insert_hashes(table['hash'])\n",
    "    )\n",
    "\n",
    "    return hidx, table\n",
    "\n",
    "hidx, t2 = index_probability_table(t1)\n",
    "\n",
    "f\"{t1.nbytes / (1024 * 1024):.2f}MB\", f\"{hidx.table.nbytes / (1024 * 1024):.2f}MB\", f\"{t2.nbytes / (1024 * 1024):.2f}MB\", f\"{(hidx.table.nbytes + t2.nbytes) / (1024 * 1024):.2f}MB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.75"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(15 * 15) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.1"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(4230 / 5) / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5mb/s upload speed -- 25 mins for 7.6Gb, 14min for 4.3Gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "\n",
    "file_path = Path.cwd()\n",
    "\n",
    "pq.write_table(\n",
    "    t2,\n",
    "    file_path / 'probabilities_normalised_brot.parquet',\n",
    "    compression='BROTLI',\n",
    "    # compression_level=16,\n",
    "    # use_dictionary=True,\n",
    "    # write_statistics=True,\n",
    "    # use_byte_stream_split=True,\n",
    "    # row_group_size=1048576  # 1MB row groups\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidx.to_parquet(file_path / 'hash_index.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Impose threshold (don't store below x, default x)\n",
    "* Can we automate to check rub probabilities?\n",
    "    * Find any gigantic clusters as a result\n",
    "    * Run at plausible thresholds?\n",
    "* Is threshold but need a rule of thumb\n",
    "    * If dedupe, n^2 - n/2\n",
    "    * If link, n^2 - n/2 ish\n",
    "* \"How do you know that\" is hard in a chain, and people want that\n",
    "\n",
    "* duckdb from pg OR pg from duckdb -- both available to work with pg\n",
    "    * Bad for atomic updates\n",
    "\n",
    "---\n",
    "\n",
    "100m\n",
    "\n",
    "ZSTD 10s at 1, 1.86Gb\n",
    "ZSTD 10s at 4, 1.86Gb \n",
    "ZSTD 50s at 15, 1.86Gb\n",
    "ZSTD 4m at 16, 1.86Gb\n",
    "ZSTD 7m at 22, 1.52Gb\n",
    "BROTLI 2m at default, 1.57Gb\n",
    "\n",
    "Index\n",
    "\n",
    "Snappy 13s at default, 3.8Gb\n",
    "BROTLI 6m at default, 2.6Gb\n",
    "ZSTD 20s at default, 2.75Gb\n",
    "\n",
    "ZSTD balanced between the two: 4.235Gb (3Gb saving)\n",
    "\n",
    "--\n",
    "\n",
    "Work through pg/duckdb idea\n",
    "\n",
    "- Clusters and contains need appending -- parquet or postgres?\n",
    "- How is duckdb informed about new parquet\n",
    "    - Lambda? API?\n",
    "- Can this perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Duckdb\n",
    "    * R/W or R mode -- we'd launch in one or tother\n",
    "        * No lambdas, just run the new command\n",
    "* Consolidating a dataset (pruning old records) -- like upsert\n",
    "    * 3 ways\n",
    "        * duckdb directly, w process. Load two parquets, prune old\n",
    "        * pandas\n",
    "        * polars\n",
    "* (?) married to parquet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
