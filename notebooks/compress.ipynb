{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compression notes\n",
    "\n",
    "* 100m SHA-1 hashes, binary, binary, binary, float\n",
    "    * Standard: 7.6Gb, written in 23 seconds\n",
    "    * ZSTD 8: 6.85Gb, written in 3 mins\n",
    "    * ZSTD 15: 6.85Gb, written in 13 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import sha1\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import random\n",
    "\n",
    "def fast_generate_hashes(n: int = int(2e7)) -> pa.Array:\n",
    "    \"\"\"Generate n SHA1 hashes using PyArrow arrays.\"\"\"\n",
    "    rand = random.randint(0, int(1e5))\n",
    "    hashes = [\n",
    "        sha1((i + rand).to_bytes(8, 'big')).digest() \n",
    "        for i in range(int(n))\n",
    "    ]\n",
    "    return pa.array(hashes, type=pa.binary())\n",
    "\n",
    "def fast_sample_pairs(hashes: pa.Array, n: int = int(1e8)) -> pa.Table:\n",
    "    \"\"\"Generate hash pairs with random new hashes.\"\"\"\n",
    "    hash_count = len(hashes)\n",
    "    \n",
    "    # Generate indices\n",
    "    left = np.random.randint(0, hash_count, n)\n",
    "    right = np.random.randint(0, hash_count - 1, n)\n",
    "    right += (right >= left)\n",
    "    \n",
    "    # Take values using PyArrow\n",
    "    left_hashes = hashes.take(pa.array(left))\n",
    "    right_hashes = hashes.take(pa.array(right))\n",
    "    \n",
    "    # Generate probabilities as PyArrow array (between 0.7 and 1.0 to 2 DP)\n",
    "    probs = pa.array(np.round(0.7 + 0.3 * np.random.random(n), 2), type=pa.float64())\n",
    "    \n",
    "    # Generate completely new random hashes instead of combining\n",
    "    new_hashes = [sha1(i.to_bytes(8, 'big')).digest() \n",
    "                  for i in range(n)]\n",
    "    combined_arr = pa.array(new_hashes, type=pa.binary())\n",
    "    \n",
    "    # Create table directly with PyArrow\n",
    "    return pa.table({\n",
    "        'hash': combined_arr,\n",
    "        'left': left_hashes,\n",
    "        'right': right_hashes,\n",
    "        'probability': probs\n",
    "    })\n",
    "\n",
    "def fast_sample_pairs_lr(left: pa.Array, right: pa.Array, n: int = int(1e8)) -> pa.Table:\n",
    "    \"\"\"Generate hash pairs with random new hashes.\n",
    "    \n",
    "    Assumes left and right are the same length.\n",
    "    \"\"\"\n",
    "    # Calculate total size of product space\n",
    "    hash_count = len(left)\n",
    "    total_pairs = len(left) * len(right)\n",
    "    \n",
    "    # If n is larger than total possible pairs, adjust n\n",
    "    n = min(n, total_pairs)\n",
    "    \n",
    "    # Generate n random indices from the product space\n",
    "    flat_indices = np.random.choice(total_pairs, size=n, replace=False)\n",
    "    \n",
    "    # Convert flat indices back to left and right indices\n",
    "    left_indices = flat_indices // hash_count\n",
    "    right_indices = flat_indices % hash_count\n",
    "    \n",
    "    # Take values using PyArrow for better performance\n",
    "    left_hashes = left.take(pa.array(left_indices))\n",
    "    right_hashes = right.take(pa.array(right_indices))\n",
    "    \n",
    "    # Generate probabilities as PyArrow array (between 0.7 and 1.0 to 2 DP)\n",
    "    probs = pa.array(np.round(0.7 + 0.3 * np.random.random(n), 2), type=pa.float64())\n",
    "    \n",
    "    # Generate completely new random hashes instead of combining\n",
    "    new_hashes = [\n",
    "        sha1(i.to_bytes(8, 'big')).digest() \n",
    "        for i in range(n)\n",
    "    ]\n",
    "    combined_arr = pa.array(new_hashes, type=pa.binary())\n",
    "    \n",
    "    # Create table directly with PyArrow\n",
    "    return pa.table({\n",
    "        'hash': combined_arr,\n",
    "        'left': left_hashes,\n",
    "        'right': right_hashes,\n",
    "        'probability': probs\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rustworkx as rx\n",
    "from matchbox.common.hash import list_to_value_ordered_hash\n",
    "\n",
    "def to_clusters(results: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Converts probabilities into a list of connected components formed at each threshold.\n",
    "\n",
    "    Returns:\n",
    "        Probabilities sorted by threshold descending.\n",
    "    \"\"\"\n",
    "    G = rx.PyGraph()\n",
    "    added: dict[bytes, int] = {}\n",
    "    components: dict[str, list] = {\"parent\": [], \"child\": [], \"threshold\": []}\n",
    "\n",
    "    # Sort probabilities descending and group by probability\n",
    "    edges_df = results.select(['left', 'right', 'probability']).sort_by([(\"probability\", \"descending\")])\n",
    "    \n",
    "    # Get unique probability thresholds, sorted\n",
    "    thresholds = pa.compute.unique(edges_df.column('probability'))\n",
    "\n",
    "    # Process edges grouped by probability threshold\n",
    "    for prob in thresholds.to_pylist():\n",
    "        mask = pa.compute.equal(edges_df.column('probability'), prob)\n",
    "        threshold_edges = edges_df.filter(mask)\n",
    "        # Get state before adding this batch of edges\n",
    "        old_components = {frozenset(comp) for comp in rx.connected_components(G)}\n",
    "\n",
    "        # Add all nodes and edges at this probability threshold\n",
    "        edge_values = zip(\n",
    "            threshold_edges.column('left').to_pylist(),\n",
    "            threshold_edges.column('right').to_pylist()\n",
    "        )\n",
    "\n",
    "        for left, right in edge_values:\n",
    "            for hash_val in (left, right):\n",
    "                if hash_val not in added:\n",
    "                    idx = G.add_node(hash_val)\n",
    "                    added[hash_val] = idx\n",
    "\n",
    "            G.add_edge(added[left], added[right], None)\n",
    "\n",
    "        new_components = {frozenset(comp) for comp in rx.connected_components(G)}\n",
    "        changed_components = new_components - old_components\n",
    "\n",
    "        # For each changed component, add ALL members at current threshold\n",
    "        for comp in changed_components:\n",
    "            children = sorted([G.get_node_data(n) for n in comp])\n",
    "            parent = list_to_value_ordered_hash(children)\n",
    "\n",
    "            components[\"parent\"].extend([parent] * len(children))\n",
    "            components[\"child\"].extend(children)\n",
    "            components[\"threshold\"].extend([prob] * len(children))\n",
    "\n",
    "    return pa.Table.from_pydict(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def _cluster_results_to_hierarchical(\n",
    "    probabilities: pa.Table,\n",
    "    clusters: pa.Table,\n",
    ") -> list[tuple[bytes, bytes, float]]:\n",
    "    \"\"\"\n",
    "    Converts results to a hierarchical structure by building up from base components.\n",
    "\n",
    "    Args:\n",
    "        probabilities: Original pairwise probabilities containing base components\n",
    "        clusters: Connected components at each threshold\n",
    "\n",
    "    Returns:\n",
    "        List of (parent, child, threshold) tuples representing the hierarchy\n",
    "    \"\"\"\n",
    "    thresholds = pa.compute.unique(clusters['threshold']).sort(order='descending')\n",
    "\n",
    "    # Add all clusters corresponding to a simple two-item probability edge\n",
    "\n",
    "    hierarchy = []\n",
    "    # Convert to record batches for efficient iteration\n",
    "    for batch in probabilities.to_batches():\n",
    "        parent_array = batch.column('hash')\n",
    "        left_array = batch.column('left')\n",
    "        right_array = batch.column('right')\n",
    "        prob_array = batch.column('probability')\n",
    "        \n",
    "        for i in range(len(batch)):\n",
    "            parent = parent_array[i].as_py()\n",
    "            left_id = left_array[i].as_py()\n",
    "            right_id = right_array[i].as_py()\n",
    "            prob = float(prob_array[i].as_py())\n",
    "            hierarchy.extend(\n",
    "                [(parent, left_id, prob), (parent, right_id, prob)]\n",
    "            )\n",
    "\n",
    "    # Create adjacency structure for quick lookups\n",
    "    adj_dict: dict[bytes, set[tuple[bytes, float]]] = defaultdict(set)\n",
    "    for parent, child, prob in hierarchy:\n",
    "        adj_dict[child].add((parent, prob))\n",
    "\n",
    "    # Process each threshold level, getting clusters at each threshold\n",
    "    for threshold in thresholds:\n",
    "        threshold_float = float(threshold.as_py())\n",
    "\n",
    "        # Filter clusters at current threshold\n",
    "        mask = pa.compute.equal(clusters.column('threshold'), threshold)\n",
    "        current_clusters = clusters.filter(mask)\n",
    "\n",
    "        # Group by parent\n",
    "        parent_groups = {}\n",
    "        for batch in current_clusters.to_batches():\n",
    "            parent_col = batch.column('parent')\n",
    "            child_col = batch.column('child')\n",
    "            for i in range(len(batch)):\n",
    "                parent = parent_col[i].as_py()\n",
    "                child = child_col[i].as_py()\n",
    "                if parent not in parent_groups:\n",
    "                    parent_groups[parent] = set()\n",
    "                parent_groups[parent].add(child)\n",
    "\n",
    "        # Process each component\n",
    "        for parent, members in parent_groups.items():\n",
    "            if len(members) <= 2:\n",
    "                continue\n",
    "\n",
    "            seen = set(members)\n",
    "            current = set(members)\n",
    "            ultimate_parents = set()\n",
    "\n",
    "            # Keep traversing until we've explored all paths\n",
    "            while current:\n",
    "                next_level = set()\n",
    "                # If any current nodes have no parents above threshold,\n",
    "                # they are ultimate parents for this threshold\n",
    "                for node in current:\n",
    "                    parents = {\n",
    "                        p for p, prob in adj_dict[node] if prob >= threshold_float\n",
    "                    }\n",
    "                    next_parents = parents - seen\n",
    "                    if not parents:  # No parents = ultimate parent\n",
    "                        ultimate_parents.add(node)\n",
    "\n",
    "                    next_level.update(next_parents)\n",
    "                    seen.update(parents)\n",
    "\n",
    "                current = next_level\n",
    "\n",
    "            for up in ultimate_parents:\n",
    "                hierarchy.append((parent, up, threshold_float))\n",
    "                adj_dict[up].add((parent, threshold_float))\n",
    "\n",
    "    return sorted(hierarchy, key=lambda x: (x[2], x[0], x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "def _cluster_results_to_hierarchical_2(\n",
    "    probabilities: pa.Table,\n",
    "    clusters: pa.Table,\n",
    ") -> list[tuple[bytes, bytes, float]]:\n",
    "    \"\"\"\n",
    "    Converts results to a hierarchical structure by building up from base components.\n",
    "    Optimized to maximize Arrow operations and minimize Python object conversions.\n",
    "\n",
    "    Args:\n",
    "        probabilities: Original pairwise probabilities containing base components\n",
    "        clusters: Connected components at each threshold\n",
    "\n",
    "    Returns:\n",
    "        List of (parent, child, threshold) tuples representing the hierarchy\n",
    "    \"\"\"\n",
    "    thresholds = pa.compute.unique(clusters['threshold']).sort(order='descending')\n",
    "    \n",
    "    # Initialize hierarchy with base probabilities\n",
    "    # Use Arrow to create the initial hierarchy structure\n",
    "    probabilities_hash_array = probabilities['hash'].combine_chunks()\n",
    "    probabilities_left_array = probabilities['left'].combine_chunks()\n",
    "    probabilities_right_array = probabilities['right'].combine_chunks()\n",
    "    probabilities_prob_array = probabilities['probability'].combine_chunks()\n",
    "    hierarchy_parent = pa.concat_arrays([\n",
    "        probabilities_hash_array,\n",
    "        probabilities_hash_array\n",
    "    ])\n",
    "    hierarchy_child = pa.concat_arrays([\n",
    "        probabilities_left_array,\n",
    "        probabilities_right_array\n",
    "    ])\n",
    "    hierarchy_prob = pa.concat_arrays([\n",
    "        probabilities_prob_array,\n",
    "        probabilities_prob_array\n",
    "    ])\n",
    "    \n",
    "    # Create initial hierarchy table\n",
    "    hierarchy_table = pa.Table.from_arrays(\n",
    "        [hierarchy_parent, hierarchy_child, hierarchy_prob],\n",
    "        names=['parent', 'child', 'probability']\n",
    "    )\n",
    "    \n",
    "    # Convert to Python objects only once for adjacency structure\n",
    "    hierarchy = list(zip(\n",
    "        hierarchy_table['parent'].to_pylist(),\n",
    "        hierarchy_table['child'].to_pylist(),\n",
    "        hierarchy_table['probability'].to_pylist()\n",
    "    ))\n",
    "    \n",
    "    # Create adjacency structure for quick lookups\n",
    "    adj_dict: dict[bytes, set[tuple[bytes, float]]] = defaultdict(set)\n",
    "    for parent, child, prob in hierarchy:\n",
    "        adj_dict[child].add((parent, prob))\n",
    "\n",
    "    # Process each threshold level\n",
    "    for threshold in thresholds.to_pylist():  # Convert threshold array once\n",
    "        threshold_float = float(threshold)\n",
    "        \n",
    "        # Filter clusters at current threshold using Arrow operations\n",
    "        current_clusters = clusters.filter(pc.equal(clusters['threshold'], threshold))\n",
    "        \n",
    "        # Group by parent using Arrow operations\n",
    "        parent_groups = defaultdict(set)\n",
    "        parent_col = current_clusters['parent']\n",
    "        child_col = current_clusters['child']\n",
    "        \n",
    "        # Batch process the grouping\n",
    "        parent_list = parent_col.to_pylist()\n",
    "        child_list = child_col.to_pylist()\n",
    "        for parent, child in zip(parent_list, child_list):\n",
    "            parent_groups[parent].add(child)\n",
    "\n",
    "        # Process each component\n",
    "        for parent, members in parent_groups.items():\n",
    "            if len(members) <= 2:\n",
    "                continue\n",
    "\n",
    "            seen = set(members)\n",
    "            current = set(members)\n",
    "            ultimate_parents = set()\n",
    "\n",
    "            while current:\n",
    "                next_level = set()\n",
    "                for node in current:\n",
    "                    parents = {\n",
    "                        p for p, prob in adj_dict[node] if prob >= threshold_float\n",
    "                    }\n",
    "                    next_parents = parents - seen\n",
    "                    if not parents:\n",
    "                        ultimate_parents.add(node)\n",
    "\n",
    "                    next_level.update(next_parents)\n",
    "                    seen.update(parents)\n",
    "\n",
    "                current = next_level\n",
    "\n",
    "            # Batch add new hierarchy entries\n",
    "            for up in ultimate_parents:\n",
    "                hierarchy.append((parent, up, threshold_float))\n",
    "                adj_dict[up].add((parent, threshold_float))\n",
    "\n",
    "    return sorted(hierarchy, key=lambda x: (x[2], x[0], x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import logging\n",
    "import time\n",
    "from typing import Any\n",
    "\n",
    "def setup_logger() -> logging.Logger:\n",
    "    \"\"\"Set up a logger with appropriate formatting.\"\"\"\n",
    "    logger = logging.getLogger('cluster_profiler')\n",
    "    if not logger.handlers:  # Avoid adding handlers if they already exist\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "def log_time(logger: logging.Logger, start_time: float, operation: str) -> None:\n",
    "    \"\"\"Log the time taken for an operation.\"\"\"\n",
    "    duration = time.time() - start_time\n",
    "    logger.debug(f\"{operation} took {duration:.4f} seconds\")\n",
    "\n",
    "def log_memory(logger: logging.Logger, obj: Any, name: str) -> None:\n",
    "    \"\"\"Log memory usage of an object.\"\"\"\n",
    "    if hasattr(obj, 'nbytes'):\n",
    "        size_mb = obj.nbytes / (1024 * 1024)\n",
    "        logger.debug(f\"Memory usage of {name}: {size_mb:.2f} MB\")\n",
    "\n",
    "def _cluster_results_to_hierarchical_3(\n",
    "    probabilities: pa.Table,\n",
    "    clusters: pa.Table,\n",
    ") -> list[tuple[bytes, bytes, float]]:\n",
    "    \"\"\"\n",
    "    Converts results to a hierarchical structure by building up from base components.\n",
    "    Optimized to maximize Arrow operations and minimize Python object conversions.\n",
    "\n",
    "    Args:\n",
    "        probabilities: Original pairwise probabilities containing base components\n",
    "        clusters: Connected components at each threshold\n",
    "\n",
    "    Returns:\n",
    "        List of (parent, child, threshold) tuples representing the hierarchy\n",
    "    \"\"\"\n",
    "    logger = setup_logger()\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    logger.info(\"Starting clustering process\")\n",
    "    logger.debug(f\"Input probabilities table size: {len(probabilities)}\")\n",
    "    logger.debug(f\"Input clusters table size: {len(clusters)}\")\n",
    "    \n",
    "    # Threshold computation\n",
    "    thresh_start = time.time()\n",
    "    thresholds = pa.compute.unique(clusters['threshold']).sort(order='descending')\n",
    "    log_time(logger, thresh_start, \"Threshold computation\")\n",
    "    logger.debug(f\"Number of unique thresholds: {len(thresholds)}\")\n",
    "    \n",
    "    # Initialize hierarchy\n",
    "    init_start = time.time()\n",
    "    probabilities_hash_array = probabilities['hash'].combine_chunks()\n",
    "    probabilities_left_array = probabilities['left'].combine_chunks()\n",
    "    probabilities_right_array = probabilities['right'].combine_chunks()\n",
    "    probabilities_prob_array = probabilities['probability'].combine_chunks()\n",
    "    \n",
    "    hierarchy_parent = pa.concat_arrays([\n",
    "        probabilities_hash_array,\n",
    "        probabilities_hash_array\n",
    "    ])\n",
    "    hierarchy_child = pa.concat_arrays([\n",
    "        probabilities_left_array,\n",
    "        probabilities_right_array\n",
    "    ])\n",
    "    hierarchy_prob = pa.concat_arrays([\n",
    "        probabilities_prob_array,\n",
    "        probabilities_prob_array\n",
    "    ])\n",
    "    log_time(logger, init_start, \"Hierarchy initialization\")\n",
    "    \n",
    "    # Create initial hierarchy table\n",
    "    table_start = time.time()\n",
    "    hierarchy_table = pa.Table.from_arrays(\n",
    "        [hierarchy_parent, hierarchy_child, hierarchy_prob],\n",
    "        names=['parent', 'child', 'probability']\n",
    "    )\n",
    "    log_time(logger, table_start, \"Hierarchy table creation\")\n",
    "    log_memory(logger, hierarchy_table, \"hierarchy_table\")\n",
    "    \n",
    "    # Convert to Python objects\n",
    "    conv_start = time.time()\n",
    "    hierarchy = list(zip(\n",
    "        hierarchy_table['parent'].to_pylist(),\n",
    "        hierarchy_table['child'].to_pylist(),\n",
    "        hierarchy_table['probability'].to_pylist()\n",
    "    ))\n",
    "    log_time(logger, conv_start, \"Python object conversion\")\n",
    "    logger.debug(f\"Initial hierarchy size: {len(hierarchy)}\")\n",
    "    \n",
    "    # Create adjacency structure\n",
    "    adj_start = time.time()\n",
    "    adj_dict: dict[bytes, set[tuple[bytes, float]]] = defaultdict(set)\n",
    "    for parent, child, prob in hierarchy:\n",
    "        adj_dict[child].add((parent, prob))\n",
    "    log_time(logger, adj_start, \"Adjacency dictionary creation\")\n",
    "    logger.debug(f\"Adjacency dictionary size: {len(adj_dict)}\")\n",
    "    \n",
    "    # Process thresholds\n",
    "    for threshold in thresholds.to_pylist():\n",
    "        thresh_iter_start = time.time()\n",
    "        logger.debug(f\"Processing threshold: {threshold}\")\n",
    "        threshold_float = float(threshold)\n",
    "        \n",
    "        # Filter clusters\n",
    "        filter_start = time.time()\n",
    "        current_clusters = clusters.filter(pc.equal(clusters['threshold'], threshold))\n",
    "        log_time(logger, filter_start, f\"Cluster filtering for threshold {threshold}\")\n",
    "        logger.debug(f\"Filtered clusters size: {len(current_clusters)}\")\n",
    "        \n",
    "        # Group by parent\n",
    "        group_start = time.time()\n",
    "        parent_groups = defaultdict(set)\n",
    "        parent_list = current_clusters['parent'].to_pylist()\n",
    "        child_list = current_clusters['child'].to_pylist()\n",
    "        for parent, child in zip(parent_list, child_list):\n",
    "            parent_groups[parent].add(child)\n",
    "        log_time(logger, group_start, \"Parent grouping\")\n",
    "        logger.debug(f\"Number of parent groups: {len(parent_groups)}\")\n",
    "        \n",
    "        # Process components\n",
    "        comp_start = time.time()\n",
    "        components_processed = 0\n",
    "        for parent, members in parent_groups.items():\n",
    "            if len(members) <= 2:\n",
    "                continue\n",
    "                \n",
    "            components_processed += 1\n",
    "            seen = set(members)\n",
    "            current = set(members)\n",
    "            ultimate_parents = set()\n",
    "            \n",
    "            while current:\n",
    "                next_level = set()\n",
    "                for node in current:\n",
    "                    parents = {\n",
    "                        p for p, prob in adj_dict[node] if prob >= threshold_float\n",
    "                    }\n",
    "                    next_parents = parents - seen\n",
    "                    if not parents:\n",
    "                        ultimate_parents.add(node)\n",
    "                    \n",
    "                    next_level.update(next_parents)\n",
    "                    seen.update(parents)\n",
    "                \n",
    "                current = next_level\n",
    "            \n",
    "            # Add new hierarchy entries\n",
    "            for up in ultimate_parents:\n",
    "                hierarchy.append((parent, up, threshold_float))\n",
    "                adj_dict[up].add((parent, threshold_float))\n",
    "                \n",
    "        log_time(logger, comp_start, f\"Component processing for threshold {threshold}\")\n",
    "        logger.debug(f\"Components processed: {components_processed}\")\n",
    "        log_time(logger, thresh_iter_start, f\"Total processing for threshold {threshold}\")\n",
    "    \n",
    "    # Final sorting\n",
    "    sort_start = time.time()\n",
    "    result = sorted(hierarchy, key=lambda x: (x[2], x[0], x[1]), reverse=True)\n",
    "    log_time(logger, sort_start, \"Final sorting\")\n",
    "    logger.debug(f\"Final hierarchy size: {len(result)}\")\n",
    "    \n",
    "    log_time(logger, total_start_time, \"Total function execution\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import logging\n",
    "import time\n",
    "\n",
    "def setup_logger() -> logging.Logger:\n",
    "    \"\"\"Set up a logger with appropriate formatting.\"\"\"\n",
    "    logger = logging.getLogger('cluster_profiler')\n",
    "    if not logger.handlers:\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "def _cluster_results_to_hierarchical_4(\n",
    "    probabilities: pa.Table,\n",
    "    clusters: pa.Table,\n",
    ") -> list[tuple[bytes, bytes, float]]:\n",
    "    \"\"\"\n",
    "    Converts results to a hierarchical structure using hybrid approach:\n",
    "    1. Use parent counting to identify potential ultimate parents\n",
    "    2. Use targeted graph traversal to verify and find missed relationships\n",
    "    \"\"\"\n",
    "    logger = setup_logger()\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    logger.info(\"Starting clustering process\")\n",
    "    logger.debug(f\"Input probabilities table size: {len(probabilities)}\")\n",
    "    logger.debug(f\"Input clusters table size: {len(clusters)}\")\n",
    "    \n",
    "    # Initialize hierarchy with base probabilities\n",
    "    init_start = time.time()\n",
    "    probabilities_hash_array = probabilities['hash'].combine_chunks()\n",
    "    probabilities_left_array = probabilities['left'].combine_chunks()\n",
    "    probabilities_right_array = probabilities['right'].combine_chunks()\n",
    "    probabilities_prob_array = probabilities['probability'].combine_chunks()\n",
    "    \n",
    "    hierarchy_parent = pa.concat_arrays([\n",
    "        probabilities_hash_array,\n",
    "        probabilities_hash_array\n",
    "    ])\n",
    "    hierarchy_child = pa.concat_arrays([\n",
    "        probabilities_left_array,\n",
    "        probabilities_right_array\n",
    "    ])\n",
    "    hierarchy_prob = pa.concat_arrays([\n",
    "        probabilities_prob_array,\n",
    "        probabilities_prob_array\n",
    "    ])\n",
    "    \n",
    "    hierarchy_table = pa.Table.from_arrays(\n",
    "        [hierarchy_parent, hierarchy_child, hierarchy_prob],\n",
    "        names=['parent', 'child', 'probability']\n",
    "    )\n",
    "    \n",
    "    # Convert to Python objects\n",
    "    hierarchy = list(zip(\n",
    "        hierarchy_table['parent'].to_pylist(),\n",
    "        hierarchy_table['child'].to_pylist(),\n",
    "        hierarchy_table['probability'].to_pylist()\n",
    "    ))\n",
    "    \n",
    "    # Create initial parent map\n",
    "    adj_dict: dict[bytes, set[tuple[bytes, float]]] = defaultdict(set)\n",
    "    for parent, child, prob in hierarchy:\n",
    "        adj_dict[child].add((parent, prob))\n",
    "    \n",
    "    logger.debug(f\"Initialization took {time.time() - init_start:.4f}s\")\n",
    "    \n",
    "    thresholds = pa.compute.unique(clusters['threshold']).sort(order='descending')\n",
    "    \n",
    "    # Process each threshold\n",
    "    for threshold in thresholds.to_pylist():\n",
    "        threshold_start = time.time()\n",
    "        logger.debug(f\"Processing threshold: {threshold}\")\n",
    "        threshold_float = float(threshold)\n",
    "        \n",
    "        # Filter clusters for this threshold\n",
    "        current_clusters = clusters.filter(pc.equal(clusters['threshold'], threshold))\n",
    "        logger.debug(f\"Filtered clusters size: {len(current_clusters)}\")\n",
    "        \n",
    "        # Group by parent\n",
    "        parent_groups = defaultdict(set)\n",
    "        parent_list = current_clusters['parent'].to_pylist()\n",
    "        child_list = current_clusters['child'].to_pylist()\n",
    "        for parent, child in zip(parent_list, child_list):\n",
    "            parent_groups[parent].add(child)\n",
    "        \n",
    "        comp_start = time.time()\n",
    "        # Process each component\n",
    "        for parent, members in parent_groups.items():\n",
    "            if len(members) <= 2:\n",
    "                continue\n",
    "                \n",
    "            # Count direct parents for each node at this threshold\n",
    "            parent_count = defaultdict(int)\n",
    "            for node in members:\n",
    "                for p, prob in adj_dict[node]:\n",
    "                    if prob >= threshold_float:\n",
    "                        parent_count[node] += 1\n",
    "            \n",
    "            # Find initial candidates with no direct parents\n",
    "            candidates = {\n",
    "                node for node in members \n",
    "                if parent_count[node] == 0\n",
    "            }\n",
    "            \n",
    "            # Now do targeted graph traversal from each non-candidate\n",
    "            # to identify any candidates that have indirect parents\n",
    "            not_ultimate = set()\n",
    "            for node in members - candidates:\n",
    "                if node in not_ultimate:\n",
    "                    continue\n",
    "                    \n",
    "                # Start traversal from this node\n",
    "                stack = [node]\n",
    "                seen = {node}\n",
    "                \n",
    "                while stack:\n",
    "                    current = stack.pop()\n",
    "                    for p, prob in adj_dict[current]:\n",
    "                        if prob >= threshold_float and p not in seen:\n",
    "                            if p in candidates:\n",
    "                                # Found a path to a candidate - it can't be an ultimate parent\n",
    "                                candidates.remove(p)\n",
    "                            stack.append(p)\n",
    "                            seen.add(p)\n",
    "                            not_ultimate.add(p)\n",
    "            \n",
    "            # Add hierarchy entries for true ultimate parents\n",
    "            for up in candidates:\n",
    "                hierarchy.append((parent, up, threshold_float))\n",
    "                adj_dict[up].add((parent, threshold_float))\n",
    "        \n",
    "        logger.debug(f\"Component processing took {time.time() - comp_start:.4f}s\")\n",
    "        logger.debug(f\"Total threshold processing took {time.time() - threshold_start:.4f}s\")\n",
    "    \n",
    "    sort_start = time.time()\n",
    "    result = sorted(hierarchy, key=lambda x: (x[2], x[0], x[1]), reverse=True)\n",
    "    logger.debug(f\"Sorting took {time.time() - sort_start:.4f}s\")\n",
    "    logger.debug(f\"Final hierarchy size: {len(result)}\")\n",
    "    \n",
    "    total_time = time.time() - total_start_time\n",
    "    logger.debug(f\"Total execution time: {total_time:.4f}s\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashes = fast_generate_hashes(int(2e5))\n",
    "probabilities = fast_sample_pairs(hashes, int(1e6))\n",
    "clusters = to_clusters(probabilities)\n",
    "hierarchical = _cluster_results_to_hierarchical(probabilities, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical = _cluster_results_to_hierarchical(probabilities, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 17:39:35,730 - cluster_profiler - INFO - Starting clustering process\n",
      "2024-12-10 17:39:35,732 - cluster_profiler - DEBUG - Input probabilities table size: 1000000\n",
      "2024-12-10 17:39:35,732 - cluster_profiler - DEBUG - Input clusters table size: 5279385\n",
      "2024-12-10 17:39:35,753 - cluster_profiler - DEBUG - Threshold computation took 0.0204 seconds\n",
      "2024-12-10 17:39:35,754 - cluster_profiler - DEBUG - Number of unique thresholds: 30\n",
      "2024-12-10 17:39:35,990 - cluster_profiler - DEBUG - Hierarchy initialization took 0.2352 seconds\n",
      "2024-12-10 17:39:35,992 - cluster_profiler - DEBUG - Hierarchy table creation took 0.0002 seconds\n",
      "2024-12-10 17:39:35,993 - cluster_profiler - DEBUG - Memory usage of hierarchy_table: 106.81 MB\n",
      "2024-12-10 17:39:40,565 - cluster_profiler - DEBUG - Python object conversion took 4.5699 seconds\n",
      "2024-12-10 17:39:40,566 - cluster_profiler - DEBUG - Initial hierarchy size: 2000000\n",
      "2024-12-10 17:39:42,735 - cluster_profiler - DEBUG - Adjacency dictionary creation took 2.1686 seconds\n",
      "2024-12-10 17:39:42,736 - cluster_profiler - DEBUG - Adjacency dictionary size: 199989\n",
      "2024-12-10 17:39:42,737 - cluster_profiler - DEBUG - Processing threshold: 1.0\n",
      "2024-12-10 17:39:42,743 - cluster_profiler - DEBUG - Cluster filtering for threshold 1.0 took 0.0055 seconds\n",
      "2024-12-10 17:39:42,744 - cluster_profiler - DEBUG - Filtered clusters size: 30689\n",
      "2024-12-10 17:39:42,822 - cluster_profiler - DEBUG - Parent grouping took 0.0781 seconds\n",
      "2024-12-10 17:39:42,823 - cluster_profiler - DEBUG - Number of parent groups: 14054\n",
      "2024-12-10 17:39:42,873 - cluster_profiler - DEBUG - Component processing for threshold 1.0 took 0.0496 seconds\n",
      "2024-12-10 17:39:42,874 - cluster_profiler - DEBUG - Components processed: 2069\n",
      "2024-12-10 17:39:42,875 - cluster_profiler - DEBUG - Total processing for threshold 1.0 took 0.1380 seconds\n",
      "2024-12-10 17:39:42,875 - cluster_profiler - DEBUG - Processing threshold: 0.99\n",
      "2024-12-10 17:39:42,884 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.99 took 0.0081 seconds\n",
      "2024-12-10 17:39:42,885 - cluster_profiler - DEBUG - Filtered clusters size: 63676\n",
      "2024-12-10 17:39:43,060 - cluster_profiler - DEBUG - Parent grouping took 0.1746 seconds\n",
      "2024-12-10 17:39:43,061 - cluster_profiler - DEBUG - Number of parent groups: 21683\n",
      "2024-12-10 17:39:43,369 - cluster_profiler - DEBUG - Component processing for threshold 0.99 took 0.3080 seconds\n",
      "2024-12-10 17:39:43,370 - cluster_profiler - DEBUG - Components processed: 9432\n",
      "2024-12-10 17:39:43,371 - cluster_profiler - DEBUG - Total processing for threshold 0.99 took 0.4955 seconds\n",
      "2024-12-10 17:39:43,372 - cluster_profiler - DEBUG - Processing threshold: 0.98\n",
      "2024-12-10 17:39:43,379 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.98 took 0.0071 seconds\n",
      "2024-12-10 17:39:43,380 - cluster_profiler - DEBUG - Filtered clusters size: 83436\n",
      "2024-12-10 17:39:44,183 - cluster_profiler - DEBUG - Parent grouping took 0.8020 seconds\n",
      "2024-12-10 17:39:44,184 - cluster_profiler - DEBUG - Number of parent groups: 17344\n",
      "2024-12-10 17:39:45,194 - cluster_profiler - DEBUG - Component processing for threshold 0.98 took 1.0095 seconds\n",
      "2024-12-10 17:39:45,194 - cluster_profiler - DEBUG - Components processed: 11006\n",
      "2024-12-10 17:39:45,195 - cluster_profiler - DEBUG - Total processing for threshold 0.98 took 1.8230 seconds\n",
      "2024-12-10 17:39:45,195 - cluster_profiler - DEBUG - Processing threshold: 0.97\n",
      "2024-12-10 17:39:45,204 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.97 took 0.0078 seconds\n",
      "2024-12-10 17:39:45,204 - cluster_profiler - DEBUG - Filtered clusters size: 108385\n",
      "2024-12-10 17:39:45,463 - cluster_profiler - DEBUG - Parent grouping took 0.2578 seconds\n",
      "2024-12-10 17:39:45,464 - cluster_profiler - DEBUG - Number of parent groups: 9956\n",
      "2024-12-10 17:39:56,241 - cluster_profiler - DEBUG - Component processing for threshold 0.97 took 10.7772 seconds\n",
      "2024-12-10 17:39:56,242 - cluster_profiler - DEBUG - Components processed: 6760\n",
      "2024-12-10 17:39:56,242 - cluster_profiler - DEBUG - Total processing for threshold 0.97 took 11.0472 seconds\n",
      "2024-12-10 17:39:56,243 - cluster_profiler - DEBUG - Processing threshold: 0.96\n",
      "2024-12-10 17:39:56,250 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.96 took 0.0071 seconds\n",
      "2024-12-10 17:39:56,251 - cluster_profiler - DEBUG - Filtered clusters size: 134693\n",
      "2024-12-10 17:39:56,535 - cluster_profiler - DEBUG - Parent grouping took 0.2837 seconds\n",
      "2024-12-10 17:39:56,536 - cluster_profiler - DEBUG - Number of parent groups: 4484\n",
      "2024-12-10 17:40:08,293 - cluster_profiler - DEBUG - Component processing for threshold 0.96 took 11.7563 seconds\n",
      "2024-12-10 17:40:08,293 - cluster_profiler - DEBUG - Components processed: 2755\n",
      "2024-12-10 17:40:08,294 - cluster_profiler - DEBUG - Total processing for threshold 0.96 took 12.0513 seconds\n",
      "2024-12-10 17:40:08,295 - cluster_profiler - DEBUG - Processing threshold: 0.95\n",
      "2024-12-10 17:40:08,305 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.95 took 0.0096 seconds\n",
      "2024-12-10 17:40:08,306 - cluster_profiler - DEBUG - Filtered clusters size: 155322\n",
      "2024-12-10 17:40:08,620 - cluster_profiler - DEBUG - Parent grouping took 0.3136 seconds\n",
      "2024-12-10 17:40:08,621 - cluster_profiler - DEBUG - Number of parent groups: 1895\n",
      "2024-12-10 17:40:15,285 - cluster_profiler - DEBUG - Component processing for threshold 0.95 took 6.6642 seconds\n",
      "2024-12-10 17:40:15,286 - cluster_profiler - DEBUG - Components processed: 993\n",
      "2024-12-10 17:40:15,287 - cluster_profiler - DEBUG - Total processing for threshold 0.95 took 6.9918 seconds\n",
      "2024-12-10 17:40:15,287 - cluster_profiler - DEBUG - Processing threshold: 0.94\n",
      "2024-12-10 17:40:15,294 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.94 took 0.0069 seconds\n",
      "2024-12-10 17:40:15,295 - cluster_profiler - DEBUG - Filtered clusters size: 169726\n",
      "2024-12-10 17:40:15,633 - cluster_profiler - DEBUG - Parent grouping took 0.3371 seconds\n",
      "2024-12-10 17:40:15,633 - cluster_profiler - DEBUG - Number of parent groups: 807\n",
      "2024-12-10 17:40:18,984 - cluster_profiler - DEBUG - Component processing for threshold 0.94 took 3.3506 seconds\n",
      "2024-12-10 17:40:18,985 - cluster_profiler - DEBUG - Components processed: 368\n",
      "2024-12-10 17:40:18,986 - cluster_profiler - DEBUG - Total processing for threshold 0.94 took 3.6987 seconds\n",
      "2024-12-10 17:40:18,986 - cluster_profiler - DEBUG - Processing threshold: 0.93\n",
      "2024-12-10 17:40:18,995 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.93 took 0.0080 seconds\n",
      "2024-12-10 17:40:18,995 - cluster_profiler - DEBUG - Filtered clusters size: 179643\n",
      "2024-12-10 17:40:19,358 - cluster_profiler - DEBUG - Parent grouping took 0.3623 seconds\n",
      "2024-12-10 17:40:19,359 - cluster_profiler - DEBUG - Number of parent groups: 364\n",
      "2024-12-10 17:40:21,576 - cluster_profiler - DEBUG - Component processing for threshold 0.93 took 2.2170 seconds\n",
      "2024-12-10 17:40:21,577 - cluster_profiler - DEBUG - Components processed: 141\n",
      "2024-12-10 17:40:21,578 - cluster_profiler - DEBUG - Total processing for threshold 0.93 took 2.5918 seconds\n",
      "2024-12-10 17:40:21,579 - cluster_profiler - DEBUG - Processing threshold: 0.92\n",
      "2024-12-10 17:40:21,590 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.92 took 0.0111 seconds\n",
      "2024-12-10 17:40:21,591 - cluster_profiler - DEBUG - Filtered clusters size: 186031\n",
      "2024-12-10 17:40:21,963 - cluster_profiler - DEBUG - Parent grouping took 0.3713 seconds\n",
      "2024-12-10 17:40:21,964 - cluster_profiler - DEBUG - Number of parent groups: 160\n",
      "2024-12-10 17:40:23,840 - cluster_profiler - DEBUG - Component processing for threshold 0.92 took 1.8757 seconds\n",
      "2024-12-10 17:40:23,841 - cluster_profiler - DEBUG - Components processed: 61\n",
      "2024-12-10 17:40:23,841 - cluster_profiler - DEBUG - Total processing for threshold 0.92 took 2.2626 seconds\n",
      "2024-12-10 17:40:23,842 - cluster_profiler - DEBUG - Processing threshold: 0.91\n",
      "2024-12-10 17:40:23,850 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.91 took 0.0078 seconds\n",
      "2024-12-10 17:40:23,851 - cluster_profiler - DEBUG - Filtered clusters size: 190338\n",
      "2024-12-10 17:40:24,247 - cluster_profiler - DEBUG - Parent grouping took 0.3960 seconds\n",
      "2024-12-10 17:40:24,248 - cluster_profiler - DEBUG - Number of parent groups: 85\n",
      "2024-12-10 17:40:26,008 - cluster_profiler - DEBUG - Component processing for threshold 0.91 took 1.7597 seconds\n",
      "2024-12-10 17:40:26,009 - cluster_profiler - DEBUG - Components processed: 26\n",
      "2024-12-10 17:40:26,010 - cluster_profiler - DEBUG - Total processing for threshold 0.91 took 2.1678 seconds\n",
      "2024-12-10 17:40:26,010 - cluster_profiler - DEBUG - Processing threshold: 0.9\n",
      "2024-12-10 17:40:26,019 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.9 took 0.0080 seconds\n",
      "2024-12-10 17:40:26,020 - cluster_profiler - DEBUG - Filtered clusters size: 193311\n",
      "2024-12-10 17:40:26,413 - cluster_profiler - DEBUG - Parent grouping took 0.3929 seconds\n",
      "2024-12-10 17:40:26,413 - cluster_profiler - DEBUG - Number of parent groups: 43\n",
      "2024-12-10 17:40:28,944 - cluster_profiler - DEBUG - Component processing for threshold 0.9 took 2.5296 seconds\n",
      "2024-12-10 17:40:28,945 - cluster_profiler - DEBUG - Components processed: 13\n",
      "2024-12-10 17:40:28,945 - cluster_profiler - DEBUG - Total processing for threshold 0.9 took 2.9350 seconds\n",
      "2024-12-10 17:40:28,946 - cluster_profiler - DEBUG - Processing threshold: 0.89\n",
      "2024-12-10 17:40:28,955 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.89 took 0.0083 seconds\n",
      "2024-12-10 17:40:28,956 - cluster_profiler - DEBUG - Filtered clusters size: 195329\n",
      "2024-12-10 17:40:29,347 - cluster_profiler - DEBUG - Parent grouping took 0.3906 seconds\n",
      "2024-12-10 17:40:29,348 - cluster_profiler - DEBUG - Number of parent groups: 22\n",
      "2024-12-10 17:40:31,146 - cluster_profiler - DEBUG - Component processing for threshold 0.89 took 1.7980 seconds\n",
      "2024-12-10 17:40:31,147 - cluster_profiler - DEBUG - Components processed: 5\n",
      "2024-12-10 17:40:31,147 - cluster_profiler - DEBUG - Total processing for threshold 0.89 took 2.2015 seconds\n",
      "2024-12-10 17:40:31,148 - cluster_profiler - DEBUG - Processing threshold: 0.88\n",
      "2024-12-10 17:40:31,157 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.88 took 0.0087 seconds\n",
      "2024-12-10 17:40:31,158 - cluster_profiler - DEBUG - Filtered clusters size: 196736\n",
      "2024-12-10 17:40:31,561 - cluster_profiler - DEBUG - Parent grouping took 0.4022 seconds\n",
      "2024-12-10 17:40:31,562 - cluster_profiler - DEBUG - Number of parent groups: 6\n",
      "2024-12-10 17:40:33,429 - cluster_profiler - DEBUG - Component processing for threshold 0.88 took 1.8662 seconds\n",
      "2024-12-10 17:40:33,429 - cluster_profiler - DEBUG - Components processed: 2\n",
      "2024-12-10 17:40:33,430 - cluster_profiler - DEBUG - Total processing for threshold 0.88 took 2.2818 seconds\n",
      "2024-12-10 17:40:33,431 - cluster_profiler - DEBUG - Processing threshold: 0.87\n",
      "2024-12-10 17:40:33,443 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.87 took 0.0119 seconds\n",
      "2024-12-10 17:40:33,444 - cluster_profiler - DEBUG - Filtered clusters size: 197678\n",
      "2024-12-10 17:40:33,842 - cluster_profiler - DEBUG - Parent grouping took 0.3974 seconds\n",
      "2024-12-10 17:40:33,843 - cluster_profiler - DEBUG - Number of parent groups: 6\n",
      "2024-12-10 17:40:35,779 - cluster_profiler - DEBUG - Component processing for threshold 0.87 took 1.9360 seconds\n",
      "2024-12-10 17:40:35,780 - cluster_profiler - DEBUG - Components processed: 3\n",
      "2024-12-10 17:40:35,781 - cluster_profiler - DEBUG - Total processing for threshold 0.87 took 2.3499 seconds\n",
      "2024-12-10 17:40:35,782 - cluster_profiler - DEBUG - Processing threshold: 0.86\n",
      "2024-12-10 17:40:35,791 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.86 took 0.0085 seconds\n",
      "2024-12-10 17:40:35,791 - cluster_profiler - DEBUG - Filtered clusters size: 198353\n",
      "2024-12-10 17:40:36,205 - cluster_profiler - DEBUG - Parent grouping took 0.4133 seconds\n",
      "2024-12-10 17:40:36,206 - cluster_profiler - DEBUG - Number of parent groups: 3\n",
      "2024-12-10 17:40:38,207 - cluster_profiler - DEBUG - Component processing for threshold 0.86 took 2.0006 seconds\n",
      "2024-12-10 17:40:38,208 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:40:38,208 - cluster_profiler - DEBUG - Total processing for threshold 0.86 took 2.4269 seconds\n",
      "2024-12-10 17:40:38,209 - cluster_profiler - DEBUG - Processing threshold: 0.85\n",
      "2024-12-10 17:40:38,218 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.85 took 0.0087 seconds\n",
      "2024-12-10 17:40:38,219 - cluster_profiler - DEBUG - Filtered clusters size: 198850\n",
      "2024-12-10 17:40:38,624 - cluster_profiler - DEBUG - Parent grouping took 0.4045 seconds\n",
      "2024-12-10 17:40:38,625 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:40:40,713 - cluster_profiler - DEBUG - Component processing for threshold 0.85 took 2.0883 seconds\n",
      "2024-12-10 17:40:40,714 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:40:40,715 - cluster_profiler - DEBUG - Total processing for threshold 0.85 took 2.5057 seconds\n",
      "2024-12-10 17:40:40,715 - cluster_profiler - DEBUG - Processing threshold: 0.84\n",
      "2024-12-10 17:40:40,724 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.84 took 0.0081 seconds\n",
      "2024-12-10 17:40:40,725 - cluster_profiler - DEBUG - Filtered clusters size: 199174\n",
      "2024-12-10 17:40:41,117 - cluster_profiler - DEBUG - Parent grouping took 0.3920 seconds\n",
      "2024-12-10 17:40:41,118 - cluster_profiler - DEBUG - Number of parent groups: 2\n",
      "2024-12-10 17:40:43,277 - cluster_profiler - DEBUG - Component processing for threshold 0.84 took 2.1577 seconds\n",
      "2024-12-10 17:40:43,277 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:40:43,278 - cluster_profiler - DEBUG - Total processing for threshold 0.84 took 2.5627 seconds\n",
      "2024-12-10 17:40:43,279 - cluster_profiler - DEBUG - Processing threshold: 0.83\n",
      "2024-12-10 17:40:43,288 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.83 took 0.0082 seconds\n",
      "2024-12-10 17:40:43,288 - cluster_profiler - DEBUG - Filtered clusters size: 199411\n",
      "2024-12-10 17:40:43,696 - cluster_profiler - DEBUG - Parent grouping took 0.4074 seconds\n",
      "2024-12-10 17:40:43,697 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:40:45,951 - cluster_profiler - DEBUG - Component processing for threshold 0.83 took 2.2534 seconds\n",
      "2024-12-10 17:40:45,952 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:40:45,952 - cluster_profiler - DEBUG - Total processing for threshold 0.83 took 2.6737 seconds\n",
      "2024-12-10 17:40:45,953 - cluster_profiler - DEBUG - Processing threshold: 0.82\n",
      "2024-12-10 17:40:45,962 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.82 took 0.0081 seconds\n",
      "2024-12-10 17:40:45,962 - cluster_profiler - DEBUG - Filtered clusters size: 199589\n",
      "2024-12-10 17:40:46,365 - cluster_profiler - DEBUG - Parent grouping took 0.4021 seconds\n",
      "2024-12-10 17:40:46,366 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:40:49,481 - cluster_profiler - DEBUG - Component processing for threshold 0.82 took 3.1153 seconds\n",
      "2024-12-10 17:40:49,482 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:40:49,483 - cluster_profiler - DEBUG - Total processing for threshold 0.82 took 3.5297 seconds\n",
      "2024-12-10 17:40:49,484 - cluster_profiler - DEBUG - Processing threshold: 0.81\n",
      "2024-12-10 17:40:49,493 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.81 took 0.0083 seconds\n",
      "2024-12-10 17:40:49,494 - cluster_profiler - DEBUG - Filtered clusters size: 199696\n",
      "2024-12-10 17:40:49,908 - cluster_profiler - DEBUG - Parent grouping took 0.4135 seconds\n",
      "2024-12-10 17:40:49,908 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:40:52,234 - cluster_profiler - DEBUG - Component processing for threshold 0.81 took 2.3250 seconds\n",
      "2024-12-10 17:40:52,235 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:40:52,235 - cluster_profiler - DEBUG - Total processing for threshold 0.81 took 2.7515 seconds\n",
      "2024-12-10 17:40:52,236 - cluster_profiler - DEBUG - Processing threshold: 0.8\n",
      "2024-12-10 17:40:52,245 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.8 took 0.0081 seconds\n",
      "2024-12-10 17:40:52,245 - cluster_profiler - DEBUG - Filtered clusters size: 199790\n",
      "2024-12-10 17:40:52,670 - cluster_profiler - DEBUG - Parent grouping took 0.4239 seconds\n",
      "2024-12-10 17:40:52,671 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:40:55,040 - cluster_profiler - DEBUG - Component processing for threshold 0.8 took 2.3690 seconds\n",
      "2024-12-10 17:40:55,041 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:40:55,042 - cluster_profiler - DEBUG - Total processing for threshold 0.8 took 2.8054 seconds\n",
      "2024-12-10 17:40:55,042 - cluster_profiler - DEBUG - Processing threshold: 0.79\n",
      "2024-12-10 17:40:55,051 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.79 took 0.0081 seconds\n",
      "2024-12-10 17:40:55,052 - cluster_profiler - DEBUG - Filtered clusters size: 199844\n",
      "2024-12-10 17:40:55,445 - cluster_profiler - DEBUG - Parent grouping took 0.3931 seconds\n",
      "2024-12-10 17:40:55,446 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:40:57,931 - cluster_profiler - DEBUG - Component processing for threshold 0.79 took 2.4840 seconds\n",
      "2024-12-10 17:40:57,932 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:40:57,932 - cluster_profiler - DEBUG - Total processing for threshold 0.79 took 2.8897 seconds\n",
      "2024-12-10 17:40:57,933 - cluster_profiler - DEBUG - Processing threshold: 0.78\n",
      "2024-12-10 17:40:57,942 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.78 took 0.0083 seconds\n",
      "2024-12-10 17:40:57,943 - cluster_profiler - DEBUG - Filtered clusters size: 199893\n",
      "2024-12-10 17:40:58,363 - cluster_profiler - DEBUG - Parent grouping took 0.4202 seconds\n",
      "2024-12-10 17:40:58,364 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:41:01,219 - cluster_profiler - DEBUG - Component processing for threshold 0.78 took 2.8547 seconds\n",
      "2024-12-10 17:41:01,220 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:41:01,221 - cluster_profiler - DEBUG - Total processing for threshold 0.78 took 3.2878 seconds\n",
      "2024-12-10 17:41:01,221 - cluster_profiler - DEBUG - Processing threshold: 0.77\n",
      "2024-12-10 17:41:01,230 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.77 took 0.0082 seconds\n",
      "2024-12-10 17:41:01,231 - cluster_profiler - DEBUG - Filtered clusters size: 199928\n",
      "2024-12-10 17:41:01,627 - cluster_profiler - DEBUG - Parent grouping took 0.3953 seconds\n",
      "2024-12-10 17:41:01,628 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:41:04,288 - cluster_profiler - DEBUG - Component processing for threshold 0.77 took 2.6596 seconds\n",
      "2024-12-10 17:41:04,288 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:41:04,289 - cluster_profiler - DEBUG - Total processing for threshold 0.77 took 3.0675 seconds\n",
      "2024-12-10 17:41:04,290 - cluster_profiler - DEBUG - Processing threshold: 0.76\n",
      "2024-12-10 17:41:04,298 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.76 took 0.0080 seconds\n",
      "2024-12-10 17:41:04,299 - cluster_profiler - DEBUG - Filtered clusters size: 199953\n",
      "2024-12-10 17:41:04,704 - cluster_profiler - DEBUG - Parent grouping took 0.4046 seconds\n",
      "2024-12-10 17:41:04,705 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:41:07,436 - cluster_profiler - DEBUG - Component processing for threshold 0.76 took 2.7303 seconds\n",
      "2024-12-10 17:41:07,436 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:41:07,437 - cluster_profiler - DEBUG - Total processing for threshold 0.76 took 3.1472 seconds\n",
      "2024-12-10 17:41:07,438 - cluster_profiler - DEBUG - Processing threshold: 0.75\n",
      "2024-12-10 17:41:07,447 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.75 took 0.0084 seconds\n",
      "2024-12-10 17:41:07,447 - cluster_profiler - DEBUG - Filtered clusters size: 199971\n",
      "2024-12-10 17:41:07,863 - cluster_profiler - DEBUG - Parent grouping took 0.4146 seconds\n",
      "2024-12-10 17:41:07,863 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:41:10,692 - cluster_profiler - DEBUG - Component processing for threshold 0.75 took 2.8281 seconds\n",
      "2024-12-10 17:41:10,693 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:41:10,693 - cluster_profiler - DEBUG - Total processing for threshold 0.75 took 3.2555 seconds\n",
      "2024-12-10 17:41:10,694 - cluster_profiler - DEBUG - Processing threshold: 0.74\n",
      "2024-12-10 17:41:10,703 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.74 took 0.0084 seconds\n",
      "2024-12-10 17:41:10,704 - cluster_profiler - DEBUG - Filtered clusters size: 199978\n",
      "2024-12-10 17:41:11,124 - cluster_profiler - DEBUG - Parent grouping took 0.4203 seconds\n",
      "2024-12-10 17:41:11,125 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:41:13,957 - cluster_profiler - DEBUG - Component processing for threshold 0.74 took 2.8317 seconds\n",
      "2024-12-10 17:41:13,958 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:41:13,959 - cluster_profiler - DEBUG - Total processing for threshold 0.74 took 3.2648 seconds\n",
      "2024-12-10 17:41:13,959 - cluster_profiler - DEBUG - Processing threshold: 0.73\n",
      "2024-12-10 17:41:13,968 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.73 took 0.0084 seconds\n",
      "2024-12-10 17:41:13,969 - cluster_profiler - DEBUG - Filtered clusters size: 199985\n",
      "2024-12-10 17:41:14,384 - cluster_profiler - DEBUG - Parent grouping took 0.4148 seconds\n",
      "2024-12-10 17:41:14,385 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:41:17,282 - cluster_profiler - DEBUG - Component processing for threshold 0.73 took 2.8965 seconds\n",
      "2024-12-10 17:41:17,283 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:41:17,283 - cluster_profiler - DEBUG - Total processing for threshold 0.73 took 3.3241 seconds\n",
      "2024-12-10 17:41:17,284 - cluster_profiler - DEBUG - Processing threshold: 0.72\n",
      "2024-12-10 17:41:17,293 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.72 took 0.0083 seconds\n",
      "2024-12-10 17:41:17,294 - cluster_profiler - DEBUG - Filtered clusters size: 199988\n",
      "2024-12-10 17:41:17,711 - cluster_profiler - DEBUG - Parent grouping took 0.4163 seconds\n",
      "2024-12-10 17:41:17,712 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:41:21,601 - cluster_profiler - DEBUG - Component processing for threshold 0.72 took 3.8890 seconds\n",
      "2024-12-10 17:41:21,602 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:41:21,603 - cluster_profiler - DEBUG - Total processing for threshold 0.72 took 4.3182 seconds\n",
      "2024-12-10 17:41:21,603 - cluster_profiler - DEBUG - Processing threshold: 0.71\n",
      "2024-12-10 17:41:21,612 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.71 took 0.0081 seconds\n",
      "2024-12-10 17:41:21,613 - cluster_profiler - DEBUG - Filtered clusters size: 199989\n",
      "2024-12-10 17:41:22,005 - cluster_profiler - DEBUG - Parent grouping took 0.3922 seconds\n",
      "2024-12-10 17:41:22,006 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:41:24,982 - cluster_profiler - DEBUG - Component processing for threshold 0.71 took 2.9756 seconds\n",
      "2024-12-10 17:41:24,983 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:41:24,983 - cluster_profiler - DEBUG - Total processing for threshold 0.71 took 3.3802 seconds\n",
      "2024-12-10 17:41:32,832 - cluster_profiler - DEBUG - Final sorting took 7.8477 seconds\n",
      "2024-12-10 17:41:32,833 - cluster_profiler - DEBUG - Final hierarchy size: 2835379\n",
      "2024-12-10 17:41:32,834 - cluster_profiler - DEBUG - Total function execution took 117.1033 seconds\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger('cluster_profiler')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "hierarchical3 = _cluster_results_to_hierarchical_3(probabilities, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 18:00:07,583 - cluster_profiler - INFO - Starting clustering process\n",
      "2024-12-10 18:00:07,584 - cluster_profiler - DEBUG - Input probabilities table size: 1000000\n",
      "2024-12-10 18:00:07,585 - cluster_profiler - DEBUG - Input clusters table size: 5279385\n",
      "2024-12-10 18:00:14,588 - cluster_profiler - DEBUG - Initialization took 7.0027s\n",
      "2024-12-10 18:00:14,610 - cluster_profiler - DEBUG - Processing threshold: 1.0\n",
      "2024-12-10 18:00:14,616 - cluster_profiler - DEBUG - Filtered clusters size: 30689\n",
      "2024-12-10 18:00:14,762 - cluster_profiler - DEBUG - Component processing took 0.0559s\n",
      "2024-12-10 18:00:14,763 - cluster_profiler - DEBUG - Total threshold processing took 0.1526s\n",
      "2024-12-10 18:00:14,763 - cluster_profiler - DEBUG - Processing threshold: 0.99\n",
      "2024-12-10 18:00:14,770 - cluster_profiler - DEBUG - Filtered clusters size: 63676\n",
      "2024-12-10 18:00:15,229 - cluster_profiler - DEBUG - Component processing took 0.3114s\n",
      "2024-12-10 18:00:15,230 - cluster_profiler - DEBUG - Total threshold processing took 0.4671s\n",
      "2024-12-10 18:00:15,231 - cluster_profiler - DEBUG - Processing threshold: 0.98\n",
      "2024-12-10 18:00:15,238 - cluster_profiler - DEBUG - Filtered clusters size: 83436\n",
      "2024-12-10 18:00:16,054 - cluster_profiler - DEBUG - Component processing took 0.6021s\n",
      "2024-12-10 18:00:16,055 - cluster_profiler - DEBUG - Total threshold processing took 0.8239s\n",
      "2024-12-10 18:00:16,055 - cluster_profiler - DEBUG - Processing threshold: 0.97\n",
      "2024-12-10 18:00:16,063 - cluster_profiler - DEBUG - Filtered clusters size: 108385\n",
      "2024-12-10 18:00:18,307 - cluster_profiler - DEBUG - Component processing took 1.9883s\n",
      "2024-12-10 18:00:18,308 - cluster_profiler - DEBUG - Total threshold processing took 2.2526s\n",
      "2024-12-10 18:00:18,308 - cluster_profiler - DEBUG - Processing threshold: 0.96\n",
      "2024-12-10 18:00:18,316 - cluster_profiler - DEBUG - Filtered clusters size: 134693\n",
      "2024-12-10 18:00:20,308 - cluster_profiler - DEBUG - Component processing took 1.6679s\n",
      "2024-12-10 18:00:20,309 - cluster_profiler - DEBUG - Total threshold processing took 2.0008s\n",
      "2024-12-10 18:00:20,310 - cluster_profiler - DEBUG - Processing threshold: 0.95\n",
      "2024-12-10 18:00:20,319 - cluster_profiler - DEBUG - Filtered clusters size: 155322\n",
      "2024-12-10 18:00:22,754 - cluster_profiler - DEBUG - Component processing took 2.1036s\n",
      "2024-12-10 18:00:22,755 - cluster_profiler - DEBUG - Total threshold processing took 2.4453s\n",
      "2024-12-10 18:00:22,756 - cluster_profiler - DEBUG - Processing threshold: 0.94\n",
      "2024-12-10 18:00:22,765 - cluster_profiler - DEBUG - Filtered clusters size: 169726\n",
      "2024-12-10 18:00:25,688 - cluster_profiler - DEBUG - Component processing took 2.5321s\n",
      "2024-12-10 18:00:25,689 - cluster_profiler - DEBUG - Total threshold processing took 2.9331s\n",
      "2024-12-10 18:00:25,689 - cluster_profiler - DEBUG - Processing threshold: 0.93\n",
      "2024-12-10 18:00:25,699 - cluster_profiler - DEBUG - Filtered clusters size: 179643\n",
      "2024-12-10 18:00:29,017 - cluster_profiler - DEBUG - Component processing took 2.9355s\n",
      "2024-12-10 18:00:29,018 - cluster_profiler - DEBUG - Total threshold processing took 3.3287s\n",
      "2024-12-10 18:00:29,019 - cluster_profiler - DEBUG - Processing threshold: 0.92\n",
      "2024-12-10 18:00:29,027 - cluster_profiler - DEBUG - Filtered clusters size: 186031\n",
      "2024-12-10 18:00:33,439 - cluster_profiler - DEBUG - Component processing took 4.0092s\n",
      "2024-12-10 18:00:33,440 - cluster_profiler - DEBUG - Total threshold processing took 4.4218s\n",
      "2024-12-10 18:00:33,441 - cluster_profiler - DEBUG - Processing threshold: 0.91\n",
      "2024-12-10 18:00:33,451 - cluster_profiler - DEBUG - Filtered clusters size: 190338\n",
      "2024-12-10 18:00:37,490 - cluster_profiler - DEBUG - Component processing took 3.6320s\n",
      "2024-12-10 18:00:37,491 - cluster_profiler - DEBUG - Total threshold processing took 4.0497s\n",
      "2024-12-10 18:00:37,491 - cluster_profiler - DEBUG - Processing threshold: 0.9\n",
      "2024-12-10 18:00:37,501 - cluster_profiler - DEBUG - Filtered clusters size: 193311\n",
      "2024-12-10 18:00:42,073 - cluster_profiler - DEBUG - Component processing took 4.1657s\n",
      "2024-12-10 18:00:42,074 - cluster_profiler - DEBUG - Total threshold processing took 4.5827s\n",
      "2024-12-10 18:00:42,075 - cluster_profiler - DEBUG - Processing threshold: 0.89\n",
      "2024-12-10 18:00:42,085 - cluster_profiler - DEBUG - Filtered clusters size: 195329\n",
      "2024-12-10 18:00:47,612 - cluster_profiler - DEBUG - Component processing took 5.0961s\n",
      "2024-12-10 18:00:47,613 - cluster_profiler - DEBUG - Total threshold processing took 5.5382s\n",
      "2024-12-10 18:00:47,613 - cluster_profiler - DEBUG - Processing threshold: 0.88\n",
      "2024-12-10 18:00:47,623 - cluster_profiler - DEBUG - Filtered clusters size: 196736\n",
      "2024-12-10 18:00:54,665 - cluster_profiler - DEBUG - Component processing took 6.6067s\n",
      "2024-12-10 18:00:54,666 - cluster_profiler - DEBUG - Total threshold processing took 7.0528s\n",
      "2024-12-10 18:00:54,667 - cluster_profiler - DEBUG - Processing threshold: 0.87\n",
      "2024-12-10 18:00:54,679 - cluster_profiler - DEBUG - Filtered clusters size: 197678\n",
      "2024-12-10 18:01:04,804 - cluster_profiler - DEBUG - Component processing took 9.7155s\n",
      "2024-12-10 18:01:04,805 - cluster_profiler - DEBUG - Total threshold processing took 10.1381s\n",
      "2024-12-10 18:01:04,805 - cluster_profiler - DEBUG - Processing threshold: 0.86\n",
      "2024-12-10 18:01:04,815 - cluster_profiler - DEBUG - Filtered clusters size: 198353\n",
      "2024-12-10 18:01:28,609 - cluster_profiler - DEBUG - Component processing took 23.3882s\n",
      "2024-12-10 18:01:28,609 - cluster_profiler - DEBUG - Total threshold processing took 23.8040s\n",
      "2024-12-10 18:01:28,610 - cluster_profiler - DEBUG - Processing threshold: 0.85\n",
      "2024-12-10 18:01:28,620 - cluster_profiler - DEBUG - Filtered clusters size: 198850\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster_profiler\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m logger\u001b[38;5;241m.\u001b[39msetLevel(logging\u001b[38;5;241m.\u001b[39mDEBUG)\n\u001b[0;32m----> 4\u001b[0m hierarchical4 \u001b[38;5;241m=\u001b[39m \u001b[43m_cluster_results_to_hierarchical_4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobabilities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclusters\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[106]\u001b[0m, in \u001b[0;36m_cluster_results_to_hierarchical_4\u001b[0;34m(probabilities, clusters)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger('cluster_profiler')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "hierarchical4 = _cluster_results_to_hierarchical_4(probabilities, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2835379"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hierarchical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2835379"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hierarchical2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2835379"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hierarchical3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hierarchical4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5279385"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "parent: binary\n",
       "child: binary\n",
       "threshold: double\n",
       "----\n",
       "parent: [[6810342717C8589102E8A1F469EDF344BBD353D9D1DFB6B547FAC9561849CEDC,6810342717C8589102E8A1F469EDF344BBD353D9D1DFB6B547FAC9561849CEDC,47EF3BF7E7AD94C6AFF6B4832E5A7AD8C4FA1C3FDB60AA5EEB0AC555AB627833,47EF3BF7E7AD94C6AFF6B4832E5A7AD8C4FA1C3FDB60AA5EEB0AC555AB627833,A79E30871550ABAB8638EBDFF3A9E351E8537EBEDFF1EF2CAD18B5CEF4F4E8B9,...,C255483BAAF15A5475EE5DD67A30566E53673969DC2DC6D6689E0487857AF49E,C255483BAAF15A5475EE5DD67A30566E53673969DC2DC6D6689E0487857AF49E,C255483BAAF15A5475EE5DD67A30566E53673969DC2DC6D6689E0487857AF49E,C255483BAAF15A5475EE5DD67A30566E53673969DC2DC6D6689E0487857AF49E,C255483BAAF15A5475EE5DD67A30566E53673969DC2DC6D6689E0487857AF49E]]\n",
       "child: [[694DCC2AE7D2038E9B3B547A23C398510A2FCB24,BA883EFEED4A93C4739969815E7D23CCC9455202,30C67178B851B0038FC5D5EA72B32032BABD7CBB,774F730EA226553663DF11E721072A6E28873E3F,B4481867BD5583E3C33D83DF4745FC059ECBBB98,...,FFFD0F3371B95C2DC93EBDE701309ECADA1DBE21,FFFDB881D8578BB3325C91AE6646A2E7C95441A7,FFFE18EE7325D9E2A0412CB2F44F33D2E198A40E,FFFE767F38648B640A9BB0A03E245ACB38B738A5,FFFF38A5B12F9C3E4B819C6172F8FAE6194CBC85]]\n",
       "threshold: [[1,1,1,1,1,...,0.71,0.71,0.71,0.71,0.71]]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy data\n",
    "\n",
    "* Probabilities standard/update: model, cluster, probability\n",
    "* Clusters (data) standard/update: hash, dataset, id\n",
    "* Clusters (cluster) standard/update: hash, dataset (null), id (null)\n",
    "* Contains standard/update\n",
    "\n",
    "Or\n",
    "\n",
    "* clusters_dataset1_standard_sml\n",
    "* clusters_dataset1_update_sml\n",
    "* clusters_dataset2_standard_sml\n",
    "* clusters_dataset2_update_sml\n",
    "* clusters_clusters_standard_sml\n",
    "* clusters_clusters_update_sml\n",
    "* contains_standard_sml\n",
    "* contains_update_sml\n",
    "* probabilities_standard_sml\n",
    "* probabilities_update_sml\n",
    "* clusters_dataset1_standard_lrg\n",
    "* clusters_dataset1_update_lrg\n",
    "* clusters_dataset2_standard_lrg\n",
    "* clusters_dataset2_update_lrg\n",
    "* clusters_clusters_standard_lrg\n",
    "* clusters_clusters_update_lrg\n",
    "* contains_standard_lrg\n",
    "* contains_update_lrg\n",
    "* probabilities_standard_lrg\n",
    "* probabilities_update_lrg\n",
    "\n",
    "By\n",
    "\n",
    "* Generate 20m hashes for dataset1\n",
    "* Generate 20m pks for dataset1\n",
    "* Generate 5m hashes for dataset2\n",
    "* Generate 5m pks for dataset2\n",
    "* Create dataset1 standard w/ 10m hashes and 180k pks\n",
    "* Create dataset1 update w/ all hashes and 200k pks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "from uuid import UUID, uuid4\n",
    "from typing import Generator\n",
    "\n",
    "def d1gen() -> Generator[UUID, None, None]:\n",
    "    \"\"\"Generate UUIDs for dataset 1.\"\"\"\n",
    "\n",
    "    for i in count():\n",
    "        yield UUID(int=i, version=4)\n",
    "\n",
    "def d2gen() -> Generator[UUID, None, None]:\n",
    "    \"\"\"Generate UUIDs for dataset 2.\"\"\"\n",
    "    for _ in count():\n",
    "        yield uuid4()\n",
    "\n",
    "d1ids = d1gen()\n",
    "d2ids = d2gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 5\n",
    "\n",
    "def num(c: int, e: int) -> int:\n",
    "    return int(c * (10 ** e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_tables(\n",
    "    size: int,\n",
    "    name: bytes,\n",
    "    id_gen: Generator[str, None, None]\n",
    ") -> tuple[pa.Table, pa.Table]:\n",
    "    \"\"\"\n",
    "    Create two Arrow tables: a standard version and an update version with controlled changes.\n",
    "    The update version maintains some hashes, reassigns some IDs, and adds new entries.\n",
    "    \n",
    "    Args:\n",
    "        size: Number of rows in each table\n",
    "        name: Dataset name as bytes (will be used as prefix for both versions)\n",
    "        id_gen: Generator that yields string IDs\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[pa.Table, pa.Table]: Standard and update tables\n",
    "    \"\"\"\n",
    "    # Generate standard table data\n",
    "    standard_hashes = fast_generate_hashes(size)\n",
    "    standard_ids = [[str(next(id_gen))] for _ in range(size)]\n",
    "    \n",
    "    # Create standard table\n",
    "    standard_ids_nested = pa.array(standard_ids, type=pa.list_(pa.string()))\n",
    "    standard_dataset = pa.array([name] * len(standard_hashes), type=pa.binary())\n",
    "    \n",
    "    standard_table = pa.Table.from_arrays(\n",
    "        [standard_hashes, standard_dataset, standard_ids_nested],\n",
    "        names=['hash', 'dataset', 'id']\n",
    "    )\n",
    "\n",
    "    # Calculate sizes for different types of changes\n",
    "    keep_count = int(size * 0.3)  # 30% remain same\n",
    "    reuse_count = int(size * 0.2)  # 20% same hash, new ID\n",
    "    move_count = int(size * 0.2)   # 20% same ID, new hash\n",
    "    new_count = size - (keep_count + reuse_count + move_count)  # 30% new entries\n",
    "\n",
    "    # Build update version\n",
    "    update_hashes = []\n",
    "    update_ids = []\n",
    "\n",
    "    # 1. Keep some entries exactly the same\n",
    "    for i in range(keep_count):\n",
    "        update_hashes.append(standard_hashes[i])\n",
    "        update_ids.append(standard_ids[i])\n",
    "    \n",
    "    # 2. Same hash, new IDs\n",
    "    for i in range(keep_count, keep_count + reuse_count):\n",
    "        update_hashes.append(standard_hashes[i])\n",
    "        update_ids.append(\n",
    "            [str(next(id_gen))]\n",
    "        )\n",
    "    \n",
    "    # 3. Same IDs, new hashes\n",
    "    new_hashes = fast_generate_hashes(move_count)\n",
    "    for i, hash in enumerate(new_hashes):\n",
    "        idx = keep_count + reuse_count + i\n",
    "        update_hashes.append(hash)\n",
    "        update_ids.append(standard_ids[idx])\n",
    "    \n",
    "    # 4. Completely new entries\n",
    "    final_new_hashes = fast_generate_hashes(new_count)\n",
    "    for hash in final_new_hashes:\n",
    "        update_hashes.append(hash)\n",
    "        update_ids.append(\n",
    "            [str(next(id_gen))]\n",
    "        )\n",
    "\n",
    "    # Create update table\n",
    "    update_ids_nested = pa.array(update_ids, type=pa.list_(pa.string()))\n",
    "    update_dataset = pa.array([name] * len(update_hashes), type=pa.binary())\n",
    "\n",
    "    update_table = pa.Table.from_arrays(\n",
    "        [pa.array(update_hashes), update_dataset, update_ids_nested],\n",
    "        names=['hash', 'dataset', 'id']\n",
    "    )\n",
    "\n",
    "    return standard_table, update_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyarrow.Table\n",
       " hash: binary\n",
       " dataset: binary\n",
       " id: list<item: string>\n",
       "   child 0, item: string\n",
       " ----\n",
       " hash: [[BB8BD9281B7698DA11C6AC58A7528954CCB32AF5,14CA930597D476D7D947BC29FF314ADC8D4B19A4,EFF6E0174705BD6169ABE3325A0D813592F6F04C,5F174867850BCA27C1664AD48A29C25B3D9B5C46,0866C5A7F70E10265E8E701432C389889EDE3A42,...,D0FDB00682A7F63981858B8BFB5674743F2CD1C5,0BA9AA21B31E28D1FEAA557F2B991375565E4066,088D04FCB221AE3BE2070AA143AA856526F44BCD,93ED08CD471857A3DBAA40364B60BA92E1441DE3,62A17D122B9DB5304999934C8180F22B480E30A6]]\n",
       " dataset: [[6461746173657431,6461746173657431,6461746173657431,6461746173657431,6461746173657431,...,6461746173657431,6461746173657431,6461746173657431,6461746173657431,6461746173657431]]\n",
       " id: [[[\"00000000-0000-4000-8000-000000000000\"],[\"00000000-0000-4000-8000-000000000001\"],...,[\"00000000-0000-4000-8000-000000030d3e\"],[\"00000000-0000-4000-8000-000000030d3f\"]]],\n",
       " pyarrow.Table\n",
       " hash: binary\n",
       " dataset: binary\n",
       " id: list<item: string>\n",
       "   child 0, item: string\n",
       " ----\n",
       " hash: [[BB8BD9281B7698DA11C6AC58A7528954CCB32AF5,14CA930597D476D7D947BC29FF314ADC8D4B19A4,EFF6E0174705BD6169ABE3325A0D813592F6F04C,5F174867850BCA27C1664AD48A29C25B3D9B5C46,0866C5A7F70E10265E8E701432C389889EDE3A42,...,5ED67B2F7A10CE360C015F3E15CD24309F242021,B639277DC8FFB24D8B8F815672612F9FC55854B3,C3959B1236AB62A9723142369D044D62300C1245,97C9F3E059C3A1026AD42D604FFFDF27175F6A58,76B7C3986B83A32E2D055ED646E59546BD0A7D97]]\n",
       " dataset: [[6461746173657431,6461746173657431,6461746173657431,6461746173657431,6461746173657431,...,6461746173657431,6461746173657431,6461746173657431,6461746173657431,6461746173657431]]\n",
       " id: [[[\"00000000-0000-4000-8000-000000000000\"],[\"00000000-0000-4000-8000-000000000001\"],...,[\"00000000-0000-4000-8000-0000000493de\"],[\"00000000-0000-4000-8000-0000000493df\"]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1_standard, dataset1_update = create_test_tables(num(2, e), b'dataset1', d1ids)\n",
    "\n",
    "dataset1_standard, dataset1_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyarrow.Table\n",
       " hash: binary\n",
       " dataset: binary\n",
       " id: list<item: string>\n",
       "   child 0, item: string\n",
       " ----\n",
       " hash: [[121E7D7DFC1F8102E589CDC2D628BF8BA14AE09B,DF6EFB89680F7F2DCA4A8B52A09DE7D073AB93E3,9450D7CB108639ED251E605DFA36F5E799AC7255,41B811585371BD576D93A6679D7B4651134BC94F,3D67BDBA1A20E6BA99712999ACF67705E39DDF9B,...,CBE891D5F3DB086232C306E3A775886119A196E3,CC9CBB781A04633F561B146D93DF9136B4AF793B,04DDAD1680AEAFBD86A54C2C702E57F54B55D994,553A343F82991092B48492AE901CD7698A358A82,386A186890E97B44B1E15CB825558D4B39B5039C]]\n",
       " dataset: [[6461746173657432,6461746173657432,6461746173657432,6461746173657432,6461746173657432,...,6461746173657432,6461746173657432,6461746173657432,6461746173657432,6461746173657432]]\n",
       " id: [[[\"ab2e9b34-3656-487d-874b-50be5d5386cc\"],[\"b18f0852-a3c6-484d-9c6b-bae7d989a17f\"],...,[\"d306f395-9255-413d-9fad-5ed0066a60e3\"],[\"41c039bc-5642-4295-b722-3b28a7aa5535\"]]],\n",
       " pyarrow.Table\n",
       " hash: binary\n",
       " dataset: binary\n",
       " id: list<item: string>\n",
       "   child 0, item: string\n",
       " ----\n",
       " hash: [[121E7D7DFC1F8102E589CDC2D628BF8BA14AE09B,DF6EFB89680F7F2DCA4A8B52A09DE7D073AB93E3,9450D7CB108639ED251E605DFA36F5E799AC7255,41B811585371BD576D93A6679D7B4651134BC94F,3D67BDBA1A20E6BA99712999ACF67705E39DDF9B,...,68776A879D039B76899917680B9F566F86477C79,CFD3BBAFC8C49078D73ACC4A92B15918DC4C82A3,C5BBEB879A2836CC3C13E265A84082FDE1AE57D7,BB68CF3D1437E005B0812C423C71A79C2B3CF003,3DFD207DA6FF8C66F70571FAB3120AC4209B171D]]\n",
       " dataset: [[6461746173657432,6461746173657432,6461746173657432,6461746173657432,6461746173657432,...,6461746173657432,6461746173657432,6461746173657432,6461746173657432,6461746173657432]]\n",
       " id: [[[\"ab2e9b34-3656-487d-874b-50be5d5386cc\"],[\"b18f0852-a3c6-484d-9c6b-bae7d989a17f\"],...,[\"35512465-2607-446e-80a1-036e048361e7\"],[\"c816ccbc-eda3-48e7-bd04-d4357b022cc0\"]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2_standard, dataset2_update = create_test_tables(num(2, e), b'dataset2', d2ids)\n",
    "\n",
    "dataset2_standard, dataset2_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup time: 0.000s\n",
      "Standard index generation: 0.003s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "def fast_consistent_pairs(\n",
    "    standard_left: pa.Array,\n",
    "    standard_right: pa.Array,\n",
    "    update_left: pa.Array, \n",
    "    update_right: pa.Array,\n",
    "    n: int = int(1e8)\n",
    ") -> tuple[pa.Table, pa.Table]:\n",
    "    \"\"\"Generate partially consistent hash pairs between standard and update tables.\"\"\"\n",
    "    t0 = time()\n",
    "    \n",
    "    # Calculate sizes and adjust n\n",
    "    standard_total = len(standard_left) * len(standard_right)\n",
    "    update_total = len(update_left) * len(update_right)\n",
    "    n = min(n, min(standard_total, update_total))\n",
    "    print(f\"Setup time: {time() - t0:.3f}s\")\n",
    "    \n",
    "    t1 = time()\n",
    "    # Generate random permutation of just the first n indices instead of all possible indices\n",
    "    standard_flat = np.random.permutation(n)\n",
    "    # Scale to full range\n",
    "    standard_flat = (standard_flat * (standard_total / n)).astype(np.int64)\n",
    "    std_left_idx = standard_flat // len(standard_right)\n",
    "    std_right_idx = standard_flat % len(standard_right)\n",
    "    print(f\"Standard index generation: {time() - t1:.3f}s\")\n",
    "    \n",
    "    t2 = time()\n",
    "    # For update table, make ~30% of pairs match standard pairs\n",
    "    match_count = n // 3\n",
    "    update_flat = np.empty(n, dtype=np.int64)\n",
    "    update_flat[:match_count] = standard_flat[:match_count]  # Copy matching pairs\n",
    "    \n",
    "    # Same optimization for the remaining indices\n",
    "    remaining = np.random.permutation(n - match_count)\n",
    "    remaining = (remaining * (update_total / (n - match_count))).astype(np.int64)\n",
    "    update_flat[match_count:] = remaining\n",
    "    \n",
    "    up_left_idx = update_flat // len(update_right)\n",
    "    up_right_idx = update_flat % len(update_right)\n",
    "    print(f\"Update index generation: {time() - t2:.3f}s\")\n",
    "    \n",
    "    t3 = time()\n",
    "    # Generate hashes using original SHA1 approach\n",
    "    standard_hashes = [\n",
    "        sha1(i.to_bytes(8, 'big')).digest() \n",
    "        for i in range(n)\n",
    "    ]\n",
    "    standard_combined = pa.array(standard_hashes, type=pa.binary())\n",
    "    print(f\"Standard hash generation: {time() - t3:.3f}s\")\n",
    "    \n",
    "    t4 = time()\n",
    "    # For update table, reuse hashes for matching pairs\n",
    "    update_hashes = standard_hashes[:match_count] + [\n",
    "        sha1((i + n).to_bytes(8, 'big')).digest()\n",
    "        for i in range(n - match_count)\n",
    "    ]\n",
    "    update_combined = pa.array(update_hashes, type=pa.binary())\n",
    "    print(f\"Update hash generation: {time() - t4:.3f}s\")\n",
    "    \n",
    "    t5 = time()\n",
    "    # Create tables\n",
    "    standard_table = pa.table({\n",
    "        'hash': standard_combined,\n",
    "        'left': standard_left.take(pa.array(std_left_idx)),\n",
    "        'right': standard_right.take(pa.array(std_right_idx)),\n",
    "        'probability': pa.array(np.round(0.7 + 0.3 * np.random.random(n), 2), type=pa.float64())\n",
    "    })\n",
    "    print(f\"Standard table creation: {time() - t5:.3f}s\")\n",
    "\n",
    "    t6 = time()\n",
    "    update_table = pa.table({\n",
    "        'hash': update_combined,\n",
    "        'left': update_left.take(pa.array(up_left_idx)),\n",
    "        'right': update_right.take(pa.array(up_right_idx)),\n",
    "        'probability': pa.array(np.round(0.7 + 0.3 * np.random.random(n), 2), type=pa.float64())\n",
    "    })\n",
    "    print(f\"Update table creation: {time() - t6:.3f}s\")\n",
    "    \n",
    "    print(f\"Total time: {time() - t0:.3f}s\")\n",
    "    return standard_table, update_table\n",
    "\n",
    "probabilities_standard, probabilities_update = fast_consistent_pairs(\n",
    "    dataset1_standard.column('hash'),\n",
    "    dataset2_standard.column('hash'),\n",
    "    dataset1_update.column('hash'),\n",
    "    dataset2_update.column('hash'),\n",
    "    n=1000\n",
    ")\n",
    "\n",
    "probabilities_standard, probabilities_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "hash: binary\n",
       "dataset: binary\n",
       "id: list<item: string>\n",
       "  child 0, item: string\n",
       "----\n",
       "hash: [[EE3F87AB6C85B96F9ECAA4102770DCD9DBC691EE,21A1F8D571F9FFC475C541610426418EDF5CCC45,1DC323B8253379511A1E5C26FE717A532FD2554F,64475A83F4E9DC9A7D6580B086414123D33C7699,6E37DB26F6309EA19AE5AA3B7582EF4525E3B1E1,...,8943E7E226DBA6E74B4D334D4DA3C6EADEAD2205,AC90B261C80108D9494A18D72FDCCD93C3DDA748,0EF069ADC5AA576D3E21B334C11B4D322E9FAF11,D0398ED473DA018B088A5CE6757156A9EDB4E504,903B543FA6EE6314E45A45641CEE52FCDDFEE2F6]]\n",
       "dataset: [[6461746173657432,6461746173657432,6461746173657432,6461746173657432,6461746173657432,...,6461746173657432,6461746173657432,6461746173657432,6461746173657432,6461746173657432]]\n",
       "id: [[[\"39bba755-998a-4ceb-9422-11b2ccd26804\"],[\"4ff8dc4b-9406-4d85-9abb-6dcf3378f29e\"],...,[\"43ca0a07-da74-44df-bab3-4d685c2e17be\"],[\"c44079c3-5d53-4f2f-8b32-342c49e8e2dd\"]]]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2_hashes = fast_generate_hashes(num(5, e - 1))\n",
    "dataset2_bytes = b\"dataset2\"\n",
    "dataset2_ids = [[str(next(d2ids))] for _ in range(num(5, e - 1))]\n",
    "\n",
    "dataset2_ids_nested = pa.array(dataset2_ids, type=pa.list_(pa.string()))\n",
    "dataset2_dataset = pa.array([dataset2_bytes] * len(dataset2_hashes), type=pa.binary())\n",
    "\n",
    "dataset_2 = pa.Table.from_arrays(\n",
    "    [dataset2_hashes, dataset2_dataset, dataset2_ids_nested],\n",
    "    names=['hash', 'dataset', 'id']\n",
    ")\n",
    "\n",
    "dataset_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashes = fast_generate_hashes(int(2e5))\n",
    "probabilities = fast_sample_pairs_lr(hashes, int(1e6))\n",
    "clusters = to_clusters(probabilities)\n",
    "hierarchical = _cluster_results_to_hierarchical(probabilities, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(hash: binary\n",
       " left: binary\n",
       " right: binary\n",
       " probability: double,\n",
       " '7653.24MB')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_table.schema, f\"{arrow_table.nbytes / (1024 * 1024):.2f}MB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "\n",
    "class HashIndex:\n",
    "    def __init__(self, hash_type: str = 'sha1', hashes: list[bytes] | pa.Array | None = None):\n",
    "        \"\"\"Create a new HashIndex instance.\n",
    "        \n",
    "        Args:\n",
    "            hash_type: Hash algorithm to use (default: 'sha1')\n",
    "            hashes (Optional): Initial list of hashes to insert (default: None)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            hash_obj = hashlib.new(hash_type)\n",
    "            self._hash_size: int = hash_obj.digest_size   # Return the digest size in bytes\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f'Unsupported hash type: {hash_type}') from e\n",
    "        \n",
    "        # Initialize empty table with correct schema\n",
    "        self.schema = pa.schema([\n",
    "            ('hash', pa.binary(self._hash_size)),\n",
    "            ('id', pa.int32())\n",
    "        ])\n",
    "        self.table = pa.Table.from_pydict({\n",
    "            'hash': [],\n",
    "            'id': []\n",
    "        }, schema=self.schema)\n",
    "        self.next_id: int = 0\n",
    "\n",
    "        if hashes is not None:\n",
    "            self.insert_hashes(hashes)\n",
    "\n",
    "    def __eq__(self, other: 'HashIndex') -> bool:\n",
    "        \"\"\"\n",
    "        Compare this HashIndex with another for equality.\n",
    "        \n",
    "        Two HashIndex instances are considered equal if they:\n",
    "            1. Have the same hash size\n",
    "            2. Have the same next_id\n",
    "            3. Have equal tables (same schema and data)\n",
    "        \n",
    "        Args:\n",
    "            other: Another HashIndex instance to compare with\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if the indexes are equal, False otherwise\n",
    "        \"\"\"\n",
    "        if not isinstance(other, HashIndex):\n",
    "            return False\n",
    "            \n",
    "        return (\n",
    "            self._hash_size == other._hash_size and\n",
    "            self.next_id == other.next_id and\n",
    "            self.table.equals(other.table)\n",
    "        )\n",
    "\n",
    "    def _list_to_array(self, hashes: list[bytes] | pa.Array) -> pa.Array:\n",
    "        if isinstance(hashes, list):\n",
    "            return pa.array(hashes, type=pa.binary(self._hash_size))\n",
    "        return hashes\n",
    "\n",
    "    def insert_hashes(self, hashes: list[bytes] | pa.Array) -> pa.Array:\n",
    "        \"\"\"\n",
    "        Insert new hashes and return their indices. For existing hashes, returns their\n",
    "        current indices. For new hashes, assigns and returns new indices.\n",
    "        \n",
    "        Args:\n",
    "            hashes: Array of SHA-1 hashes to insert\n",
    "        Returns:\n",
    "            Array of indices (both existing and newly assigned)\n",
    "        \"\"\"\n",
    "        hashes = self._list_to_array(hashes)\n",
    "        \n",
    "        # If table is empty, fast path to create initial table\n",
    "        if len(self.table) == 0:\n",
    "            ids = pa.array(range(len(hashes)), type=pa.int32())\n",
    "            self.table = pa.Table.from_arrays([hashes, ids], schema=self.schema)\n",
    "            self.next_id = len(hashes)\n",
    "            # Sort table by hash for future binary searches\n",
    "            self.table = self.table.sort_by('hash')\n",
    "            return ids\n",
    "\n",
    "        # Find existing hashes using binary search\n",
    "        indices = pc.index_in(hashes, self.table['hash'])\n",
    "        is_new = pc.is_null(indices)\n",
    "        new_count = pc.sum(pc.cast(is_new, pa.int32())).as_py()\n",
    "        \n",
    "        if new_count > 0:\n",
    "            # Get the new hashes\n",
    "            new_hashes = pc.filter(hashes, is_new)\n",
    "            \n",
    "            # Pre-allocate new IDs array\n",
    "            new_ids = pa.array(\n",
    "                range(self.next_id, self.next_id + new_count), \n",
    "                type=pa.int32()\n",
    "            )\n",
    "            self.next_id += new_count\n",
    "            \n",
    "            # Append in one operation and sort once\n",
    "            new_table = pa.Table.from_arrays([new_hashes, new_ids], schema=self.schema)\n",
    "            self.table = pa.concat_tables([self.table, new_table])\n",
    "            self.table = self.table.sort_by('hash')\n",
    "            \n",
    "            # Final lookup to get all IDs in correct order\n",
    "            indices = pc.index_in(hashes, self.table['hash'])\n",
    "        \n",
    "        return pc.take(self.table['id'], indices)\n",
    "\n",
    "    def get_hashes(self, ids: list[int] | pa.Array) -> pa.Array:\n",
    "        \"\"\"\n",
    "        Look up hashes by their IDs\n",
    "        \n",
    "        Args:\n",
    "            ids: Array of IDs to look up\n",
    "        Returns:\n",
    "            Array of corresponding hashes (null for unknown indices)\n",
    "        \"\"\"\n",
    "        if isinstance(ids, list):\n",
    "            ids = pa.array(ids, type=pa.int32())\n",
    "        \n",
    "        positions = pc.index_in(ids, self.table['id'])\n",
    "        return pc.take(self.table['hash'], positions)\n",
    "\n",
    "    def get_indices(self, hashes: list[bytes] | pa.Array) -> pa.Array:\n",
    "        \"\"\"\n",
    "        Look up IDs for existing hashes. Returns null for unknown hashes.\n",
    "        \n",
    "        Args:\n",
    "            hashes: Array of hashes to look up\n",
    "        Returns:\n",
    "            Array of corresponding IDs (null for unknown hashes)\n",
    "        \"\"\"\n",
    "        hashes = self._list_to_array(hashes)\n",
    "            \n",
    "        indices = pc.index_in(hashes, self.table['hash'])\n",
    "        return pc.take(self.table['id'], indices)\n",
    "    \n",
    "\n",
    "    def to_parquet(self, path: str | Path, compression: str = 'zstd') -> None:\n",
    "        \"\"\"\n",
    "        Save the HashIndex to a Parquet file.\n",
    "        \n",
    "        Args:\n",
    "            path: Path to save the Parquet file\n",
    "            compression: Compression algorithm to use (default: 'zstd')\n",
    "                Options include: 'none', 'snappy', 'gzip', 'brotli', 'lz4', 'zstd'\n",
    "        \n",
    "        Raises:\n",
    "            IOError: If the file cannot be written\n",
    "            ValueError: If the compression algorithm is not supported\n",
    "        \"\"\"\n",
    "        path = Path(path)\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        metadata = {\n",
    "            b'next_id': str(self.next_id).encode(),\n",
    "            b'hash_size': str(self._hash_size).encode()\n",
    "        }\n",
    "\n",
    "        existing_metadata = self.table.schema.metadata or {}\n",
    "        merged_metadata = {**existing_metadata, **metadata}\n",
    "        \n",
    "        try:\n",
    "            pq.write_table(\n",
    "                self.table.replace_schema_metadata(merged_metadata),\n",
    "                path,\n",
    "                compression=compression,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise IOError(f\"Failed to write Parquet file: {e}\") from e\n",
    "\n",
    "    @classmethod\n",
    "    def from_parquet(cls, path: str | Path) -> 'HashIndex':\n",
    "        \"\"\"\n",
    "        Load a HashIndex from a Parquet file.\n",
    "        \n",
    "        Args:\n",
    "            path: Path to the Parquet file\n",
    "        \n",
    "        Returns:\n",
    "            HashIndex: New HashIndex instance loaded from the file\n",
    "        \n",
    "        Raises:\n",
    "            IOError: If the file cannot be read or is invalid\n",
    "            ValueError: If the file format is invalid\n",
    "        \"\"\"\n",
    "        path = Path(path)\n",
    "        \n",
    "        try:\n",
    "            table = pq.read_table(path)\n",
    "            metadata = table.schema.metadata\n",
    "            \n",
    "            if not metadata or b'next_id' not in metadata or b'hash_size' not in metadata:\n",
    "                raise ValueError(\"Invalid Parquet file: missing required metadata\")\n",
    "            \n",
    "            # Create new instance\n",
    "            instance = cls.__new__(cls)\n",
    "            \n",
    "            instance._hash_size = int(metadata[b'hash_size'].decode())\n",
    "            instance.next_id = int(metadata[b'next_id'].decode())\n",
    "            \n",
    "            instance.schema = table.schema\n",
    "            instance.table = table\n",
    "            \n",
    "            return instance\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise IOError(f\"Failed to load Parquet file: {e}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<pyarrow.lib.BinaryArray object at 0x118363fa0>\n",
       " [\n",
       "   7BDB9431F1E8DDBF1ACCF691D4B8661CF9B43A25,\n",
       "   392BCA5B7AFC18C6727FC30EC57B718DDAAFF953,\n",
       "   F884626B7A54AEFFECB3CA3DDB1493613CC0C7A9,\n",
       "   C94022C8F551879B31D10F99BCC9A3561BAE7612,\n",
       "   08458C06AD784FCDC7809825056654AD5F119C4C\n",
       " ],\n",
       " <pyarrow.lib.BinaryArray object at 0x11f06ada0>\n",
       " [\n",
       "   31E7C33F498E4B7E95B1C52ECACAF348CEFFAE71,\n",
       "   CC141AB85B40C152864B1A24689072D718332BDD,\n",
       "   C769376877CFA78C46210E9B91D4446E82A6D7CE,\n",
       "   9263614A6B6C53967ABA62FA2859F785C58B1016,\n",
       "   15B8ABB98A8ED7D0F008C1234583DCE9DC34AFFF\n",
       " ])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hashes = fast_generate_hashes(int(2e7))\n",
    "test_hashes2 = pa.concat_arrays([fast_generate_hashes(int(2e7) / 2), test_hashes[:int(2e7 / 2)]])\n",
    "hidx_5 = HashIndex(hash_type='sha1', hashes=test_hashes)\n",
    "test_hashes[:5], test_hashes2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pa.array(random.sample(range(len(test_hashes2)), k=len(test_hashes2)))\n",
    "test_hashes3 = test_hashes2.take(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.ChunkedArray object at 0x13dc3c8b0>\n",
       "[\n",
       "  [\n",
       "    8997172,\n",
       "    6063643,\n",
       "    6260234,\n",
       "    3255430,\n",
       "    7485144,\n",
       "    ...\n",
       "    9917674,\n",
       "    5612100,\n",
       "    2723141,\n",
       "    201414,\n",
       "    4680984\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidx_5.insert_hashes(test_hashes3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<pyarrow.lib.BinaryArray object at 0x1187140a0>\n",
       " [\n",
       "   1F21ABB3185DA8D5340A8298C6CBC9C21F8AF9CD,\n",
       "   813583A225BA22DE65149EC502FA79A082F93D8C,\n",
       "   C4587910358DC70CD81B6B106FD724685D8AE971,\n",
       "   79924525EAF98B48E03E00213E0FFBFD17AD8778,\n",
       "   5E668D4D878659EE35DC8F3E7EFE5B19D68890FA\n",
       " ],\n",
       " <pyarrow.lib.BinaryArray object at 0x11ec591e0>\n",
       " [\n",
       "   BDBD93052D2290495857C2A46936C468CA4A7FD6,\n",
       "   AC0FFE1E2F41A6D12D5F56E32A00F8B48D47EF66,\n",
       "   A3DD64FC8F69428EA0693218AF31FEA22F8BCDDE,\n",
       "   0C7FF3A1FB8441369A80059DC2FA08F678A05D86,\n",
       "   BD939DD6F750A40D0C3CFCB3F1C3B202A8BEADE2\n",
       " ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hashes = fast_generate_hashes(40)\n",
    "test_hashes2 = fast_generate_hashes(40)\n",
    "hidx = HashIndex(hash_type='sha1')\n",
    "test_hashes[:5], test_hashes2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path.cwd()\n",
    "\n",
    "hidx3 = HashIndex(hash_type='sha1', hashes=fast_generate_hashes(int(2e7)))\n",
    "\n",
    "hidx3.to_parquet(file_path / 'hash_index.parquet')\n",
    "\n",
    "# del hidx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidx3 = HashIndex.from_parquet(file_path / 'hash_index.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.ChunkedArray object at 0x11ec7e6b0>\n",
       "[\n",
       "  [\n",
       "    3F0B311BB3EF9EAEB211378D538A67491E43429F,\n",
       "    1234FE9CB4C1413559FD09FBBAF512DD26DF7CF6,\n",
       "    C163CF811B2021CE8D17AFE5720F9DED2D602543,\n",
       "    D151C016C7C94022B0E8A7826EE698A0B5FB947E,\n",
       "    30EE3D4D4A6497F0C1A85158F895398C9E901F63\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidx3.get_hashes([random.randint(0, int(2e7)) for _ in range(int(2e5))])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<pyarrow.lib.Int32Array object at 0x11ecc1f60>\n",
       " [\n",
       "   0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4\n",
       " ],\n",
       " <pyarrow.lib.ChunkedArray object at 0x11ef1f600>\n",
       " [\n",
       "   [\n",
       "     40,\n",
       "     41,\n",
       "     42,\n",
       "     43,\n",
       "     44\n",
       "   ]\n",
       " ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidx.insert_hashes(test_hashes)[:5], hidx.insert_hashes(test_hashes2)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = hidx.get_hashes([0])[0]\n",
    "i = hidx.get_indices([h])[0]\n",
    "h2 = hidx.get_hashes([i])[0]\n",
    "\n",
    "h == h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = Path.cwd()\n",
    "\n",
    "hidx.to_parquet(file_path / 'hash_index.parquet')\n",
    "hidx2 = HashIndex.from_parquet(file_path / 'hash_index.parquet')\n",
    "\n",
    "hidx == hidx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.ChunkedArray object at 0x15a223d80>\n",
       "[\n",
       "  [\n",
       "    9E24FC1BC65D8A134990D02A6C4A40E3AAB20FAA,\n",
       "    09C61C3BE151A112228E61AC7C7C1B81DC625CDF,\n",
       "    CEED556D7FE741F9BC0FA5A464DEF839989F3E74,\n",
       "    76506960448B8BB6CB2AF337FBE9852E1ABB1486,\n",
       "    null\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidx.get_hashes([0, 40, 7, 42, 190])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<pyarrow.lib.ChunkedArray object at 0x120b61e90>\n",
       " [\n",
       "   [\n",
       "     0,\n",
       "     1,\n",
       "     2,\n",
       "     3,\n",
       "     4\n",
       "   ]\n",
       " ],\n",
       " <pyarrow.lib.ChunkedArray object at 0x15a2d1800>\n",
       " [\n",
       "   [\n",
       "     40,\n",
       "     41,\n",
       "     42,\n",
       "     43,\n",
       "     44\n",
       "   ]\n",
       " ])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidx.get_indices(test_hashes[:5]), hidx.get_indices(test_hashes2[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashes = fast_generate_hashes(int(2e7))\n",
    "t1 = fast_sample_pairs(hashes, int(1e8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('7653.24MB', '4135.36MB', '1907.35MB', '6042.71MB')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def index_probability_table(table: pa.Table) -> tuple[HashIndex, pa.Table]:\n",
    "\n",
    "    hidx = HashIndex(hash_type='sha1')\n",
    "\n",
    "    table = table.set_column(\n",
    "        table.column_names.index(\"left\"),\n",
    "        \"left\",\n",
    "        hidx.insert_hashes(table['left'])\n",
    "    )\n",
    "    table = table.set_column(\n",
    "        table.column_names.index(\"right\"),\n",
    "        \"right\",\n",
    "        hidx.insert_hashes(table['right'])\n",
    "    )\n",
    "    table = table.set_column(\n",
    "        table.column_names.index(\"hash\"),\n",
    "        \"hash\",\n",
    "        hidx.insert_hashes(table['hash'])\n",
    "    )\n",
    "\n",
    "    return hidx, table\n",
    "\n",
    "hidx, t2 = index_probability_table(t1)\n",
    "\n",
    "f\"{t1.nbytes / (1024 * 1024):.2f}MB\", f\"{hidx.table.nbytes / (1024 * 1024):.2f}MB\", f\"{t2.nbytes / (1024 * 1024):.2f}MB\", f\"{(hidx.table.nbytes + t2.nbytes) / (1024 * 1024):.2f}MB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.75"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(15 * 15) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.1"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(4230 / 5) / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5mb/s upload speed -- 25 mins for 7.6Gb, 14min for 4.3Gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "\n",
    "file_path = Path.cwd()\n",
    "\n",
    "pq.write_table(\n",
    "    t2,\n",
    "    file_path / 'probabilities_normalised_brot.parquet',\n",
    "    compression='BROTLI',\n",
    "    # compression_level=16,\n",
    "    # use_dictionary=True,\n",
    "    # write_statistics=True,\n",
    "    # use_byte_stream_split=True,\n",
    "    # row_group_size=1048576  # 1MB row groups\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidx.to_parquet(file_path / 'hash_index.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Impose threshold (don't store below x, default x)\n",
    "* Can we automate to check rub probabilities?\n",
    "    * Find any gigantic clusters as a result\n",
    "    * Run at plausible thresholds?\n",
    "* Is threshold but need a rule of thumb\n",
    "    * If dedupe, n^2 - n/2\n",
    "    * If link, n^2 - n/2 ish\n",
    "* \"How do you know that\" is hard in a chain, and people want that\n",
    "\n",
    "* duckdb from pg OR pg from duckdb -- both available to work with pg\n",
    "    * Bad for atomic updates\n",
    "\n",
    "---\n",
    "\n",
    "100m\n",
    "\n",
    "ZSTD 10s at 1, 1.86Gb\n",
    "ZSTD 10s at 4, 1.86Gb \n",
    "ZSTD 50s at 15, 1.86Gb\n",
    "ZSTD 4m at 16, 1.86Gb\n",
    "ZSTD 7m at 22, 1.52Gb\n",
    "BROTLI 2m at default, 1.57Gb\n",
    "\n",
    "Index\n",
    "\n",
    "Snappy 13s at default, 3.8Gb\n",
    "BROTLI 6m at default, 2.6Gb\n",
    "ZSTD 20s at default, 2.75Gb\n",
    "\n",
    "ZSTD balanced between the two: 4.235Gb (3Gb saving)\n",
    "\n",
    "--\n",
    "\n",
    "Work through pg/duckdb idea\n",
    "\n",
    "- Clusters and contains need appending -- parquet or postgres?\n",
    "- How is duckdb informed about new parquet\n",
    "    - Lambda? API?\n",
    "- Can this perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Duckdb\n",
    "    * R/W or R mode -- we'd launch in one or tother\n",
    "        * No lambdas, just run the new command\n",
    "* Consolidating a dataset (pruning old records) -- like upsert\n",
    "    * 3 ways\n",
    "        * duckdb directly, w process. Load two parquets, prune old\n",
    "        * pandas\n",
    "        * polars\n",
    "* (?) married to parquet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
