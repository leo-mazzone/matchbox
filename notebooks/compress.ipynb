{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compression notes\n",
    "\n",
    "* 100m SHA-1 hashes, binary, binary, binary, float\n",
    "    * Standard: 7.6Gb, written in 23 seconds\n",
    "    * ZSTD 8: 6.85Gb, written in 3 mins\n",
    "    * ZSTD 15: 6.85Gb, written in 13 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import sha1\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import random\n",
    "\n",
    "def fast_generate_hashes(n: int = int(2e7)) -> pa.Array:\n",
    "    \"\"\"Generate n SHA1 hashes using PyArrow arrays.\"\"\"\n",
    "    rand = random.randint(0, int(1e5))\n",
    "    hashes = [\n",
    "        sha1((i + rand).to_bytes(8, 'big')).digest() \n",
    "        for i in range(int(n))\n",
    "    ]\n",
    "    return pa.array(hashes, type=pa.binary())\n",
    "\n",
    "def fast_sample_pairs(hashes: pa.Array, n: int = int(1e8)) -> pa.Table:\n",
    "    \"\"\"Generate hash pairs with random new hashes.\"\"\"\n",
    "    hash_count = len(hashes)\n",
    "    \n",
    "    # Generate indices\n",
    "    left = np.random.randint(0, hash_count, n)\n",
    "    right = np.random.randint(0, hash_count - 1, n)\n",
    "    right += (right >= left)\n",
    "    \n",
    "    # Take values using PyArrow\n",
    "    left_hashes = hashes.take(pa.array(left))\n",
    "    right_hashes = hashes.take(pa.array(right))\n",
    "    \n",
    "    # Generate probabilities as PyArrow array (between 0.7 and 1.0 to 2 DP)\n",
    "    probs = pa.array(np.round(0.7 + 0.3 * np.random.random(n), 2), type=pa.float64())\n",
    "    \n",
    "    # Generate completely new random hashes instead of combining\n",
    "    new_hashes = [sha1(i.to_bytes(8, 'big')).digest() \n",
    "                  for i in range(n)]\n",
    "    combined_arr = pa.array(new_hashes, type=pa.binary())\n",
    "    \n",
    "    # Create table directly with PyArrow\n",
    "    return pa.table({\n",
    "        'hash': combined_arr,\n",
    "        'left': left_hashes,\n",
    "        'right': right_hashes,\n",
    "        'probability': probs\n",
    "    })\n",
    "\n",
    "def fast_sample_pairs_lr(left: pa.Array, right: pa.Array, n: int = int(1e8)) -> pa.Table:\n",
    "    \"\"\"Generate hash pairs with random new hashes.\n",
    "    \n",
    "    Assumes left and right are the same length.\n",
    "    \"\"\"\n",
    "    # Calculate total size of product space\n",
    "    hash_count = len(left)\n",
    "    total_pairs = len(left) * len(right)\n",
    "    \n",
    "    # If n is larger than total possible pairs, adjust n\n",
    "    n = min(n, total_pairs)\n",
    "    \n",
    "    # Generate n random indices from the product space\n",
    "    flat_indices = np.random.choice(total_pairs, size=n, replace=False)\n",
    "    \n",
    "    # Convert flat indices back to left and right indices\n",
    "    left_indices = flat_indices // hash_count\n",
    "    right_indices = flat_indices % hash_count\n",
    "    \n",
    "    # Take values using PyArrow for better performance\n",
    "    left_hashes = left.take(pa.array(left_indices))\n",
    "    right_hashes = right.take(pa.array(right_indices))\n",
    "    \n",
    "    # Generate probabilities as PyArrow array (between 0.7 and 1.0 to 2 DP)\n",
    "    probs = pa.array(np.round(0.7 + 0.3 * np.random.random(n), 2), type=pa.float64())\n",
    "    \n",
    "    # Generate completely new random hashes instead of combining\n",
    "    new_hashes = [\n",
    "        sha1(i.to_bytes(8, 'big')).digest() \n",
    "        for i in range(n)\n",
    "    ]\n",
    "    combined_arr = pa.array(new_hashes, type=pa.binary())\n",
    "    \n",
    "    # Create table directly with PyArrow\n",
    "    return pa.table({\n",
    "        'hash': combined_arr,\n",
    "        'left': left_hashes,\n",
    "        'right': right_hashes,\n",
    "        'probability': probs\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rustworkx as rx\n",
    "from matchbox.common.hash import list_to_value_ordered_hash\n",
    "\n",
    "def to_clusters(results: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Converts probabilities into a list of connected components formed at each threshold.\n",
    "\n",
    "    Returns:\n",
    "        Probabilities sorted by threshold descending.\n",
    "    \"\"\"\n",
    "    G = rx.PyGraph()\n",
    "    added: dict[bytes, int] = {}\n",
    "    components: dict[str, list] = {\"parent\": [], \"child\": [], \"threshold\": []}\n",
    "\n",
    "    # Sort probabilities descending and group by probability\n",
    "    edges_df = results.select(['left', 'right', 'probability']).sort_by([(\"probability\", \"descending\")])\n",
    "    \n",
    "    # Get unique probability thresholds, sorted\n",
    "    thresholds = pa.compute.unique(edges_df.column('probability'))\n",
    "\n",
    "    # Process edges grouped by probability threshold\n",
    "    for prob in thresholds.to_pylist():\n",
    "        mask = pa.compute.equal(edges_df.column('probability'), prob)\n",
    "        threshold_edges = edges_df.filter(mask)\n",
    "        # Get state before adding this batch of edges\n",
    "        old_components = {frozenset(comp) for comp in rx.connected_components(G)}\n",
    "\n",
    "        # Add all nodes and edges at this probability threshold\n",
    "        edge_values = zip(\n",
    "            threshold_edges.column('left').to_pylist(),\n",
    "            threshold_edges.column('right').to_pylist()\n",
    "        )\n",
    "\n",
    "        for left, right in edge_values:\n",
    "            for hash_val in (left, right):\n",
    "                if hash_val not in added:\n",
    "                    idx = G.add_node(hash_val)\n",
    "                    added[hash_val] = idx\n",
    "\n",
    "            G.add_edge(added[left], added[right], None)\n",
    "\n",
    "        new_components = {frozenset(comp) for comp in rx.connected_components(G)}\n",
    "        changed_components = new_components - old_components\n",
    "\n",
    "        # For each changed component, add ALL members at current threshold\n",
    "        for comp in changed_components:\n",
    "            children = sorted([G.get_node_data(n) for n in comp])\n",
    "            parent = list_to_value_ordered_hash(children)\n",
    "\n",
    "            components[\"parent\"].extend([parent] * len(children))\n",
    "            components[\"child\"].extend(children)\n",
    "            components[\"threshold\"].extend([prob] * len(children))\n",
    "\n",
    "    return pa.Table.from_pydict(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def _cluster_results_to_hierarchical(\n",
    "    probabilities: pa.Table,\n",
    "    clusters: pa.Table,\n",
    ") -> list[tuple[bytes, bytes, float]]:\n",
    "    \"\"\"\n",
    "    Converts results to a hierarchical structure by building up from base components.\n",
    "\n",
    "    Args:\n",
    "        probabilities: Original pairwise probabilities containing base components\n",
    "        clusters: Connected components at each threshold\n",
    "\n",
    "    Returns:\n",
    "        List of (parent, child, threshold) tuples representing the hierarchy\n",
    "    \"\"\"\n",
    "    thresholds = pa.compute.unique(clusters['threshold']).sort(order='descending')\n",
    "\n",
    "    # Add all clusters corresponding to a simple two-item probability edge\n",
    "\n",
    "    hierarchy = []\n",
    "    # Convert to record batches for efficient iteration\n",
    "    for batch in probabilities.to_batches():\n",
    "        parent_array = batch.column('hash')\n",
    "        left_array = batch.column('left')\n",
    "        right_array = batch.column('right')\n",
    "        prob_array = batch.column('probability')\n",
    "        \n",
    "        for i in range(len(batch)):\n",
    "            parent = parent_array[i].as_py()\n",
    "            left_id = left_array[i].as_py()\n",
    "            right_id = right_array[i].as_py()\n",
    "            prob = float(prob_array[i].as_py())\n",
    "            hierarchy.extend(\n",
    "                [(parent, left_id, prob), (parent, right_id, prob)]\n",
    "            )\n",
    "\n",
    "    # Create adjacency structure for quick lookups\n",
    "    adj_dict: dict[bytes, set[tuple[bytes, float]]] = defaultdict(set)\n",
    "    for parent, child, prob in hierarchy:\n",
    "        adj_dict[child].add((parent, prob))\n",
    "\n",
    "    # Process each threshold level, getting clusters at each threshold\n",
    "    for threshold in thresholds:\n",
    "        threshold_float = float(threshold.as_py())\n",
    "\n",
    "        # Filter clusters at current threshold\n",
    "        mask = pa.compute.equal(clusters.column('threshold'), threshold)\n",
    "        current_clusters = clusters.filter(mask)\n",
    "\n",
    "        # Group by parent\n",
    "        parent_groups = {}\n",
    "        for batch in current_clusters.to_batches():\n",
    "            parent_col = batch.column('parent')\n",
    "            child_col = batch.column('child')\n",
    "            for i in range(len(batch)):\n",
    "                parent = parent_col[i].as_py()\n",
    "                child = child_col[i].as_py()\n",
    "                if parent not in parent_groups:\n",
    "                    parent_groups[parent] = set()\n",
    "                parent_groups[parent].add(child)\n",
    "\n",
    "        # Process each component\n",
    "        for parent, members in parent_groups.items():\n",
    "            if len(members) <= 2:\n",
    "                continue\n",
    "\n",
    "            seen = set(members)\n",
    "            current = set(members)\n",
    "            ultimate_parents = set()\n",
    "\n",
    "            # Keep traversing until we've explored all paths\n",
    "            while current:\n",
    "                next_level = set()\n",
    "                # If any current nodes have no parents above threshold,\n",
    "                # they are ultimate parents for this threshold\n",
    "                for node in current:\n",
    "                    parents = {\n",
    "                        p for p, prob in adj_dict[node] if prob >= threshold_float\n",
    "                    }\n",
    "                    next_parents = parents - seen\n",
    "                    if not parents:  # No parents = ultimate parent\n",
    "                        ultimate_parents.add(node)\n",
    "\n",
    "                    next_level.update(next_parents)\n",
    "                    seen.update(parents)\n",
    "\n",
    "                current = next_level\n",
    "\n",
    "            for up in ultimate_parents:\n",
    "                hierarchy.append((parent, up, threshold_float))\n",
    "                adj_dict[up].add((parent, threshold_float))\n",
    "\n",
    "    return sorted(hierarchy, key=lambda x: (x[2], x[0], x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from collections import defaultdict\n",
    "\n",
    "def _cluster_results_to_hierarchical_pa(\n",
    "    probabilities: pa.Table,\n",
    "    clusters: pa.Table,\n",
    ") -> pa.Table:\n",
    "    \"\"\"\n",
    "    Converts results to a hierarchical structure by building up from base components.\n",
    "\n",
    "    Args:\n",
    "        probabilities: Original pairwise probabilities containing base components\n",
    "        clusters: Connected components at each threshold\n",
    "\n",
    "    Returns:\n",
    "        PyArrow Table with schema:\n",
    "            parent: binary\n",
    "            child: binary\n",
    "            threshold: double\n",
    "    \"\"\"\n",
    "    thresholds = pa.compute.unique(clusters['threshold']).sort(order='descending')\n",
    "\n",
    "    # Add all clusters corresponding to a simple two-item probability edge\n",
    "    hierarchy = []\n",
    "    # Convert to record batches for efficient iteration\n",
    "    for batch in probabilities.to_batches():\n",
    "        parent_array = batch.column('hash')\n",
    "        left_array = batch.column('left')\n",
    "        right_array = batch.column('right')\n",
    "        prob_array = batch.column('probability')\n",
    "        \n",
    "        for i in range(len(batch)):\n",
    "            parent = parent_array[i].as_py()\n",
    "            left_id = left_array[i].as_py()\n",
    "            right_id = right_array[i].as_py()\n",
    "            prob = float(prob_array[i].as_py())\n",
    "            hierarchy.extend(\n",
    "                [(parent, left_id, prob), (parent, right_id, prob)]\n",
    "            )\n",
    "\n",
    "    # Create adjacency structure for quick lookups\n",
    "    adj_dict: dict[bytes, set[tuple[bytes, float]]] = defaultdict(set)\n",
    "    for parent, child, prob in hierarchy:\n",
    "        adj_dict[child].add((parent, prob))\n",
    "\n",
    "    # Process each threshold level, getting clusters at each threshold\n",
    "    for threshold in thresholds:\n",
    "        threshold_float = float(threshold.as_py())\n",
    "\n",
    "        # Filter clusters at current threshold\n",
    "        mask = pa.compute.equal(clusters.column('threshold'), threshold)\n",
    "        current_clusters = clusters.filter(mask)\n",
    "\n",
    "        # Group by parent\n",
    "        parent_groups = {}\n",
    "        for batch in current_clusters.to_batches():\n",
    "            parent_col = batch.column('parent')\n",
    "            child_col = batch.column('child')\n",
    "            for i in range(len(batch)):\n",
    "                parent = parent_col[i].as_py()\n",
    "                child = child_col[i].as_py()\n",
    "                if parent not in parent_groups:\n",
    "                    parent_groups[parent] = set()\n",
    "                parent_groups[parent].add(child)\n",
    "\n",
    "        # Process each component\n",
    "        for parent, members in parent_groups.items():\n",
    "            if len(members) <= 2:\n",
    "                continue\n",
    "\n",
    "            seen = set(members)\n",
    "            current = set(members)\n",
    "            ultimate_parents = set()\n",
    "\n",
    "            # Keep traversing until we've explored all paths\n",
    "            while current:\n",
    "                next_level = set()\n",
    "                # If any current nodes have no parents above threshold,\n",
    "                # they are ultimate parents for this threshold\n",
    "                for node in current:\n",
    "                    parents = {\n",
    "                        p for p, prob in adj_dict[node] if prob >= threshold_float\n",
    "                    }\n",
    "                    next_parents = parents - seen\n",
    "                    if not parents:  # No parents = ultimate parent\n",
    "                        ultimate_parents.add(node)\n",
    "\n",
    "                    next_level.update(next_parents)\n",
    "                    seen.update(parents)\n",
    "\n",
    "                current = next_level\n",
    "\n",
    "            for up in ultimate_parents:\n",
    "                hierarchy.append((parent, up, threshold_float))\n",
    "                adj_dict[up].add((parent, threshold_float))\n",
    "\n",
    "    # Sort the hierarchy\n",
    "    hierarchy.sort(key=lambda x: (x[2], x[0], x[1]), reverse=True)\n",
    "    \n",
    "    # Convert to PyArrow Table\n",
    "    return pa.Table.from_arrays(\n",
    "        [\n",
    "            pa.array([h[0] for h in hierarchy], type=pa.binary()),\n",
    "            pa.array([h[1] for h in hierarchy], type=pa.binary()),\n",
    "            pa.array([h[2] for h in hierarchy], type=pa.float64())\n",
    "        ],\n",
    "        names=['parent', 'child', 'threshold']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import logging\n",
    "import time\n",
    "\n",
    "def setup_logger() -> logging.Logger:\n",
    "    \"\"\"Set up a logger with appropriate formatting.\"\"\"\n",
    "    logger = logging.getLogger('cluster_profiler')\n",
    "    if not logger.handlers:\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "def _cluster_results_to_hierarchical_4(\n",
    "    probabilities: pa.Table,\n",
    "    clusters: pa.Table,\n",
    ") -> list[tuple[bytes, bytes, float]]:\n",
    "    \"\"\"\n",
    "    Converts results to a hierarchical structure using hybrid approach:\n",
    "    1. Use parent counting to identify potential ultimate parents\n",
    "    2. Use targeted graph traversal to verify and find missed relationships\n",
    "    \"\"\"\n",
    "    logger = setup_logger()\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    logger.info(\"Starting clustering process\")\n",
    "    logger.debug(f\"Input probabilities table size: {len(probabilities)}\")\n",
    "    logger.debug(f\"Input clusters table size: {len(clusters)}\")\n",
    "    \n",
    "    # Initialize hierarchy with base probabilities\n",
    "    init_start = time.time()\n",
    "    probabilities_hash_array = probabilities['hash'].combine_chunks()\n",
    "    probabilities_left_array = probabilities['left'].combine_chunks()\n",
    "    probabilities_right_array = probabilities['right'].combine_chunks()\n",
    "    probabilities_prob_array = probabilities['probability'].combine_chunks()\n",
    "    \n",
    "    hierarchy_parent = pa.concat_arrays([\n",
    "        probabilities_hash_array,\n",
    "        probabilities_hash_array\n",
    "    ])\n",
    "    hierarchy_child = pa.concat_arrays([\n",
    "        probabilities_left_array,\n",
    "        probabilities_right_array\n",
    "    ])\n",
    "    hierarchy_prob = pa.concat_arrays([\n",
    "        probabilities_prob_array,\n",
    "        probabilities_prob_array\n",
    "    ])\n",
    "    \n",
    "    hierarchy_table = pa.Table.from_arrays(\n",
    "        [hierarchy_parent, hierarchy_child, hierarchy_prob],\n",
    "        names=['parent', 'child', 'probability']\n",
    "    )\n",
    "    \n",
    "    # Convert to Python objects\n",
    "    hierarchy = list(zip(\n",
    "        hierarchy_table['parent'].to_pylist(),\n",
    "        hierarchy_table['child'].to_pylist(),\n",
    "        hierarchy_table['probability'].to_pylist()\n",
    "    ))\n",
    "    \n",
    "    # Create initial parent map\n",
    "    adj_dict: dict[bytes, set[tuple[bytes, float]]] = defaultdict(set)\n",
    "    for parent, child, prob in hierarchy:\n",
    "        adj_dict[child].add((parent, prob))\n",
    "    \n",
    "    logger.debug(f\"Initialization took {time.time() - init_start:.4f}s\")\n",
    "    \n",
    "    thresholds = pa.compute.unique(clusters['threshold']).sort(order='descending')\n",
    "    \n",
    "    # Process each threshold\n",
    "    for threshold in thresholds.to_pylist():\n",
    "        threshold_start = time.time()\n",
    "        logger.debug(f\"Processing threshold: {threshold}\")\n",
    "        threshold_float = float(threshold)\n",
    "        \n",
    "        # Filter clusters for this threshold\n",
    "        current_clusters = clusters.filter(pc.equal(clusters['threshold'], threshold))\n",
    "        logger.debug(f\"Filtered clusters size: {len(current_clusters)}\")\n",
    "        \n",
    "        # Group by parent\n",
    "        parent_groups = defaultdict(set)\n",
    "        parent_list = current_clusters['parent'].to_pylist()\n",
    "        child_list = current_clusters['child'].to_pylist()\n",
    "        for parent, child in zip(parent_list, child_list):\n",
    "            parent_groups[parent].add(child)\n",
    "        \n",
    "        comp_start = time.time()\n",
    "        # Process each component\n",
    "        for parent, members in parent_groups.items():\n",
    "            if len(members) <= 2:\n",
    "                continue\n",
    "                \n",
    "            # Count direct parents for each node at this threshold\n",
    "            parent_count = defaultdict(int)\n",
    "            for node in members:\n",
    "                for p, prob in adj_dict[node]:\n",
    "                    if prob >= threshold_float:\n",
    "                        parent_count[node] += 1\n",
    "            \n",
    "            # Find initial candidates with no direct parents\n",
    "            candidates = {\n",
    "                node for node in members \n",
    "                if parent_count[node] == 0\n",
    "            }\n",
    "            \n",
    "            # Now do targeted graph traversal from each non-candidate\n",
    "            # to identify any candidates that have indirect parents\n",
    "            not_ultimate = set()\n",
    "            for node in members - candidates:\n",
    "                if node in not_ultimate:\n",
    "                    continue\n",
    "                    \n",
    "                # Start traversal from this node\n",
    "                stack = [node]\n",
    "                seen = {node}\n",
    "                \n",
    "                while stack:\n",
    "                    current = stack.pop()\n",
    "                    for p, prob in adj_dict[current]:\n",
    "                        if prob >= threshold_float and p not in seen:\n",
    "                            if p in candidates:\n",
    "                                # Found a path to a candidate - it can't be an ultimate parent\n",
    "                                candidates.remove(p)\n",
    "                            stack.append(p)\n",
    "                            seen.add(p)\n",
    "                            not_ultimate.add(p)\n",
    "            \n",
    "            # Add hierarchy entries for true ultimate parents\n",
    "            for up in candidates:\n",
    "                hierarchy.append((parent, up, threshold_float))\n",
    "                adj_dict[up].add((parent, threshold_float))\n",
    "        \n",
    "        logger.debug(f\"Component processing took {time.time() - comp_start:.4f}s\")\n",
    "        logger.debug(f\"Total threshold processing took {time.time() - threshold_start:.4f}s\")\n",
    "    \n",
    "    sort_start = time.time()\n",
    "    result = sorted(hierarchy, key=lambda x: (x[2], x[0], x[1]), reverse=True)\n",
    "    logger.debug(f\"Sorting took {time.time() - sort_start:.4f}s\")\n",
    "    logger.debug(f\"Final hierarchy size: {len(result)}\")\n",
    "    \n",
    "    total_time = time.time() - total_start_time\n",
    "    logger.debug(f\"Total execution time: {total_time:.4f}s\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashes = fast_generate_hashes(int(2e5))\n",
    "probabilities = fast_sample_pairs(hashes, int(1e6))\n",
    "clusters = to_clusters(probabilities)\n",
    "hierarchical = _cluster_results_to_hierarchical(probabilities, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical = _cluster_results_to_hierarchical(probabilities, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 17:39:35,730 - cluster_profiler - INFO - Starting clustering process\n",
      "2024-12-10 17:39:35,732 - cluster_profiler - DEBUG - Input probabilities table size: 1000000\n",
      "2024-12-10 17:39:35,732 - cluster_profiler - DEBUG - Input clusters table size: 5279385\n",
      "2024-12-10 17:39:35,753 - cluster_profiler - DEBUG - Threshold computation took 0.0204 seconds\n",
      "2024-12-10 17:39:35,754 - cluster_profiler - DEBUG - Number of unique thresholds: 30\n",
      "2024-12-10 17:39:35,990 - cluster_profiler - DEBUG - Hierarchy initialization took 0.2352 seconds\n",
      "2024-12-10 17:39:35,992 - cluster_profiler - DEBUG - Hierarchy table creation took 0.0002 seconds\n",
      "2024-12-10 17:39:35,993 - cluster_profiler - DEBUG - Memory usage of hierarchy_table: 106.81 MB\n",
      "2024-12-10 17:39:40,565 - cluster_profiler - DEBUG - Python object conversion took 4.5699 seconds\n",
      "2024-12-10 17:39:40,566 - cluster_profiler - DEBUG - Initial hierarchy size: 2000000\n",
      "2024-12-10 17:39:42,735 - cluster_profiler - DEBUG - Adjacency dictionary creation took 2.1686 seconds\n",
      "2024-12-10 17:39:42,736 - cluster_profiler - DEBUG - Adjacency dictionary size: 199989\n",
      "2024-12-10 17:39:42,737 - cluster_profiler - DEBUG - Processing threshold: 1.0\n",
      "2024-12-10 17:39:42,743 - cluster_profiler - DEBUG - Cluster filtering for threshold 1.0 took 0.0055 seconds\n",
      "2024-12-10 17:39:42,744 - cluster_profiler - DEBUG - Filtered clusters size: 30689\n",
      "2024-12-10 17:39:42,822 - cluster_profiler - DEBUG - Parent grouping took 0.0781 seconds\n",
      "2024-12-10 17:39:42,823 - cluster_profiler - DEBUG - Number of parent groups: 14054\n",
      "2024-12-10 17:39:42,873 - cluster_profiler - DEBUG - Component processing for threshold 1.0 took 0.0496 seconds\n",
      "2024-12-10 17:39:42,874 - cluster_profiler - DEBUG - Components processed: 2069\n",
      "2024-12-10 17:39:42,875 - cluster_profiler - DEBUG - Total processing for threshold 1.0 took 0.1380 seconds\n",
      "2024-12-10 17:39:42,875 - cluster_profiler - DEBUG - Processing threshold: 0.99\n",
      "2024-12-10 17:39:42,884 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.99 took 0.0081 seconds\n",
      "2024-12-10 17:39:42,885 - cluster_profiler - DEBUG - Filtered clusters size: 63676\n",
      "2024-12-10 17:39:43,060 - cluster_profiler - DEBUG - Parent grouping took 0.1746 seconds\n",
      "2024-12-10 17:39:43,061 - cluster_profiler - DEBUG - Number of parent groups: 21683\n",
      "2024-12-10 17:39:43,369 - cluster_profiler - DEBUG - Component processing for threshold 0.99 took 0.3080 seconds\n",
      "2024-12-10 17:39:43,370 - cluster_profiler - DEBUG - Components processed: 9432\n",
      "2024-12-10 17:39:43,371 - cluster_profiler - DEBUG - Total processing for threshold 0.99 took 0.4955 seconds\n",
      "2024-12-10 17:39:43,372 - cluster_profiler - DEBUG - Processing threshold: 0.98\n",
      "2024-12-10 17:39:43,379 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.98 took 0.0071 seconds\n",
      "2024-12-10 17:39:43,380 - cluster_profiler - DEBUG - Filtered clusters size: 83436\n",
      "2024-12-10 17:39:44,183 - cluster_profiler - DEBUG - Parent grouping took 0.8020 seconds\n",
      "2024-12-10 17:39:44,184 - cluster_profiler - DEBUG - Number of parent groups: 17344\n",
      "2024-12-10 17:39:45,194 - cluster_profiler - DEBUG - Component processing for threshold 0.98 took 1.0095 seconds\n",
      "2024-12-10 17:39:45,194 - cluster_profiler - DEBUG - Components processed: 11006\n",
      "2024-12-10 17:39:45,195 - cluster_profiler - DEBUG - Total processing for threshold 0.98 took 1.8230 seconds\n",
      "2024-12-10 17:39:45,195 - cluster_profiler - DEBUG - Processing threshold: 0.97\n",
      "2024-12-10 17:39:45,204 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.97 took 0.0078 seconds\n",
      "2024-12-10 17:39:45,204 - cluster_profiler - DEBUG - Filtered clusters size: 108385\n",
      "2024-12-10 17:39:45,463 - cluster_profiler - DEBUG - Parent grouping took 0.2578 seconds\n",
      "2024-12-10 17:39:45,464 - cluster_profiler - DEBUG - Number of parent groups: 9956\n",
      "2024-12-10 17:39:56,241 - cluster_profiler - DEBUG - Component processing for threshold 0.97 took 10.7772 seconds\n",
      "2024-12-10 17:39:56,242 - cluster_profiler - DEBUG - Components processed: 6760\n",
      "2024-12-10 17:39:56,242 - cluster_profiler - DEBUG - Total processing for threshold 0.97 took 11.0472 seconds\n",
      "2024-12-10 17:39:56,243 - cluster_profiler - DEBUG - Processing threshold: 0.96\n",
      "2024-12-10 17:39:56,250 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.96 took 0.0071 seconds\n",
      "2024-12-10 17:39:56,251 - cluster_profiler - DEBUG - Filtered clusters size: 134693\n",
      "2024-12-10 17:39:56,535 - cluster_profiler - DEBUG - Parent grouping took 0.2837 seconds\n",
      "2024-12-10 17:39:56,536 - cluster_profiler - DEBUG - Number of parent groups: 4484\n",
      "2024-12-10 17:40:08,293 - cluster_profiler - DEBUG - Component processing for threshold 0.96 took 11.7563 seconds\n",
      "2024-12-10 17:40:08,293 - cluster_profiler - DEBUG - Components processed: 2755\n",
      "2024-12-10 17:40:08,294 - cluster_profiler - DEBUG - Total processing for threshold 0.96 took 12.0513 seconds\n",
      "2024-12-10 17:40:08,295 - cluster_profiler - DEBUG - Processing threshold: 0.95\n",
      "2024-12-10 17:40:08,305 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.95 took 0.0096 seconds\n",
      "2024-12-10 17:40:08,306 - cluster_profiler - DEBUG - Filtered clusters size: 155322\n",
      "2024-12-10 17:40:08,620 - cluster_profiler - DEBUG - Parent grouping took 0.3136 seconds\n",
      "2024-12-10 17:40:08,621 - cluster_profiler - DEBUG - Number of parent groups: 1895\n",
      "2024-12-10 17:40:15,285 - cluster_profiler - DEBUG - Component processing for threshold 0.95 took 6.6642 seconds\n",
      "2024-12-10 17:40:15,286 - cluster_profiler - DEBUG - Components processed: 993\n",
      "2024-12-10 17:40:15,287 - cluster_profiler - DEBUG - Total processing for threshold 0.95 took 6.9918 seconds\n",
      "2024-12-10 17:40:15,287 - cluster_profiler - DEBUG - Processing threshold: 0.94\n",
      "2024-12-10 17:40:15,294 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.94 took 0.0069 seconds\n",
      "2024-12-10 17:40:15,295 - cluster_profiler - DEBUG - Filtered clusters size: 169726\n",
      "2024-12-10 17:40:15,633 - cluster_profiler - DEBUG - Parent grouping took 0.3371 seconds\n",
      "2024-12-10 17:40:15,633 - cluster_profiler - DEBUG - Number of parent groups: 807\n",
      "2024-12-10 17:40:18,984 - cluster_profiler - DEBUG - Component processing for threshold 0.94 took 3.3506 seconds\n",
      "2024-12-10 17:40:18,985 - cluster_profiler - DEBUG - Components processed: 368\n",
      "2024-12-10 17:40:18,986 - cluster_profiler - DEBUG - Total processing for threshold 0.94 took 3.6987 seconds\n",
      "2024-12-10 17:40:18,986 - cluster_profiler - DEBUG - Processing threshold: 0.93\n",
      "2024-12-10 17:40:18,995 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.93 took 0.0080 seconds\n",
      "2024-12-10 17:40:18,995 - cluster_profiler - DEBUG - Filtered clusters size: 179643\n",
      "2024-12-10 17:40:19,358 - cluster_profiler - DEBUG - Parent grouping took 0.3623 seconds\n",
      "2024-12-10 17:40:19,359 - cluster_profiler - DEBUG - Number of parent groups: 364\n",
      "2024-12-10 17:40:21,576 - cluster_profiler - DEBUG - Component processing for threshold 0.93 took 2.2170 seconds\n",
      "2024-12-10 17:40:21,577 - cluster_profiler - DEBUG - Components processed: 141\n",
      "2024-12-10 17:40:21,578 - cluster_profiler - DEBUG - Total processing for threshold 0.93 took 2.5918 seconds\n",
      "2024-12-10 17:40:21,579 - cluster_profiler - DEBUG - Processing threshold: 0.92\n",
      "2024-12-10 17:40:21,590 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.92 took 0.0111 seconds\n",
      "2024-12-10 17:40:21,591 - cluster_profiler - DEBUG - Filtered clusters size: 186031\n",
      "2024-12-10 17:40:21,963 - cluster_profiler - DEBUG - Parent grouping took 0.3713 seconds\n",
      "2024-12-10 17:40:21,964 - cluster_profiler - DEBUG - Number of parent groups: 160\n",
      "2024-12-10 17:40:23,840 - cluster_profiler - DEBUG - Component processing for threshold 0.92 took 1.8757 seconds\n",
      "2024-12-10 17:40:23,841 - cluster_profiler - DEBUG - Components processed: 61\n",
      "2024-12-10 17:40:23,841 - cluster_profiler - DEBUG - Total processing for threshold 0.92 took 2.2626 seconds\n",
      "2024-12-10 17:40:23,842 - cluster_profiler - DEBUG - Processing threshold: 0.91\n",
      "2024-12-10 17:40:23,850 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.91 took 0.0078 seconds\n",
      "2024-12-10 17:40:23,851 - cluster_profiler - DEBUG - Filtered clusters size: 190338\n",
      "2024-12-10 17:40:24,247 - cluster_profiler - DEBUG - Parent grouping took 0.3960 seconds\n",
      "2024-12-10 17:40:24,248 - cluster_profiler - DEBUG - Number of parent groups: 85\n",
      "2024-12-10 17:40:26,008 - cluster_profiler - DEBUG - Component processing for threshold 0.91 took 1.7597 seconds\n",
      "2024-12-10 17:40:26,009 - cluster_profiler - DEBUG - Components processed: 26\n",
      "2024-12-10 17:40:26,010 - cluster_profiler - DEBUG - Total processing for threshold 0.91 took 2.1678 seconds\n",
      "2024-12-10 17:40:26,010 - cluster_profiler - DEBUG - Processing threshold: 0.9\n",
      "2024-12-10 17:40:26,019 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.9 took 0.0080 seconds\n",
      "2024-12-10 17:40:26,020 - cluster_profiler - DEBUG - Filtered clusters size: 193311\n",
      "2024-12-10 17:40:26,413 - cluster_profiler - DEBUG - Parent grouping took 0.3929 seconds\n",
      "2024-12-10 17:40:26,413 - cluster_profiler - DEBUG - Number of parent groups: 43\n",
      "2024-12-10 17:40:28,944 - cluster_profiler - DEBUG - Component processing for threshold 0.9 took 2.5296 seconds\n",
      "2024-12-10 17:40:28,945 - cluster_profiler - DEBUG - Components processed: 13\n",
      "2024-12-10 17:40:28,945 - cluster_profiler - DEBUG - Total processing for threshold 0.9 took 2.9350 seconds\n",
      "2024-12-10 17:40:28,946 - cluster_profiler - DEBUG - Processing threshold: 0.89\n",
      "2024-12-10 17:40:28,955 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.89 took 0.0083 seconds\n",
      "2024-12-10 17:40:28,956 - cluster_profiler - DEBUG - Filtered clusters size: 195329\n",
      "2024-12-10 17:40:29,347 - cluster_profiler - DEBUG - Parent grouping took 0.3906 seconds\n",
      "2024-12-10 17:40:29,348 - cluster_profiler - DEBUG - Number of parent groups: 22\n",
      "2024-12-10 17:40:31,146 - cluster_profiler - DEBUG - Component processing for threshold 0.89 took 1.7980 seconds\n",
      "2024-12-10 17:40:31,147 - cluster_profiler - DEBUG - Components processed: 5\n",
      "2024-12-10 17:40:31,147 - cluster_profiler - DEBUG - Total processing for threshold 0.89 took 2.2015 seconds\n",
      "2024-12-10 17:40:31,148 - cluster_profiler - DEBUG - Processing threshold: 0.88\n",
      "2024-12-10 17:40:31,157 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.88 took 0.0087 seconds\n",
      "2024-12-10 17:40:31,158 - cluster_profiler - DEBUG - Filtered clusters size: 196736\n",
      "2024-12-10 17:40:31,561 - cluster_profiler - DEBUG - Parent grouping took 0.4022 seconds\n",
      "2024-12-10 17:40:31,562 - cluster_profiler - DEBUG - Number of parent groups: 6\n",
      "2024-12-10 17:40:33,429 - cluster_profiler - DEBUG - Component processing for threshold 0.88 took 1.8662 seconds\n",
      "2024-12-10 17:40:33,429 - cluster_profiler - DEBUG - Components processed: 2\n",
      "2024-12-10 17:40:33,430 - cluster_profiler - DEBUG - Total processing for threshold 0.88 took 2.2818 seconds\n",
      "2024-12-10 17:40:33,431 - cluster_profiler - DEBUG - Processing threshold: 0.87\n",
      "2024-12-10 17:40:33,443 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.87 took 0.0119 seconds\n",
      "2024-12-10 17:40:33,444 - cluster_profiler - DEBUG - Filtered clusters size: 197678\n",
      "2024-12-10 17:40:33,842 - cluster_profiler - DEBUG - Parent grouping took 0.3974 seconds\n",
      "2024-12-10 17:40:33,843 - cluster_profiler - DEBUG - Number of parent groups: 6\n",
      "2024-12-10 17:40:35,779 - cluster_profiler - DEBUG - Component processing for threshold 0.87 took 1.9360 seconds\n",
      "2024-12-10 17:40:35,780 - cluster_profiler - DEBUG - Components processed: 3\n",
      "2024-12-10 17:40:35,781 - cluster_profiler - DEBUG - Total processing for threshold 0.87 took 2.3499 seconds\n",
      "2024-12-10 17:40:35,782 - cluster_profiler - DEBUG - Processing threshold: 0.86\n",
      "2024-12-10 17:40:35,791 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.86 took 0.0085 seconds\n",
      "2024-12-10 17:40:35,791 - cluster_profiler - DEBUG - Filtered clusters size: 198353\n",
      "2024-12-10 17:40:36,205 - cluster_profiler - DEBUG - Parent grouping took 0.4133 seconds\n",
      "2024-12-10 17:40:36,206 - cluster_profiler - DEBUG - Number of parent groups: 3\n",
      "2024-12-10 17:40:38,207 - cluster_profiler - DEBUG - Component processing for threshold 0.86 took 2.0006 seconds\n",
      "2024-12-10 17:40:38,208 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:40:38,208 - cluster_profiler - DEBUG - Total processing for threshold 0.86 took 2.4269 seconds\n",
      "2024-12-10 17:40:38,209 - cluster_profiler - DEBUG - Processing threshold: 0.85\n",
      "2024-12-10 17:40:38,218 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.85 took 0.0087 seconds\n",
      "2024-12-10 17:40:38,219 - cluster_profiler - DEBUG - Filtered clusters size: 198850\n",
      "2024-12-10 17:40:38,624 - cluster_profiler - DEBUG - Parent grouping took 0.4045 seconds\n",
      "2024-12-10 17:40:38,625 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:40:40,713 - cluster_profiler - DEBUG - Component processing for threshold 0.85 took 2.0883 seconds\n",
      "2024-12-10 17:40:40,714 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:40:40,715 - cluster_profiler - DEBUG - Total processing for threshold 0.85 took 2.5057 seconds\n",
      "2024-12-10 17:40:40,715 - cluster_profiler - DEBUG - Processing threshold: 0.84\n",
      "2024-12-10 17:40:40,724 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.84 took 0.0081 seconds\n",
      "2024-12-10 17:40:40,725 - cluster_profiler - DEBUG - Filtered clusters size: 199174\n",
      "2024-12-10 17:40:41,117 - cluster_profiler - DEBUG - Parent grouping took 0.3920 seconds\n",
      "2024-12-10 17:40:41,118 - cluster_profiler - DEBUG - Number of parent groups: 2\n",
      "2024-12-10 17:40:43,277 - cluster_profiler - DEBUG - Component processing for threshold 0.84 took 2.1577 seconds\n",
      "2024-12-10 17:40:43,277 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:40:43,278 - cluster_profiler - DEBUG - Total processing for threshold 0.84 took 2.5627 seconds\n",
      "2024-12-10 17:40:43,279 - cluster_profiler - DEBUG - Processing threshold: 0.83\n",
      "2024-12-10 17:40:43,288 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.83 took 0.0082 seconds\n",
      "2024-12-10 17:40:43,288 - cluster_profiler - DEBUG - Filtered clusters size: 199411\n",
      "2024-12-10 17:40:43,696 - cluster_profiler - DEBUG - Parent grouping took 0.4074 seconds\n",
      "2024-12-10 17:40:43,697 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:40:45,951 - cluster_profiler - DEBUG - Component processing for threshold 0.83 took 2.2534 seconds\n",
      "2024-12-10 17:40:45,952 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:40:45,952 - cluster_profiler - DEBUG - Total processing for threshold 0.83 took 2.6737 seconds\n",
      "2024-12-10 17:40:45,953 - cluster_profiler - DEBUG - Processing threshold: 0.82\n",
      "2024-12-10 17:40:45,962 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.82 took 0.0081 seconds\n",
      "2024-12-10 17:40:45,962 - cluster_profiler - DEBUG - Filtered clusters size: 199589\n",
      "2024-12-10 17:40:46,365 - cluster_profiler - DEBUG - Parent grouping took 0.4021 seconds\n",
      "2024-12-10 17:40:46,366 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:40:49,481 - cluster_profiler - DEBUG - Component processing for threshold 0.82 took 3.1153 seconds\n",
      "2024-12-10 17:40:49,482 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:40:49,483 - cluster_profiler - DEBUG - Total processing for threshold 0.82 took 3.5297 seconds\n",
      "2024-12-10 17:40:49,484 - cluster_profiler - DEBUG - Processing threshold: 0.81\n",
      "2024-12-10 17:40:49,493 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.81 took 0.0083 seconds\n",
      "2024-12-10 17:40:49,494 - cluster_profiler - DEBUG - Filtered clusters size: 199696\n",
      "2024-12-10 17:40:49,908 - cluster_profiler - DEBUG - Parent grouping took 0.4135 seconds\n",
      "2024-12-10 17:40:49,908 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:40:52,234 - cluster_profiler - DEBUG - Component processing for threshold 0.81 took 2.3250 seconds\n",
      "2024-12-10 17:40:52,235 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:40:52,235 - cluster_profiler - DEBUG - Total processing for threshold 0.81 took 2.7515 seconds\n",
      "2024-12-10 17:40:52,236 - cluster_profiler - DEBUG - Processing threshold: 0.8\n",
      "2024-12-10 17:40:52,245 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.8 took 0.0081 seconds\n",
      "2024-12-10 17:40:52,245 - cluster_profiler - DEBUG - Filtered clusters size: 199790\n",
      "2024-12-10 17:40:52,670 - cluster_profiler - DEBUG - Parent grouping took 0.4239 seconds\n",
      "2024-12-10 17:40:52,671 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:40:55,040 - cluster_profiler - DEBUG - Component processing for threshold 0.8 took 2.3690 seconds\n",
      "2024-12-10 17:40:55,041 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:40:55,042 - cluster_profiler - DEBUG - Total processing for threshold 0.8 took 2.8054 seconds\n",
      "2024-12-10 17:40:55,042 - cluster_profiler - DEBUG - Processing threshold: 0.79\n",
      "2024-12-10 17:40:55,051 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.79 took 0.0081 seconds\n",
      "2024-12-10 17:40:55,052 - cluster_profiler - DEBUG - Filtered clusters size: 199844\n",
      "2024-12-10 17:40:55,445 - cluster_profiler - DEBUG - Parent grouping took 0.3931 seconds\n",
      "2024-12-10 17:40:55,446 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:40:57,931 - cluster_profiler - DEBUG - Component processing for threshold 0.79 took 2.4840 seconds\n",
      "2024-12-10 17:40:57,932 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:40:57,932 - cluster_profiler - DEBUG - Total processing for threshold 0.79 took 2.8897 seconds\n",
      "2024-12-10 17:40:57,933 - cluster_profiler - DEBUG - Processing threshold: 0.78\n",
      "2024-12-10 17:40:57,942 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.78 took 0.0083 seconds\n",
      "2024-12-10 17:40:57,943 - cluster_profiler - DEBUG - Filtered clusters size: 199893\n",
      "2024-12-10 17:40:58,363 - cluster_profiler - DEBUG - Parent grouping took 0.4202 seconds\n",
      "2024-12-10 17:40:58,364 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:41:01,219 - cluster_profiler - DEBUG - Component processing for threshold 0.78 took 2.8547 seconds\n",
      "2024-12-10 17:41:01,220 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:41:01,221 - cluster_profiler - DEBUG - Total processing for threshold 0.78 took 3.2878 seconds\n",
      "2024-12-10 17:41:01,221 - cluster_profiler - DEBUG - Processing threshold: 0.77\n",
      "2024-12-10 17:41:01,230 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.77 took 0.0082 seconds\n",
      "2024-12-10 17:41:01,231 - cluster_profiler - DEBUG - Filtered clusters size: 199928\n",
      "2024-12-10 17:41:01,627 - cluster_profiler - DEBUG - Parent grouping took 0.3953 seconds\n",
      "2024-12-10 17:41:01,628 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:41:04,288 - cluster_profiler - DEBUG - Component processing for threshold 0.77 took 2.6596 seconds\n",
      "2024-12-10 17:41:04,288 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:41:04,289 - cluster_profiler - DEBUG - Total processing for threshold 0.77 took 3.0675 seconds\n",
      "2024-12-10 17:41:04,290 - cluster_profiler - DEBUG - Processing threshold: 0.76\n",
      "2024-12-10 17:41:04,298 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.76 took 0.0080 seconds\n",
      "2024-12-10 17:41:04,299 - cluster_profiler - DEBUG - Filtered clusters size: 199953\n",
      "2024-12-10 17:41:04,704 - cluster_profiler - DEBUG - Parent grouping took 0.4046 seconds\n",
      "2024-12-10 17:41:04,705 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:41:07,436 - cluster_profiler - DEBUG - Component processing for threshold 0.76 took 2.7303 seconds\n",
      "2024-12-10 17:41:07,436 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:41:07,437 - cluster_profiler - DEBUG - Total processing for threshold 0.76 took 3.1472 seconds\n",
      "2024-12-10 17:41:07,438 - cluster_profiler - DEBUG - Processing threshold: 0.75\n",
      "2024-12-10 17:41:07,447 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.75 took 0.0084 seconds\n",
      "2024-12-10 17:41:07,447 - cluster_profiler - DEBUG - Filtered clusters size: 199971\n",
      "2024-12-10 17:41:07,863 - cluster_profiler - DEBUG - Parent grouping took 0.4146 seconds\n",
      "2024-12-10 17:41:07,863 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:41:10,692 - cluster_profiler - DEBUG - Component processing for threshold 0.75 took 2.8281 seconds\n",
      "2024-12-10 17:41:10,693 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:41:10,693 - cluster_profiler - DEBUG - Total processing for threshold 0.75 took 3.2555 seconds\n",
      "2024-12-10 17:41:10,694 - cluster_profiler - DEBUG - Processing threshold: 0.74\n",
      "2024-12-10 17:41:10,703 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.74 took 0.0084 seconds\n",
      "2024-12-10 17:41:10,704 - cluster_profiler - DEBUG - Filtered clusters size: 199978\n",
      "2024-12-10 17:41:11,124 - cluster_profiler - DEBUG - Parent grouping took 0.4203 seconds\n",
      "2024-12-10 17:41:11,125 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:41:13,957 - cluster_profiler - DEBUG - Component processing for threshold 0.74 took 2.8317 seconds\n",
      "2024-12-10 17:41:13,958 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:41:13,959 - cluster_profiler - DEBUG - Total processing for threshold 0.74 took 3.2648 seconds\n",
      "2024-12-10 17:41:13,959 - cluster_profiler - DEBUG - Processing threshold: 0.73\n",
      "2024-12-10 17:41:13,968 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.73 took 0.0084 seconds\n",
      "2024-12-10 17:41:13,969 - cluster_profiler - DEBUG - Filtered clusters size: 199985\n",
      "2024-12-10 17:41:14,384 - cluster_profiler - DEBUG - Parent grouping took 0.4148 seconds\n",
      "2024-12-10 17:41:14,385 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:41:17,282 - cluster_profiler - DEBUG - Component processing for threshold 0.73 took 2.8965 seconds\n",
      "2024-12-10 17:41:17,283 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:41:17,283 - cluster_profiler - DEBUG - Total processing for threshold 0.73 took 3.3241 seconds\n",
      "2024-12-10 17:41:17,284 - cluster_profiler - DEBUG - Processing threshold: 0.72\n",
      "2024-12-10 17:41:17,293 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.72 took 0.0083 seconds\n",
      "2024-12-10 17:41:17,294 - cluster_profiler - DEBUG - Filtered clusters size: 199988\n",
      "2024-12-10 17:41:17,711 - cluster_profiler - DEBUG - Parent grouping took 0.4163 seconds\n",
      "2024-12-10 17:41:17,712 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:41:21,601 - cluster_profiler - DEBUG - Component processing for threshold 0.72 took 3.8890 seconds\n",
      "2024-12-10 17:41:21,602 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:41:21,603 - cluster_profiler - DEBUG - Total processing for threshold 0.72 took 4.3182 seconds\n",
      "2024-12-10 17:41:21,603 - cluster_profiler - DEBUG - Processing threshold: 0.71\n",
      "2024-12-10 17:41:21,612 - cluster_profiler - DEBUG - Cluster filtering for threshold 0.71 took 0.0081 seconds\n",
      "2024-12-10 17:41:21,613 - cluster_profiler - DEBUG - Filtered clusters size: 199989\n",
      "2024-12-10 17:41:22,005 - cluster_profiler - DEBUG - Parent grouping took 0.3922 seconds\n",
      "2024-12-10 17:41:22,006 - cluster_profiler - DEBUG - Number of parent groups: 1\n",
      "2024-12-10 17:41:24,982 - cluster_profiler - DEBUG - Component processing for threshold 0.71 took 2.9756 seconds\n",
      "2024-12-10 17:41:24,983 - cluster_profiler - DEBUG - Components processed: 1\n",
      "2024-12-10 17:41:24,983 - cluster_profiler - DEBUG - Total processing for threshold 0.71 took 3.3802 seconds\n",
      "2024-12-10 17:41:32,832 - cluster_profiler - DEBUG - Final sorting took 7.8477 seconds\n",
      "2024-12-10 17:41:32,833 - cluster_profiler - DEBUG - Final hierarchy size: 2835379\n",
      "2024-12-10 17:41:32,834 - cluster_profiler - DEBUG - Total function execution took 117.1033 seconds\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger('cluster_profiler')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "hierarchical3 = _cluster_results_to_hierarchical_3(probabilities, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 18:00:07,583 - cluster_profiler - INFO - Starting clustering process\n",
      "2024-12-10 18:00:07,584 - cluster_profiler - DEBUG - Input probabilities table size: 1000000\n",
      "2024-12-10 18:00:07,585 - cluster_profiler - DEBUG - Input clusters table size: 5279385\n",
      "2024-12-10 18:00:14,588 - cluster_profiler - DEBUG - Initialization took 7.0027s\n",
      "2024-12-10 18:00:14,610 - cluster_profiler - DEBUG - Processing threshold: 1.0\n",
      "2024-12-10 18:00:14,616 - cluster_profiler - DEBUG - Filtered clusters size: 30689\n",
      "2024-12-10 18:00:14,762 - cluster_profiler - DEBUG - Component processing took 0.0559s\n",
      "2024-12-10 18:00:14,763 - cluster_profiler - DEBUG - Total threshold processing took 0.1526s\n",
      "2024-12-10 18:00:14,763 - cluster_profiler - DEBUG - Processing threshold: 0.99\n",
      "2024-12-10 18:00:14,770 - cluster_profiler - DEBUG - Filtered clusters size: 63676\n",
      "2024-12-10 18:00:15,229 - cluster_profiler - DEBUG - Component processing took 0.3114s\n",
      "2024-12-10 18:00:15,230 - cluster_profiler - DEBUG - Total threshold processing took 0.4671s\n",
      "2024-12-10 18:00:15,231 - cluster_profiler - DEBUG - Processing threshold: 0.98\n",
      "2024-12-10 18:00:15,238 - cluster_profiler - DEBUG - Filtered clusters size: 83436\n",
      "2024-12-10 18:00:16,054 - cluster_profiler - DEBUG - Component processing took 0.6021s\n",
      "2024-12-10 18:00:16,055 - cluster_profiler - DEBUG - Total threshold processing took 0.8239s\n",
      "2024-12-10 18:00:16,055 - cluster_profiler - DEBUG - Processing threshold: 0.97\n",
      "2024-12-10 18:00:16,063 - cluster_profiler - DEBUG - Filtered clusters size: 108385\n",
      "2024-12-10 18:00:18,307 - cluster_profiler - DEBUG - Component processing took 1.9883s\n",
      "2024-12-10 18:00:18,308 - cluster_profiler - DEBUG - Total threshold processing took 2.2526s\n",
      "2024-12-10 18:00:18,308 - cluster_profiler - DEBUG - Processing threshold: 0.96\n",
      "2024-12-10 18:00:18,316 - cluster_profiler - DEBUG - Filtered clusters size: 134693\n",
      "2024-12-10 18:00:20,308 - cluster_profiler - DEBUG - Component processing took 1.6679s\n",
      "2024-12-10 18:00:20,309 - cluster_profiler - DEBUG - Total threshold processing took 2.0008s\n",
      "2024-12-10 18:00:20,310 - cluster_profiler - DEBUG - Processing threshold: 0.95\n",
      "2024-12-10 18:00:20,319 - cluster_profiler - DEBUG - Filtered clusters size: 155322\n",
      "2024-12-10 18:00:22,754 - cluster_profiler - DEBUG - Component processing took 2.1036s\n",
      "2024-12-10 18:00:22,755 - cluster_profiler - DEBUG - Total threshold processing took 2.4453s\n",
      "2024-12-10 18:00:22,756 - cluster_profiler - DEBUG - Processing threshold: 0.94\n",
      "2024-12-10 18:00:22,765 - cluster_profiler - DEBUG - Filtered clusters size: 169726\n",
      "2024-12-10 18:00:25,688 - cluster_profiler - DEBUG - Component processing took 2.5321s\n",
      "2024-12-10 18:00:25,689 - cluster_profiler - DEBUG - Total threshold processing took 2.9331s\n",
      "2024-12-10 18:00:25,689 - cluster_profiler - DEBUG - Processing threshold: 0.93\n",
      "2024-12-10 18:00:25,699 - cluster_profiler - DEBUG - Filtered clusters size: 179643\n",
      "2024-12-10 18:00:29,017 - cluster_profiler - DEBUG - Component processing took 2.9355s\n",
      "2024-12-10 18:00:29,018 - cluster_profiler - DEBUG - Total threshold processing took 3.3287s\n",
      "2024-12-10 18:00:29,019 - cluster_profiler - DEBUG - Processing threshold: 0.92\n",
      "2024-12-10 18:00:29,027 - cluster_profiler - DEBUG - Filtered clusters size: 186031\n",
      "2024-12-10 18:00:33,439 - cluster_profiler - DEBUG - Component processing took 4.0092s\n",
      "2024-12-10 18:00:33,440 - cluster_profiler - DEBUG - Total threshold processing took 4.4218s\n",
      "2024-12-10 18:00:33,441 - cluster_profiler - DEBUG - Processing threshold: 0.91\n",
      "2024-12-10 18:00:33,451 - cluster_profiler - DEBUG - Filtered clusters size: 190338\n",
      "2024-12-10 18:00:37,490 - cluster_profiler - DEBUG - Component processing took 3.6320s\n",
      "2024-12-10 18:00:37,491 - cluster_profiler - DEBUG - Total threshold processing took 4.0497s\n",
      "2024-12-10 18:00:37,491 - cluster_profiler - DEBUG - Processing threshold: 0.9\n",
      "2024-12-10 18:00:37,501 - cluster_profiler - DEBUG - Filtered clusters size: 193311\n",
      "2024-12-10 18:00:42,073 - cluster_profiler - DEBUG - Component processing took 4.1657s\n",
      "2024-12-10 18:00:42,074 - cluster_profiler - DEBUG - Total threshold processing took 4.5827s\n",
      "2024-12-10 18:00:42,075 - cluster_profiler - DEBUG - Processing threshold: 0.89\n",
      "2024-12-10 18:00:42,085 - cluster_profiler - DEBUG - Filtered clusters size: 195329\n",
      "2024-12-10 18:00:47,612 - cluster_profiler - DEBUG - Component processing took 5.0961s\n",
      "2024-12-10 18:00:47,613 - cluster_profiler - DEBUG - Total threshold processing took 5.5382s\n",
      "2024-12-10 18:00:47,613 - cluster_profiler - DEBUG - Processing threshold: 0.88\n",
      "2024-12-10 18:00:47,623 - cluster_profiler - DEBUG - Filtered clusters size: 196736\n",
      "2024-12-10 18:00:54,665 - cluster_profiler - DEBUG - Component processing took 6.6067s\n",
      "2024-12-10 18:00:54,666 - cluster_profiler - DEBUG - Total threshold processing took 7.0528s\n",
      "2024-12-10 18:00:54,667 - cluster_profiler - DEBUG - Processing threshold: 0.87\n",
      "2024-12-10 18:00:54,679 - cluster_profiler - DEBUG - Filtered clusters size: 197678\n",
      "2024-12-10 18:01:04,804 - cluster_profiler - DEBUG - Component processing took 9.7155s\n",
      "2024-12-10 18:01:04,805 - cluster_profiler - DEBUG - Total threshold processing took 10.1381s\n",
      "2024-12-10 18:01:04,805 - cluster_profiler - DEBUG - Processing threshold: 0.86\n",
      "2024-12-10 18:01:04,815 - cluster_profiler - DEBUG - Filtered clusters size: 198353\n",
      "2024-12-10 18:01:28,609 - cluster_profiler - DEBUG - Component processing took 23.3882s\n",
      "2024-12-10 18:01:28,609 - cluster_profiler - DEBUG - Total threshold processing took 23.8040s\n",
      "2024-12-10 18:01:28,610 - cluster_profiler - DEBUG - Processing threshold: 0.85\n",
      "2024-12-10 18:01:28,620 - cluster_profiler - DEBUG - Filtered clusters size: 198850\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster_profiler\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m logger\u001b[38;5;241m.\u001b[39msetLevel(logging\u001b[38;5;241m.\u001b[39mDEBUG)\n\u001b[0;32m----> 4\u001b[0m hierarchical4 \u001b[38;5;241m=\u001b[39m \u001b[43m_cluster_results_to_hierarchical_4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobabilities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclusters\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[106]\u001b[0m, in \u001b[0;36m_cluster_results_to_hierarchical_4\u001b[0;34m(probabilities, clusters)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger('cluster_profiler')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "hierarchical4 = _cluster_results_to_hierarchical_4(probabilities, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2835379"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hierarchical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2835379"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hierarchical2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2835379"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hierarchical3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hierarchical4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5279385"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "parent: binary\n",
       "child: binary\n",
       "threshold: double\n",
       "----\n",
       "parent: [[6810342717C8589102E8A1F469EDF344BBD353D9D1DFB6B547FAC9561849CEDC,6810342717C8589102E8A1F469EDF344BBD353D9D1DFB6B547FAC9561849CEDC,47EF3BF7E7AD94C6AFF6B4832E5A7AD8C4FA1C3FDB60AA5EEB0AC555AB627833,47EF3BF7E7AD94C6AFF6B4832E5A7AD8C4FA1C3FDB60AA5EEB0AC555AB627833,A79E30871550ABAB8638EBDFF3A9E351E8537EBEDFF1EF2CAD18B5CEF4F4E8B9,...,C255483BAAF15A5475EE5DD67A30566E53673969DC2DC6D6689E0487857AF49E,C255483BAAF15A5475EE5DD67A30566E53673969DC2DC6D6689E0487857AF49E,C255483BAAF15A5475EE5DD67A30566E53673969DC2DC6D6689E0487857AF49E,C255483BAAF15A5475EE5DD67A30566E53673969DC2DC6D6689E0487857AF49E,C255483BAAF15A5475EE5DD67A30566E53673969DC2DC6D6689E0487857AF49E]]\n",
       "child: [[694DCC2AE7D2038E9B3B547A23C398510A2FCB24,BA883EFEED4A93C4739969815E7D23CCC9455202,30C67178B851B0038FC5D5EA72B32032BABD7CBB,774F730EA226553663DF11E721072A6E28873E3F,B4481867BD5583E3C33D83DF4745FC059ECBBB98,...,FFFD0F3371B95C2DC93EBDE701309ECADA1DBE21,FFFDB881D8578BB3325C91AE6646A2E7C95441A7,FFFE18EE7325D9E2A0412CB2F44F33D2E198A40E,FFFE767F38648B640A9BB0A03E245ACB38B738A5,FFFF38A5B12F9C3E4B819C6172F8FAE6194CBC85]]\n",
       "threshold: [[1,1,1,1,1,...,0.71,0.71,0.71,0.71,0.71]]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "\n",
    "def create_test_tables(\n",
    "    size: int,\n",
    "    name: bytes,\n",
    "    id_gen: Generator[str, None, None]\n",
    ") -> tuple[pa.Table, pa.Table]:\n",
    "    \"\"\"\n",
    "    Create two Arrow tables: a standard version and an update version with controlled changes.\n",
    "    The update version maintains some hashes, reassigns some IDs, and adds new entries.\n",
    "    \n",
    "    Args:\n",
    "        size: Number of rows in each table\n",
    "        name: Dataset name as bytes (will be used as prefix for both versions)\n",
    "        id_gen: Generator that yields string IDs\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[pa.Table, pa.Table]: Standard and update tables\n",
    "    \"\"\"\n",
    "    # Generate standard table data\n",
    "    standard_hashes = fast_generate_hashes(size)\n",
    "    standard_ids = [[str(next(id_gen))] for _ in range(size)]\n",
    "    \n",
    "    # Create standard table\n",
    "    standard_ids_nested = pa.array(standard_ids, type=pa.list_(pa.string()))\n",
    "    standard_dataset = pa.array([name] * len(standard_hashes), type=pa.binary())\n",
    "    \n",
    "    standard_table = pa.Table.from_arrays(\n",
    "        [standard_hashes, standard_dataset, standard_ids_nested],\n",
    "        names=['hash', 'dataset', 'id']\n",
    "    )\n",
    "\n",
    "    # Calculate sizes for different types of changes\n",
    "    keep_count = int(size * 0.3)  # 30% remain same\n",
    "    reuse_count = int(size * 0.2)  # 20% same hash, new ID\n",
    "    move_count = int(size * 0.2)   # 20% same ID, new hash\n",
    "    new_count = size - (keep_count + reuse_count + move_count)  # 30% new entries\n",
    "\n",
    "    # Build update version\n",
    "    update_hashes = []\n",
    "    update_ids = []\n",
    "\n",
    "    # 1. Keep some entries exactly the same\n",
    "    for i in range(keep_count):\n",
    "        update_hashes.append(standard_hashes[i])\n",
    "        update_ids.append(standard_ids[i])\n",
    "    \n",
    "    # 2. Same hash, new IDs\n",
    "    for i in range(keep_count, keep_count + reuse_count):\n",
    "        update_hashes.append(standard_hashes[i])\n",
    "        update_ids.append(\n",
    "            [str(next(id_gen))]\n",
    "        )\n",
    "    \n",
    "    # 3. Same IDs, new hashes\n",
    "    new_hashes = fast_generate_hashes(move_count)\n",
    "    for i, hash in enumerate(new_hashes):\n",
    "        idx = keep_count + reuse_count + i\n",
    "        update_hashes.append(hash)\n",
    "        update_ids.append(standard_ids[idx])\n",
    "    \n",
    "    # 4. Completely new entries\n",
    "    final_new_hashes = fast_generate_hashes(new_count)\n",
    "    for hash in final_new_hashes:\n",
    "        update_hashes.append(hash)\n",
    "        update_ids.append(\n",
    "            [str(next(id_gen))]\n",
    "        )\n",
    "\n",
    "    # Create update table\n",
    "    update_ids_nested = pa.array(update_ids, type=pa.list_(pa.string()))\n",
    "    update_dataset = pa.array([name] * len(update_hashes), type=pa.binary())\n",
    "\n",
    "    update_table = pa.Table.from_arrays(\n",
    "        [pa.array(update_hashes), update_dataset, update_ids_nested],\n",
    "        names=['hash', 'dataset', 'id']\n",
    "    )\n",
    "\n",
    "    return standard_table, update_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_consistent_pairs(\n",
    "   standard_left: pa.Array,\n",
    "   standard_right: pa.Array,\n",
    "   update_left: pa.Array, \n",
    "   update_right: pa.Array,\n",
    "   n: int = int(1e8)\n",
    ") -> tuple[pa.Table, pa.Table]:\n",
    "   \"\"\"Generate partially consistent hash pairs between standard and update tables.\"\"\"\n",
    "   # Calculate sizes and adjust n\n",
    "   standard_total = len(standard_left) * len(standard_right)\n",
    "   update_total = len(update_left) * len(update_right)\n",
    "   n = min(n, min(standard_total, update_total))\n",
    "   \n",
    "   # Generate random permutation of just the first n indices instead of all possible indices\n",
    "   standard_flat = np.random.permutation(n)\n",
    "   # Scale to full range \n",
    "   standard_flat = (standard_flat * (standard_total / n)).astype(np.int64)\n",
    "   std_left_idx = standard_flat // len(standard_right)\n",
    "   std_right_idx = standard_flat % len(standard_right)\n",
    "   \n",
    "   # For update table, make ~30% of pairs match standard pairs\n",
    "   match_count = n // 3\n",
    "   update_flat = np.empty(n, dtype=np.int64)\n",
    "   update_flat[:match_count] = standard_flat[:match_count]  # Copy matching pairs\n",
    "   \n",
    "   # Same optimization for the remaining indices\n",
    "   remaining = np.random.permutation(n - match_count)\n",
    "   remaining = (remaining * (update_total / (n - match_count))).astype(np.int64)\n",
    "   update_flat[match_count:] = remaining\n",
    "   \n",
    "   up_left_idx = update_flat // len(update_right)\n",
    "   up_right_idx = update_flat % len(update_right)\n",
    "   \n",
    "   # Generate hashes using original SHA1 approach\n",
    "   standard_hashes = [\n",
    "       sha1(i.to_bytes(8, 'big')).digest() \n",
    "       for i in range(n)\n",
    "   ]\n",
    "   standard_combined = pa.array(standard_hashes, type=pa.binary())\n",
    "   \n",
    "   # For update table, reuse hashes for matching pairs\n",
    "   update_hashes = standard_hashes[:match_count] + [\n",
    "       sha1((i + n).to_bytes(8, 'big')).digest()\n",
    "       for i in range(n - match_count)\n",
    "   ]\n",
    "   update_combined = pa.array(update_hashes, type=pa.binary())\n",
    "   \n",
    "   # Create tables\n",
    "   standard_table = pa.table({\n",
    "       'hash': standard_combined,\n",
    "       'left': standard_left.take(pa.array(std_left_idx)),\n",
    "       'right': standard_right.take(pa.array(std_right_idx)),\n",
    "       'probability': pa.array(np.round(0.7 + 0.3 * np.random.random(n), 1), type=pa.float64())\n",
    "   })\n",
    "\n",
    "   update_table = pa.table({\n",
    "       'hash': update_combined,\n",
    "       'left': update_left.take(pa.array(up_left_idx)),\n",
    "       'right': update_right.take(pa.array(up_right_idx)),\n",
    "       'probability': pa.array(np.round(0.7 + 0.3 * np.random.random(n), 1), type=pa.float64())\n",
    "   })\n",
    "   \n",
    "   return standard_table, update_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster_tables(\n",
    "    hierarchical_results: pa.Table,\n",
    "    model: bytes,\n",
    ") -> tuple[pa.Table, pa.Table, pa.Table]:\n",
    "    \"\"\"\n",
    "    Creates three PyArrow tables from hierarchical clustering results:\n",
    "    - clusters: unique clusters with their metadata\n",
    "    - contains: parent-child relationships between clusters\n",
    "    - probabilities: probability scores for each cluster\n",
    "\n",
    "    Args:\n",
    "        hierarchical_results: PyArrow Table with columns (parent, child, threshold)\n",
    "        model: bytes identifier for the model\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (clusters, contains, probabilities) PyArrow tables where:\n",
    "        - clusters: schema (hash: binary, dataset: binary, id: list[string])\n",
    "        - contains: schema (parent: binary, child: binary)\n",
    "        - probabilities: schema (model: binary, cluster: binary, probability: double)\n",
    "    \"\"\"\n",
    "    # Extract unique clusters (parents) and create clusters table\n",
    "    unique_clusters = pa.compute.unique(hierarchical_results['parent'])\n",
    "    clusters = pa.Table.from_arrays(\n",
    "        [\n",
    "            unique_clusters,  # hash\n",
    "            pa.array([None] * len(unique_clusters), type=pa.binary()),  # dataset\n",
    "            pa.array([[]] * len(unique_clusters), type=pa.list_(pa.string())),  # id as list[str]\n",
    "        ],\n",
    "        names=['hash', 'dataset', 'id']\n",
    "    )\n",
    "\n",
    "    # Contains table is just parent-child relationships\n",
    "    contains = pa.Table.from_arrays(\n",
    "        [\n",
    "            hierarchical_results['parent'],\n",
    "            hierarchical_results['child']\n",
    "        ],\n",
    "        names=['parent', 'child']\n",
    "    )\n",
    "\n",
    "    # Probabilities table with model reference\n",
    "    probabilities = pa.Table.from_arrays(\n",
    "        [\n",
    "            pa.array([model] * len(hierarchical_results)),\n",
    "            hierarchical_results['parent'],\n",
    "            hierarchical_results['threshold']\n",
    "        ],\n",
    "        names=['model', 'cluster', 'probability']\n",
    "    )\n",
    "\n",
    "    return clusters, contains, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "from uuid import UUID, uuid4\n",
    "from typing import Generator\n",
    "\n",
    "def d1gen() -> Generator[UUID, None, None]:\n",
    "    \"\"\"Generate UUIDs for dataset 1.\"\"\"\n",
    "\n",
    "    for i in count():\n",
    "        yield UUID(int=i, version=4)\n",
    "\n",
    "def d2gen() -> Generator[UUID, None, None]:\n",
    "    \"\"\"Generate UUIDs for dataset 2.\"\"\"\n",
    "    for _ in count():\n",
    "        yield uuid4()\n",
    "\n",
    "d1ids = d1gen()\n",
    "d2ids = d2gen()\n",
    "e = 5\n",
    "\n",
    "def num(c: int, e: int) -> int:\n",
    "    return int(c * (10 ** e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyarrow.Table\n",
       " hash: binary\n",
       " dataset: binary\n",
       " id: list<item: string>\n",
       "   child 0, item: string\n",
       " ----\n",
       " hash: [[9C040A5C8480B237889178EF40C1C6FB86952A0A,4ECD8C7B01F6A7D9176F4BD953B7F3727A63F9C8,D5C1123F6593AC43D9269A924A5925217FF4FA27,4988AD6307E29F5A76CF6740B12409FA2AB2A590,E6460B2522372F1BAF36A7A670F24E8FA1EDC7C7,...,3B6E289D196F37E928D7702EECA1F9641B8820F4,F000E0D7C76AE2D49130DF89E78601F86AF4297C,62EE98755E52EAD4D079FD9930C4408926DFFC91,ED431DC11F055B0D2C6D12248D63AC85DC077535,9B0942FCB21E40030C5E1DD04B8957910162581D]]\n",
       " dataset: [[6461746173657431,6461746173657431,6461746173657431,6461746173657431,6461746173657431,...,6461746173657431,6461746173657431,6461746173657431,6461746173657431,6461746173657431]]\n",
       " id: [[[\"00000000-0000-4000-8000-000000000000\"],[\"00000000-0000-4000-8000-000000000001\"],...,[\"00000000-0000-4000-8000-000000030d3e\"],[\"00000000-0000-4000-8000-000000030d3f\"]]],\n",
       " pyarrow.Table\n",
       " hash: binary\n",
       " dataset: binary\n",
       " id: list<item: string>\n",
       "   child 0, item: string\n",
       " ----\n",
       " hash: [[9C040A5C8480B237889178EF40C1C6FB86952A0A,4ECD8C7B01F6A7D9176F4BD953B7F3727A63F9C8,D5C1123F6593AC43D9269A924A5925217FF4FA27,4988AD6307E29F5A76CF6740B12409FA2AB2A590,E6460B2522372F1BAF36A7A670F24E8FA1EDC7C7,...,A9529E3415BB1D02FADD837F3D1686B5A161D62C,E78609A021330E2B8DBD8CC5E2A00FA01C15EF71,6AA1DC59BF1E8BE9F1472A9E177B851CD2567BCB,93362BD5667DDDF3F53EB6722DD793B3D2E5525A,0A2C28F5F2BD850F2F483A724DBAF2FAC3A583FC]]\n",
       " dataset: [[6461746173657431,6461746173657431,6461746173657431,6461746173657431,6461746173657431,...,6461746173657431,6461746173657431,6461746173657431,6461746173657431,6461746173657431]]\n",
       " id: [[[\"00000000-0000-4000-8000-000000000000\"],[\"00000000-0000-4000-8000-000000000001\"],...,[\"00000000-0000-4000-8000-0000000493de\"],[\"00000000-0000-4000-8000-0000000493df\"]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1_standard, dataset1_update = create_test_tables(num(2, e), b'dataset1', d1ids)\n",
    "\n",
    "dataset1_standard, dataset1_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyarrow.Table\n",
       " hash: binary\n",
       " dataset: binary\n",
       " id: list<item: string>\n",
       "   child 0, item: string\n",
       " ----\n",
       " hash: [[E077D790FF20938F8E19C9C4F7E3E59930B0B3E9,0D286E6669557272E16DD209535393B5BD952621,AD3A0E12AEA08792920352D69F15C53856401440,4A93B310390B9E76979F90BF8B644F58C6F711B5,D50B80E464E3155247DA16C08A5D02BCD43D384D,...,67FF447772326F273612122A29EC814CE93C363A,3DB8A929D4E6FB6009FD2FFED78593BCADD7A573,2BFC00B5536C70C34B6BDC1D1B13C83F4E6FEA2A,D677EED50730A9060F4FF65916F39FC5C1056173,3C5717B645A62B029C60DB23F4B83C58673A9AE2]]\n",
       " dataset: [[6461746173657432,6461746173657432,6461746173657432,6461746173657432,6461746173657432,...,6461746173657432,6461746173657432,6461746173657432,6461746173657432,6461746173657432]]\n",
       " id: [[[\"0deb97cf-fc20-43c0-992f-bdeac2198b99\"],[\"88fda3b5-bb31-45d9-892b-8815409a4c51\"],...,[\"44097854-ce09-4e13-870b-a05361e056ac\"],[\"20b8d1ac-9037-4160-9db0-056dec9a8284\"]]],\n",
       " pyarrow.Table\n",
       " hash: binary\n",
       " dataset: binary\n",
       " id: list<item: string>\n",
       "   child 0, item: string\n",
       " ----\n",
       " hash: [[E077D790FF20938F8E19C9C4F7E3E59930B0B3E9,0D286E6669557272E16DD209535393B5BD952621,AD3A0E12AEA08792920352D69F15C53856401440,4A93B310390B9E76979F90BF8B644F58C6F711B5,D50B80E464E3155247DA16C08A5D02BCD43D384D,...,E4E4D6245197838B8FB01A1357F7E3AF2570F5D5,B1A3F426B5739EF6DA1943646CBCE0ED79C5DDA6,F260189A578A8D77BE3016BADD05D3FA0D54475D,4851DC9CA0700857D60D4767148E7EC70CDFCA43,1766AFBB332EBF481ADAE4EA88E1DE8EBF0E01F1]]\n",
       " dataset: [[6461746173657432,6461746173657432,6461746173657432,6461746173657432,6461746173657432,...,6461746173657432,6461746173657432,6461746173657432,6461746173657432,6461746173657432]]\n",
       " id: [[[\"0deb97cf-fc20-43c0-992f-bdeac2198b99\"],[\"88fda3b5-bb31-45d9-892b-8815409a4c51\"],...,[\"cb459671-3c37-4a2d-b703-ca33db07ceb0\"],[\"89e15256-908b-4d85-b593-655f569de5ae\"]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2_standard, dataset2_update = create_test_tables(num(5, e - 1), b'dataset2', d2ids)\n",
    "\n",
    "dataset2_standard, dataset2_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyarrow.Table\n",
       " hash: binary\n",
       " left: binary\n",
       " right: binary\n",
       " probability: double\n",
       " ----\n",
       " hash: [[05FE405753166F125559E7C9AC558654F107C7E9,CB473678976F425D6EC1339838F11011007AD27D,07AAE1B618F604C684EE3189FA1723BEF8656FE4,461D6580E38CCB6DC72699B6C945E53831DCDF03,7F028DDBB42E47AC2CD00E27A37BD191F1C2B925,...,D3425A8AD85C009A7C0207DE64C76CD24A022146,64F3C46ED38031FC857E20E304267CC5E133D292,F5DF0A4361A13D8AF3D6D8382BAE765529C133D7,550E7F8689BFF4E9A8C04DBC7B34684604C8B8C7,A3E65CF5997DDA194FC6B35EF0269CBB2B941EC3]]\n",
       " left: [[E7502302147F2431B1674E20DA763C8B695FE31F,4D39E35913D18FAC2590CB2AF34AC1133CC02955,7801345A966FA7AD2B2A79E76E24E66AF98AD8F0,B352E4FA8F1A268EC829B8E9ACE4B02C091FEBBC,87C87ADAABE3C15067C25BCE6760228F6E589C20,...,12957B7AA1D85ECE6EE44BE54A8E285E7AB6503F,B35A57234925703B657847DDBBEC8C67588F6F65,DB3B912035CCB118E833CB81B630C78AF67347DF,86D488D87535FF833191B66F68C612DA863CBEBC,D909864ED2ADA555190ADAD3C0812764E00D1385]]\n",
       " right: [[2E12C97374A512FC471ABC37F74EE535D21729FD,553D748C69FEB2CF98044626DB10D4E54C57FCCB,553D748C69FEB2CF98044626DB10D4E54C57FCCB,BAA0653CD9A4A0166AF970B589079E94D0B7135A,553D748C69FEB2CF98044626DB10D4E54C57FCCB,...,2E12C97374A512FC471ABC37F74EE535D21729FD,E077D790FF20938F8E19C9C4F7E3E59930B0B3E9,2E12C97374A512FC471ABC37F74EE535D21729FD,E077D790FF20938F8E19C9C4F7E3E59930B0B3E9,2C49834A89A535FAB3299DDC091306DD75AA4684]]\n",
       " probability: [[0.9,0.8,0.9,0.9,0.8,...,0.8,0.9,0.8,0.7,0.8]],\n",
       " pyarrow.Table\n",
       " hash: binary\n",
       " left: binary\n",
       " right: binary\n",
       " probability: double\n",
       " ----\n",
       " hash: [[05FE405753166F125559E7C9AC558654F107C7E9,CB473678976F425D6EC1339838F11011007AD27D,07AAE1B618F604C684EE3189FA1723BEF8656FE4,461D6580E38CCB6DC72699B6C945E53831DCDF03,7F028DDBB42E47AC2CD00E27A37BD191F1C2B925,...,7BDCBA5BE8EC3464391FB1CA26BB25CAA45C357A,6BE0275AE916D1B8D3CD6AE41D7D894EA188A437,B9411381CC4F092FB883B53069979BB6689F87E9,38677C34C39F1B4417EF408BF2AC098E88A7FBD2,604F72CA8A5E0F7BA05EC860DAD81C8261986DE6]]\n",
       " left: [[E7502302147F2431B1674E20DA763C8B695FE31F,0056B06BDFB6318EB066405644FE888C2B1C40E8,51B8E2F6BAA3C0E1A5CF97AA6D863F1FB743E9F0,B352E4FA8F1A268EC829B8E9ACE4B02C091FEBBC,87C87ADAABE3C15067C25BCE6760228F6E589C20,...,48563C7D5E043389AD98556E96BD8F41AA706438,A51C680420E8AE3ED55F6E2A8ACA401B0E7DBB90,9CDBF4C6E54D8508EF388DB132C53BF49D120BBA,E06BACDB074C7D959C89D9FDF609414A26CC76D8,773800CD1F33FA496E36ED33AD8EDFB3539F36F5]]\n",
       " right: [[2E12C97374A512FC471ABC37F74EE535D21729FD,553D748C69FEB2CF98044626DB10D4E54C57FCCB,553D748C69FEB2CF98044626DB10D4E54C57FCCB,9B958BF956A60C62360C574847F91A5EF8D84ED1,553D748C69FEB2CF98044626DB10D4E54C57FCCB,...,F833AA66E715E1D7EDC88B85897026DDB64827C0,AB49DA8BD789EB909A3D09DECF4C8C083CC11BC2,A60EF125CC52C0187A6FFA51EC662A456B63FE52,014256DBCE4F91B07837F23043234EC07ECBB518,8E4D4BBE8FF8074D8E294C468F9AD6BEE479178D]]\n",
       " probability: [[0.9,0.7,0.8,0.8,0.9,...,0.8,0.8,0.7,0.8,0.8]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probresults_standard, probresults_update = fast_consistent_pairs(\n",
    "    dataset1_standard['hash'],\n",
    "    dataset2_standard['hash'],\n",
    "    dataset1_update['hash'],\n",
    "    dataset2_update['hash'],\n",
    "    n=num(1, e + 1)\n",
    ")\n",
    "\n",
    "probresults_standard, probresults_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyarrow.Table\n",
       " parent: binary\n",
       " child: binary\n",
       " threshold: double\n",
       " ----\n",
       " parent: [[EBA0E483A900E754CAE6CBE056D3D501797CEAC5B70917444DCE9410A22AA129,EBA0E483A900E754CAE6CBE056D3D501797CEAC5B70917444DCE9410A22AA129,EBA0E483A900E754CAE6CBE056D3D501797CEAC5B70917444DCE9410A22AA129,EBA0E483A900E754CAE6CBE056D3D501797CEAC5B70917444DCE9410A22AA129,EBA0E483A900E754CAE6CBE056D3D501797CEAC5B70917444DCE9410A22AA129,...,B328820AABFA090CCB74F65113A88E4FE00BA8DA7236A34B508FA2AC0E925EA6,B328820AABFA090CCB74F65113A88E4FE00BA8DA7236A34B508FA2AC0E925EA6,B328820AABFA090CCB74F65113A88E4FE00BA8DA7236A34B508FA2AC0E925EA6,B328820AABFA090CCB74F65113A88E4FE00BA8DA7236A34B508FA2AC0E925EA6,B328820AABFA090CCB74F65113A88E4FE00BA8DA7236A34B508FA2AC0E925EA6]]\n",
       " child: [[00001658DE925885461A13C4E24CD137E744ED1B,0000B8D0F72373DE4280242FC96CA62614A0891B,0000DDAFBCAD207A896B2D393FBA871C8836E397,0001079693B64B8A12DD91940B81695A73DCAF02,00015C7B3ADF292DDBCC64EC077133929A874FAB,...,FFFC4C7E568DD8B54AA71A19AF35E521F823877D,FFFC9368B92E60D59727D1DDFC652E61D48CD255,FFFD0F3371B95C2DC93EBDE701309ECADA1DBE21,FFFE18EE7325D9E2A0412CB2F44F33D2E198A40E,FFFF38A5B12F9C3E4B819C6172F8FAE6194CBC85]]\n",
       " threshold: [[1,1,1,1,1,...,0.7,0.7,0.7,0.7,0.7]],\n",
       " pyarrow.Table\n",
       " parent: binary\n",
       " child: binary\n",
       " threshold: double\n",
       " ----\n",
       " parent: [[3705C753BD9CE6D7717AF016385537978B15A27D04842A0CF05A54DB2B97DD18,3705C753BD9CE6D7717AF016385537978B15A27D04842A0CF05A54DB2B97DD18,9E72681EC082611243C7E4E2AED5076FC5A0DEDCCC8F875172A29F078D028E89,9E72681EC082611243C7E4E2AED5076FC5A0DEDCCC8F875172A29F078D028E89,21679F4C8707CB012CB31EC2844C7355661FE48DECCBC872D2DAE7FB6ED619B5,...,2BD86FC34BB0F8227E2A3DE3E1FE76848FE96BE95D4782C6FE3309847C0E0B95,2BD86FC34BB0F8227E2A3DE3E1FE76848FE96BE95D4782C6FE3309847C0E0B95,2BD86FC34BB0F8227E2A3DE3E1FE76848FE96BE95D4782C6FE3309847C0E0B95,2BD86FC34BB0F8227E2A3DE3E1FE76848FE96BE95D4782C6FE3309847C0E0B95,2BD86FC34BB0F8227E2A3DE3E1FE76848FE96BE95D4782C6FE3309847C0E0B95]]\n",
       " child: [[319377C9F5C450D0245A1513E65AEE6DF591541B,88B08CFAC96D98CFC4677BC12DC285A5B91C179F,65AFB9917EE9C3CFDC0E2DA0D83C503E3F7A560F,92AE206C042107C4CDE7E3E21D98535A83718D86,5949BB83762184160681072C724BF644168B5114,...,FFFA22D7077169DAF45E0293E1B9EDA7E306DDA2,FFFA256703C735F8C44150DA9EC4C5D569F5B9CA,FFFC05696E3415842E614FCF402EB3609A495FCA,FFFE18EE7325D9E2A0412CB2F44F33D2E198A40E,FFFF38A5B12F9C3E4B819C6172F8FAE6194CBC85]]\n",
       " threshold: [[1,1,1,1,1,...,0.7,0.7,0.7,0.7,0.7]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusresults_standard = to_clusters(probresults_standard)\n",
    "clusresults_update = to_clusters(probresults_update)\n",
    "\n",
    "clusresults_standard, clusresults_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyarrow.Table\n",
       " parent: binary\n",
       " child: binary\n",
       " threshold: double\n",
       " ----\n",
       " parent: [[FFFF38A5B12F9C3E4B819C6172F8FAE6194CBC85,FFFF38A5B12F9C3E4B819C6172F8FAE6194CBC85,FFFE82715AFD70E960B991A006B9F0A673119977,FFFE82715AFD70E960B991A006B9F0A673119977,FFFE461EB0EBF8728C2C0D056E9182990F164456,...,0001F46D6A16D92C8DEA684D0F5F234EF1CEC9CB,00011D36B2CE905CF31CAEE334C8DB457BB7422A,00011D36B2CE905CF31CAEE334C8DB457BB7422A,0000DDAFBCAD207A896B2D393FBA871C8836E397,0000DDAFBCAD207A896B2D393FBA871C8836E397]]\n",
       " child: [[BAA0653CD9A4A0166AF970B589079E94D0B7135A,00C3EC91919F27ECF9900CB64D1BEE3381A6C2DA,553D748C69FEB2CF98044626DB10D4E54C57FCCB,2B7BB9AB89ECB35F2836832CE1126F814F0EFABF,553D748C69FEB2CF98044626DB10D4E54C57FCCB,...,9ACA0E6DCA737395F70C2B989CAC7E594B5D74C1,DBD4372FB9EE890ABAA8AAA9719C91A53BEEC133,2C49834A89A535FAB3299DDC091306DD75AA4684,5A75F48BDADD09070013B2DF2B47A47090CCBC1F,2C49834A89A535FAB3299DDC091306DD75AA4684]]\n",
       " threshold: [[1,1,1,1,1,...,0.7,0.7,0.7,0.7,0.7]],\n",
       " pyarrow.Table\n",
       " parent: binary\n",
       " child: binary\n",
       " threshold: double\n",
       " ----\n",
       " parent: [[FFFF1B739DE791F09CD65B906BA7EE03B4C0B2C0,FFFF1B739DE791F09CD65B906BA7EE03B4C0B2C0,FFFE81A8DA567D3416B60C6D3E46C6B0A5660EE6,FFFE81A8DA567D3416B60C6D3E46C6B0A5660EE6,FFFE21D16FC13D47580C930DC3552C1DFBD31E44,...,000168E8778C00DC519C4585A070F2A8A6779DB6,00015C7B3ADF292DDBCC64EC077133929A874FAB,00015C7B3ADF292DDBCC64EC077133929A874FAB,00006CCE61645D540F4BA98F3FA0D5A9DD8CECD4,00006CCE61645D540F4BA98F3FA0D5A9DD8CECD4]]\n",
       " child: [[B52E818668DDE97104CA93BC386452F730F32096,4B353C207D422A29E692897F426CB22440A8440A,AA2F42046CCCF07E552F01792869AB089DE0EB42,88760D433BECB9026620FE2400E08672522BBE16,68D2C3CB3FAD2A4DA9EE06B9C39298669DBBCA64,...,5E7E12641B0AE36E1EBA9DFD86508CCABCC8AFD5,E077D790FF20938F8E19C9C4F7E3E59930B0B3E9,52EF65B02E3FD68D9A67954AFABC6C6E1390E242,EF743A1AC55F0A542646B5E0F5AD0FDE1DFDAE59,18F6868B10D760129C50816A1240E9BFF3C5FC73]]\n",
       " threshold: [[1,1,1,1,1,...,0.7,0.7,0.7,0.7,0.7]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hierarchy_standard = _cluster_results_to_hierarchical_pa(probresults_standard, clusresults_standard)\n",
    "hierarchy_update = _cluster_results_to_hierarchical_pa(probresults_update, clusresults_update)\n",
    "\n",
    "hierarchy_standard, hierarchy_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Set, List, Tuple\n",
    "\n",
    "def _cluster_results_to_hierarchical_pa2(\n",
    "    probabilities: pa.Table,\n",
    "    clusters: pa.Table,\n",
    ") -> pa.Table:\n",
    "    \"\"\"\n",
    "    Converts results to a hierarchical structure by processing thresholds sequentially,\n",
    "    maintaining ultimate parent tracking to avoid graph traversal.\n",
    "    \n",
    "    Args:\n",
    "        probabilities: Original pairwise probabilities containing base components\n",
    "        clusters: Connected components at each threshold\n",
    "        \n",
    "    Returns:\n",
    "        PyArrow Table with schema:\n",
    "            parent: binary\n",
    "            child: binary\n",
    "            threshold: double\n",
    "    \"\"\"\n",
    "    # Sort thresholds in descending order\n",
    "    thresholds = pa.compute.unique(clusters['threshold']).sort(order='descending')\n",
    "    \n",
    "    # Initialize data structures\n",
    "    hierarchy: List[Tuple[bytes, bytes, float]] = []\n",
    "    ultimate_parents: Dict[bytes, Set[bytes]] = defaultdict(set)\n",
    "    \n",
    "    # Process each threshold level\n",
    "    for threshold in thresholds:\n",
    "        threshold_float = float(threshold.as_py())\n",
    "        \n",
    "        # Filter and process pairwise probabilities at this threshold\n",
    "        prob_mask = pa.compute.equal(probabilities['probability'], threshold)\n",
    "        current_probs = probabilities.filter(prob_mask)\n",
    "        \n",
    "        # Add new pairwise relationships at this threshold\n",
    "        for batch in current_probs.to_batches():\n",
    "            parent_array = batch.column('hash')\n",
    "            left_array = batch.column('left')\n",
    "            right_array = batch.column('right')\n",
    "            \n",
    "            for i in range(len(batch)):\n",
    "                parent = parent_array[i].as_py()\n",
    "                left_id = left_array[i].as_py()\n",
    "                right_id = right_array[i].as_py()\n",
    "                \n",
    "                # Add to hierarchy\n",
    "                hierarchy.extend([\n",
    "                    (parent, left_id, threshold_float),\n",
    "                    (parent, right_id, threshold_float)\n",
    "                ])\n",
    "                \n",
    "                # Update ultimate parents\n",
    "                ultimate_parents[left_id].add(parent)\n",
    "                ultimate_parents[right_id].add(parent)\n",
    "        \n",
    "        # Process clusters at this threshold\n",
    "        cluster_mask = pa.compute.equal(clusters['threshold'], threshold)\n",
    "        current_clusters = clusters.filter(cluster_mask)\n",
    "        \n",
    "        # Group by parent to process components together\n",
    "        for batch in current_clusters.to_batches():\n",
    "            parent_col = batch.column('parent')\n",
    "            child_col = batch.column('child')\n",
    "            \n",
    "            parent_groups: Dict[bytes, Set[bytes]] = defaultdict(set)\n",
    "            for i in range(len(batch)):\n",
    "                parent = parent_col[i].as_py()\n",
    "                child = child_col[i].as_py()\n",
    "                parent_groups[parent].add(child)\n",
    "            \n",
    "            # Process each component\n",
    "            for new_parent, children in parent_groups.items():\n",
    "                if len(children) <= 2:\n",
    "                    continue  # Skip pairs already handled by pairwise probabilities\n",
    "                \n",
    "                # Collect all current ultimate parents for children in this component\n",
    "                current_ultimate_parents: Set[bytes] = set()\n",
    "                for child in children:\n",
    "                    current_ultimate_parents.update(ultimate_parents[child])\n",
    "                \n",
    "                # Add edges from ultimate parents to new parent\n",
    "                for up in current_ultimate_parents:\n",
    "                    hierarchy.append((new_parent, up, threshold_float))\n",
    "                \n",
    "                # Update ultimate parents for all children in the component\n",
    "                for child in children:\n",
    "                    ultimate_parents[child] = {new_parent}\n",
    "    \n",
    "    # Sort hierarchy by threshold (descending), then parent, then child\n",
    "    hierarchy.sort(key=lambda x: (x[2], x[0], x[1]), reverse=True)\n",
    "    \n",
    "    # Convert to PyArrow Table\n",
    "    return pa.Table.from_arrays(\n",
    "        [\n",
    "            pa.array([h[0] for h in hierarchy], type=pa.binary()),\n",
    "            pa.array([h[1] for h in hierarchy], type=pa.binary()),\n",
    "            pa.array([h[2] for h in hierarchy], type=pa.float64())\n",
    "        ],\n",
    "        names=['parent', 'child', 'threshold']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hierarchy_standard2 = _cluster_results_to_hierarchical_pa2(probresults_standard, clusresults_standard)\n",
    "\n",
    "hierarchy_standard2.num_rows == hierarchy_standard2.num_rows, hierarchy_standard2 == hierarchy_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hierarchy_standard.equals(hierarchy_standard2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "parent: binary\n",
       "child: binary\n",
       "threshold: double\n",
       "----\n",
       "parent: [[FFFF38A5B12F9C3E4B819C6172F8FAE6194CBC85,FFFF38A5B12F9C3E4B819C6172F8FAE6194CBC85,FFFE82715AFD70E960B991A006B9F0A673119977,FFFE82715AFD70E960B991A006B9F0A673119977,FFFE461EB0EBF8728C2C0D056E9182990F164456,...,0001F46D6A16D92C8DEA684D0F5F234EF1CEC9CB,00011D36B2CE905CF31CAEE334C8DB457BB7422A,00011D36B2CE905CF31CAEE334C8DB457BB7422A,0000DDAFBCAD207A896B2D393FBA871C8836E397,0000DDAFBCAD207A896B2D393FBA871C8836E397]]\n",
       "child: [[BAA0653CD9A4A0166AF970B589079E94D0B7135A,00C3EC91919F27ECF9900CB64D1BEE3381A6C2DA,553D748C69FEB2CF98044626DB10D4E54C57FCCB,2B7BB9AB89ECB35F2836832CE1126F814F0EFABF,553D748C69FEB2CF98044626DB10D4E54C57FCCB,...,9ACA0E6DCA737395F70C2B989CAC7E594B5D74C1,DBD4372FB9EE890ABAA8AAA9719C91A53BEEC133,2C49834A89A535FAB3299DDC091306DD75AA4684,5A75F48BDADD09070013B2DF2B47A47090CCBC1F,2C49834A89A535FAB3299DDC091306DD75AA4684]]\n",
       "threshold: [[1,1,1,1,1,...,0.7,0.7,0.7,0.7,0.7]]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hierarchy_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_pa(t, t2) -> bool:\n",
    "    def sort_table(t):\n",
    "        sort_keys = [(name, 'ascending') for name in sorted(t.column_names, reverse=True)]\n",
    "        sort_indices = pa.compute.sort_indices(t, sort_keys)\n",
    "        sorted_table = pa.compute.take(t, sort_indices)\n",
    "        return sorted_table\n",
    "    t = sort_table(t)\n",
    "    t2 = sort_table(t2)\n",
    "    return t.equals(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_pa(hierarchy_standard, hierarchy_standard2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyarrow.Table\n",
       " parent: binary\n",
       " child: binary\n",
       " threshold: double\n",
       " ----\n",
       " parent: [[0000DDAFBCAD207A896B2D393FBA871C8836E397,0000DDAFBCAD207A896B2D393FBA871C8836E397,00011D36B2CE905CF31CAEE334C8DB457BB7422A,00011D36B2CE905CF31CAEE334C8DB457BB7422A,0001F46D6A16D92C8DEA684D0F5F234EF1CEC9CB,...,FFFE461EB0EBF8728C2C0D056E9182990F164456,FFFE82715AFD70E960B991A006B9F0A673119977,FFFE82715AFD70E960B991A006B9F0A673119977,FFFF38A5B12F9C3E4B819C6172F8FAE6194CBC85,FFFF38A5B12F9C3E4B819C6172F8FAE6194CBC85]]\n",
       " child: [[2C49834A89A535FAB3299DDC091306DD75AA4684,5A75F48BDADD09070013B2DF2B47A47090CCBC1F,2C49834A89A535FAB3299DDC091306DD75AA4684,DBD4372FB9EE890ABAA8AAA9719C91A53BEEC133,9ACA0E6DCA737395F70C2B989CAC7E594B5D74C1,...,553D748C69FEB2CF98044626DB10D4E54C57FCCB,2B7BB9AB89ECB35F2836832CE1126F814F0EFABF,553D748C69FEB2CF98044626DB10D4E54C57FCCB,00C3EC91919F27ECF9900CB64D1BEE3381A6C2DA,BAA0653CD9A4A0166AF970B589079E94D0B7135A]]\n",
       " threshold: [[0.7,0.7,0.7,0.7,0.7,...,1,1,1,1,1]],\n",
       " pyarrow.Table\n",
       " parent: binary\n",
       " child: binary\n",
       " threshold: double\n",
       " ----\n",
       " parent: [[0000DDAFBCAD207A896B2D393FBA871C8836E397,0000DDAFBCAD207A896B2D393FBA871C8836E397,00011D36B2CE905CF31CAEE334C8DB457BB7422A,00011D36B2CE905CF31CAEE334C8DB457BB7422A,0001F46D6A16D92C8DEA684D0F5F234EF1CEC9CB,...,FFFE461EB0EBF8728C2C0D056E9182990F164456,FFFE82715AFD70E960B991A006B9F0A673119977,FFFE82715AFD70E960B991A006B9F0A673119977,FFFF38A5B12F9C3E4B819C6172F8FAE6194CBC85,FFFF38A5B12F9C3E4B819C6172F8FAE6194CBC85]]\n",
       " child: [[2C49834A89A535FAB3299DDC091306DD75AA4684,5A75F48BDADD09070013B2DF2B47A47090CCBC1F,2C49834A89A535FAB3299DDC091306DD75AA4684,DBD4372FB9EE890ABAA8AAA9719C91A53BEEC133,9ACA0E6DCA737395F70C2B989CAC7E594B5D74C1,...,553D748C69FEB2CF98044626DB10D4E54C57FCCB,2B7BB9AB89ECB35F2836832CE1126F814F0EFABF,553D748C69FEB2CF98044626DB10D4E54C57FCCB,00C3EC91919F27ECF9900CB64D1BEE3381A6C2DA,BAA0653CD9A4A0166AF970B589079E94D0B7135A]]\n",
       " threshold: [[0.7,0.7,0.7,0.7,0.7,...,1,1,1,1,1]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h, h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowTypeError",
     "evalue": "Tried executing function with non-array, non-scalar type: Table(parent: binary\nchild: binary\nthreshold: double\n----\nparent:\n  [\n    [\n      FFFF38A5B12F9C3E4B819C6172F8FAE6194CBC85,\n      FFFF38A5B12F9C3E4B819C6172F8FAE6194CBC85,\n      FFFE82715AFD70E960B991A006B9F0A673119977,\n      FFFE82715AFD70E960B991A006B9F0A673119977,\n      FFFE461EB0EBF8728C2C0D056E9182990F164456,\n      FFFE461EB0EBF8728C2C0D056E9182990F164456,\n      FFFE18EE7325D9E2A0412CB2F44F33D2E198A40E,\n      FFFE18EE7325D9E2A0412CB2F44F33D2E198A40E,\n      FFFC1818B7D4261CC5545F7955181CA6DEA91427,\n      FFFC1818B7D4261CC5545F7955181CA6DEA91427,\n      ...\n      000375F8A761CE35991D37ABC5A2B87021F760EA,\n      000375F8A761CE35991D37ABC5A2B87021F760EA,\n      00021C0E82B50CF532ADEC29A95A90D2B0D34675,\n      00021C0E82B50CF532ADEC29A95A90D2B0D34675,\n      0001F46D6A16D92C8DEA684D0F5F234EF1CEC9CB,\n      0001F46D6A16D92C8DEA684D0F5F234EF1CEC9CB,\n      00011D36B2CE905CF31CAEE334C8DB457BB7422A,\n      00011D36B2CE905CF31CAEE334C8DB457BB7422A,\n      0000DDAFBCAD207A896B2D393FBA871C8836E397,\n      0000DDAFBCAD207A896B2D393FBA871C8836E397\n    ]\n  ]\nchild:\n  [\n    [\n      BAA0653CD9A4A0166AF970B589079E94D0B7135A,\n      00C3EC91919F27ECF9900CB64D1BEE3381A6C2DA,\n      553D748C69FEB2CF98044626DB10D4E54C57FCCB,\n      2B7BB9AB89ECB35F2836832CE1126F814F0EFABF,\n      553D748C69FEB2CF98044626DB10D4E54C57FCCB,\n      1A90D556DD301848799A8ACE7251F1C37D6674C9,\n      6A112AB5BEB8CC0738F5A5F1A5EB701DDBD76FA9,\n      2C49834A89A535FAB3299DDC091306DD75AA4684,\n      78D2C836B934592ADCDD72CFAD9F7AB909079B2A,\n      2E12C97374A512FC471ABC37F74EE535D21729FD,\n      ...\n      E077D790FF20938F8E19C9C4F7E3E59930B0B3E9,\n      CF1F82798847C1FE3FEB5BF334A6FF919B3DA822,\n      9624DC02B216D0B25D6763EA30187BC1316456E5,\n      2C49834A89A535FAB3299DDC091306DD75AA4684,\n      E077D790FF20938F8E19C9C4F7E3E59930B0B3E9,\n      9ACA0E6DCA737395F70C2B989CAC7E594B5D74C1,\n      DBD4372FB9EE890ABAA8AAA9719C91A53BEEC133,\n      2C49834A89A535FAB3299DDC091306DD75AA4684,\n      5A75F48BDADD09070013B2DF2B47A47090CCBC1F,\n      2C49834A89A535FAB3299DDC091306DD75AA4684\n    ]\n  ]\nthreshold:\n  [\n    [\n      1,\n      1,\n      1,\n      1,\n      1,\n      1,\n      1,\n      1,\n      1,\n      1,\n      ...\n      0.7,\n      0.7,\n      0.7,\n      0.7,\n      0.7,\n      0.7,\n      0.7,\n      0.7,\n      0.7,\n      0.7\n    ]\n  ]\n)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mequal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhierarchy_standard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhierarchy_standard2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m different_indices \u001b[38;5;241m=\u001b[39m pc\u001b[38;5;241m.\u001b[39minvert(matches)\u001b[38;5;241m.\u001b[39mto_pylist()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRows that differ:\u001b[39m\u001b[38;5;124m\"\u001b[39m, [i \u001b[38;5;28;01mfor\u001b[39;00m i, differs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(different_indices) \u001b[38;5;28;01mif\u001b[39;00m differs])\n",
      "File \u001b[0;32m~/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/compute.py:247\u001b[0m, in \u001b[0;36m_make_generic_wrapper.<locals>.wrapper\u001b[0;34m(memory_pool, *args)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], Expression):\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Expression\u001b[38;5;241m.\u001b[39m_call(func_name, \u001b[38;5;28mlist\u001b[39m(args))\n\u001b[0;32m--> 247\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_pool\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/_compute.pyx:385\u001b[0m, in \u001b[0;36mpyarrow._compute.Function.call\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/DS/matchbox/.venv/lib/python3.11/site-packages/pyarrow/error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowTypeError\u001b[0m: Tried executing function with non-array, non-scalar type: Table(parent: binary\nchild: binary\nthreshold: double\n----\nparent:\n  [\n    [\n      FFFF38A5B12F9C3E4B819C6172F8FAE6194CBC85,\n      FFFF38A5B12F9C3E4B819C6172F8FAE6194CBC85,\n      FFFE82715AFD70E960B991A006B9F0A673119977,\n      FFFE82715AFD70E960B991A006B9F0A673119977,\n      FFFE461EB0EBF8728C2C0D056E9182990F164456,\n      FFFE461EB0EBF8728C2C0D056E9182990F164456,\n      FFFE18EE7325D9E2A0412CB2F44F33D2E198A40E,\n      FFFE18EE7325D9E2A0412CB2F44F33D2E198A40E,\n      FFFC1818B7D4261CC5545F7955181CA6DEA91427,\n      FFFC1818B7D4261CC5545F7955181CA6DEA91427,\n      ...\n      000375F8A761CE35991D37ABC5A2B87021F760EA,\n      000375F8A761CE35991D37ABC5A2B87021F760EA,\n      00021C0E82B50CF532ADEC29A95A90D2B0D34675,\n      00021C0E82B50CF532ADEC29A95A90D2B0D34675,\n      0001F46D6A16D92C8DEA684D0F5F234EF1CEC9CB,\n      0001F46D6A16D92C8DEA684D0F5F234EF1CEC9CB,\n      00011D36B2CE905CF31CAEE334C8DB457BB7422A,\n      00011D36B2CE905CF31CAEE334C8DB457BB7422A,\n      0000DDAFBCAD207A896B2D393FBA871C8836E397,\n      0000DDAFBCAD207A896B2D393FBA871C8836E397\n    ]\n  ]\nchild:\n  [\n    [\n      BAA0653CD9A4A0166AF970B589079E94D0B7135A,\n      00C3EC91919F27ECF9900CB64D1BEE3381A6C2DA,\n      553D748C69FEB2CF98044626DB10D4E54C57FCCB,\n      2B7BB9AB89ECB35F2836832CE1126F814F0EFABF,\n      553D748C69FEB2CF98044626DB10D4E54C57FCCB,\n      1A90D556DD301848799A8ACE7251F1C37D6674C9,\n      6A112AB5BEB8CC0738F5A5F1A5EB701DDBD76FA9,\n      2C49834A89A535FAB3299DDC091306DD75AA4684,\n      78D2C836B934592ADCDD72CFAD9F7AB909079B2A,\n      2E12C97374A512FC471ABC37F74EE535D21729FD,\n      ...\n      E077D790FF20938F8E19C9C4F7E3E59930B0B3E9,\n      CF1F82798847C1FE3FEB5BF334A6FF919B3DA822,\n      9624DC02B216D0B25D6763EA30187BC1316456E5,\n      2C49834A89A535FAB3299DDC091306DD75AA4684,\n      E077D790FF20938F8E19C9C4F7E3E59930B0B3E9,\n      9ACA0E6DCA737395F70C2B989CAC7E594B5D74C1,\n      DBD4372FB9EE890ABAA8AAA9719C91A53BEEC133,\n      2C49834A89A535FAB3299DDC091306DD75AA4684,\n      5A75F48BDADD09070013B2DF2B47A47090CCBC1F,\n      2C49834A89A535FAB3299DDC091306DD75AA4684\n    ]\n  ]\nthreshold:\n  [\n    [\n      1,\n      1,\n      1,\n      1,\n      1,\n      1,\n      1,\n      1,\n      1,\n      1,\n      ...\n      0.7,\n      0.7,\n      0.7,\n      0.7,\n      0.7,\n      0.7,\n      0.7,\n      0.7,\n      0.7,\n      0.7\n    ]\n  ]\n)"
     ]
    }
   ],
   "source": [
    "matches = pa.compute.equal(hierarchy_standard, hierarchy_standard2)\n",
    "different_indices = pc.invert(matches).to_pylist()\n",
    "print(\"Rows that differ:\", [i for i, differs in enumerate(different_indices) if differs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy_standard = _cluster_results_to_hierarchical_pa2(probresults_standard, clusresults_standard)\n",
    "hierarchy_update = _cluster_results_to_hierarchical_pa2(probresults_update, clusresults_update)\n",
    "\n",
    "hierarchy_standard, hierarchy_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyarrow.Table\n",
       " hash: binary\n",
       " dataset: binary\n",
       " id: list<item: string>\n",
       "   child 0, item: string\n",
       " ----\n",
       " hash: [[FFFF38A5B12F9C3E4B819C6172F8FAE6194CBC85,FFFE82715AFD70E960B991A006B9F0A673119977,FFFE461EB0EBF8728C2C0D056E9182990F164456,FFFE18EE7325D9E2A0412CB2F44F33D2E198A40E,FFFC1818B7D4261CC5545F7955181CA6DEA91427,...,000375F8A761CE35991D37ABC5A2B87021F760EA,00021C0E82B50CF532ADEC29A95A90D2B0D34675,0001F46D6A16D92C8DEA684D0F5F234EF1CEC9CB,00011D36B2CE905CF31CAEE334C8DB457BB7422A,0000DDAFBCAD207A896B2D393FBA871C8836E397]]\n",
       " dataset: [[null,null,null,null,null,...,null,null,null,null,null]]\n",
       " id: [[[],[],...,[],[]]],\n",
       " pyarrow.Table\n",
       " parent: binary\n",
       " child: binary\n",
       " ----\n",
       " parent: [[FFFF38A5B12F9C3E4B819C6172F8FAE6194CBC85,FFFF38A5B12F9C3E4B819C6172F8FAE6194CBC85,FFFE82715AFD70E960B991A006B9F0A673119977,FFFE82715AFD70E960B991A006B9F0A673119977,FFFE461EB0EBF8728C2C0D056E9182990F164456,...,0001F46D6A16D92C8DEA684D0F5F234EF1CEC9CB,00011D36B2CE905CF31CAEE334C8DB457BB7422A,00011D36B2CE905CF31CAEE334C8DB457BB7422A,0000DDAFBCAD207A896B2D393FBA871C8836E397,0000DDAFBCAD207A896B2D393FBA871C8836E397]]\n",
       " child: [[BAA0653CD9A4A0166AF970B589079E94D0B7135A,00C3EC91919F27ECF9900CB64D1BEE3381A6C2DA,553D748C69FEB2CF98044626DB10D4E54C57FCCB,2B7BB9AB89ECB35F2836832CE1126F814F0EFABF,553D748C69FEB2CF98044626DB10D4E54C57FCCB,...,9ACA0E6DCA737395F70C2B989CAC7E594B5D74C1,DBD4372FB9EE890ABAA8AAA9719C91A53BEEC133,2C49834A89A535FAB3299DDC091306DD75AA4684,5A75F48BDADD09070013B2DF2B47A47090CCBC1F,2C49834A89A535FAB3299DDC091306DD75AA4684]],\n",
       " pyarrow.Table\n",
       " model: binary\n",
       " cluster: binary\n",
       " probability: double\n",
       " ----\n",
       " model: [[6D6F64656C31,6D6F64656C31,6D6F64656C31,6D6F64656C31,6D6F64656C31,...,6D6F64656C31,6D6F64656C31,6D6F64656C31,6D6F64656C31,6D6F64656C31]]\n",
       " cluster: [[FFFF38A5B12F9C3E4B819C6172F8FAE6194CBC85,FFFF38A5B12F9C3E4B819C6172F8FAE6194CBC85,FFFE82715AFD70E960B991A006B9F0A673119977,FFFE82715AFD70E960B991A006B9F0A673119977,FFFE461EB0EBF8728C2C0D056E9182990F164456,...,0001F46D6A16D92C8DEA684D0F5F234EF1CEC9CB,00011D36B2CE905CF31CAEE334C8DB457BB7422A,00011D36B2CE905CF31CAEE334C8DB457BB7422A,0000DDAFBCAD207A896B2D393FBA871C8836E397,0000DDAFBCAD207A896B2D393FBA871C8836E397]]\n",
       " probability: [[1,1,1,1,1,...,0.7,0.7,0.7,0.7,0.7]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters_standard, contains_standard, probabilities_standard = create_cluster_tables(hierarchy_standard, b'model1')\n",
    "\n",
    "clusters_standard, contains_standard, probabilities_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyarrow.Table\n",
       " hash: binary\n",
       " dataset: binary\n",
       " id: list<item: string>\n",
       "   child 0, item: string\n",
       " ----\n",
       " hash: [[FFFF1B739DE791F09CD65B906BA7EE03B4C0B2C0,FFFE81A8DA567D3416B60C6D3E46C6B0A5660EE6,FFFE21D16FC13D47580C930DC3552C1DFBD31E44,FFFCA010C9717AE8C2EE953FFA6CD759140F503A,FFFC4C7E568DD8B54AA71A19AF35E521F823877D,...,0001A5266A19D350D892CA619A04A8BE69DCCFC7,00019995A21C0B063A8531860254C2F9A8FDB9D0,000168E8778C00DC519C4585A070F2A8A6779DB6,00015C7B3ADF292DDBCC64EC077133929A874FAB,00006CCE61645D540F4BA98F3FA0D5A9DD8CECD4]]\n",
       " dataset: [[null,null,null,null,null,...,null,null,null,null,null]]\n",
       " id: [[[],[],...,[],[]]],\n",
       " pyarrow.Table\n",
       " parent: binary\n",
       " child: binary\n",
       " ----\n",
       " parent: [[FFFF1B739DE791F09CD65B906BA7EE03B4C0B2C0,FFFF1B739DE791F09CD65B906BA7EE03B4C0B2C0,FFFE81A8DA567D3416B60C6D3E46C6B0A5660EE6,FFFE81A8DA567D3416B60C6D3E46C6B0A5660EE6,FFFE21D16FC13D47580C930DC3552C1DFBD31E44,...,000168E8778C00DC519C4585A070F2A8A6779DB6,00015C7B3ADF292DDBCC64EC077133929A874FAB,00015C7B3ADF292DDBCC64EC077133929A874FAB,00006CCE61645D540F4BA98F3FA0D5A9DD8CECD4,00006CCE61645D540F4BA98F3FA0D5A9DD8CECD4]]\n",
       " child: [[B52E818668DDE97104CA93BC386452F730F32096,4B353C207D422A29E692897F426CB22440A8440A,AA2F42046CCCF07E552F01792869AB089DE0EB42,88760D433BECB9026620FE2400E08672522BBE16,68D2C3CB3FAD2A4DA9EE06B9C39298669DBBCA64,...,5E7E12641B0AE36E1EBA9DFD86508CCABCC8AFD5,E077D790FF20938F8E19C9C4F7E3E59930B0B3E9,52EF65B02E3FD68D9A67954AFABC6C6E1390E242,EF743A1AC55F0A542646B5E0F5AD0FDE1DFDAE59,18F6868B10D760129C50816A1240E9BFF3C5FC73]],\n",
       " pyarrow.Table\n",
       " model: binary\n",
       " cluster: binary\n",
       " probability: double\n",
       " ----\n",
       " model: [[6D6F64656C31,6D6F64656C31,6D6F64656C31,6D6F64656C31,6D6F64656C31,...,6D6F64656C31,6D6F64656C31,6D6F64656C31,6D6F64656C31,6D6F64656C31]]\n",
       " cluster: [[FFFF1B739DE791F09CD65B906BA7EE03B4C0B2C0,FFFF1B739DE791F09CD65B906BA7EE03B4C0B2C0,FFFE81A8DA567D3416B60C6D3E46C6B0A5660EE6,FFFE81A8DA567D3416B60C6D3E46C6B0A5660EE6,FFFE21D16FC13D47580C930DC3552C1DFBD31E44,...,000168E8778C00DC519C4585A070F2A8A6779DB6,00015C7B3ADF292DDBCC64EC077133929A874FAB,00015C7B3ADF292DDBCC64EC077133929A874FAB,00006CCE61645D540F4BA98F3FA0D5A9DD8CECD4,00006CCE61645D540F4BA98F3FA0D5A9DD8CECD4]]\n",
       " probability: [[1,1,1,1,1,...,0.7,0.7,0.7,0.7,0.7]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters_update, contains_update, probabilities_update = create_cluster_tables(hierarchy_update, b'model1')\n",
    "\n",
    "clusters_update, contains_update, probabilities_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've therefore decided to produce three sets of parquets: small, medium and large. I figured this would be useful for you too as you can use small to ensure your plumbing is all set up right before you scale to medium and large.\n",
    " \n",
    "Each set of files includes:\n",
    "\n",
    "* `clusters_dataset1_standard_{size}`\n",
    "* `clusters_dataset1_update_{size}`\n",
    "* `clusters_dataset2_standard_{size}`\n",
    "* `clusters_dataset2_update_{size}`\n",
    "* `clusters_clusters_standard_{size}`\n",
    "* `clusters_clusters_update_{size}`\n",
    "* `contains_standard_{size}`\n",
    "* `contains_update_{size}`\n",
    "* `probabilities_standard_{size}`\n",
    "* `probabilities_update_{size}`\n",
    "\n",
    "The first word of these names is the table to insert into. Final word is whether it's the initial or updated insert. The shape of them should be exactly right for the PostgreSQL table with column names matching the ORM, and datatypes in the paired Arrow format, so `BYTEA == pa.binary`, `ARRAY<VARCHAR> == pa.list_(pa.string)`, `FLOAT == pa.double`. I've used snappy compression to match the post-processed parquet format we're likely to use, rather than the hyper-compressed format we upload.\n",
    " \n",
    "Because the `Clusters` table contains both data and clusters, and these files describe a linking process, there's the inserts for each dataset, then the inserts of the clusters that match those datasets.\n",
    " \n",
    "To achieve proportional scaling with the same functions, my counts are in scientific notation where I've parameterised the exponent. In small it's 5, in medium it's 6, and in large it's 7, which I believe gets us to the 100m probabilities scale for the probabilities table (1 ** 10 ^ e + 1 == 1e8 == 100m).\n",
    " \n",
    "Dataset1 is analogous to HMRC exports: there's 2 ** 10 ^ e rows, or 20m in large.\n",
    " \n",
    "Dataset2 is analogous to Companies House: there's 5 ** 10 ^ e - 1 rows, or 5m in large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "\n",
    "file_path = Path.cwd()\n",
    "\n",
    "size = \"med\"\n",
    "dir_path = Path.cwd() / size\n",
    "\n",
    "pq.write_table(dataset1_standard, dir_path / f'clusters_dataset1_standard_{size}.parquet')\n",
    "pq.write_table(dataset1_update, dir_path / f'clusters_dataset1_update_{size}.parquet')\n",
    "pq.write_table(dataset2_standard, dir_path / f'clusters_dataset2_standard_{size}.parquet')\n",
    "pq.write_table(dataset2_update, dir_path / f'clusters_dataset2_update_{size}.parquet')\n",
    "pq.write_table(clusters_standard, dir_path / f'clusters_clusters_standard_{size}.parquet')\n",
    "pq.write_table(clusters_update, dir_path / f'clusters_clusters_update_{size}.parquet')\n",
    "pq.write_table(contains_standard, dir_path / f'contains_standard_{size}.parquet')\n",
    "pq.write_table(contains_update, dir_path / f'contains_update_{size}.parquet')\n",
    "pq.write_table(probabilities_standard, dir_path / f'probabilities_standard_{size}.parquet')\n",
    "pq.write_table(probabilities_update, dir_path / f'probabilities_update_{size}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(hash: binary\n",
       " left: binary\n",
       " right: binary\n",
       " probability: double,\n",
       " '7653.24MB')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_table.schema, f\"{arrow_table.nbytes / (1024 * 1024):.2f}MB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "\n",
    "class HashIndex:\n",
    "    def __init__(self, hash_type: str = 'sha1', hashes: list[bytes] | pa.Array | None = None):\n",
    "        \"\"\"Create a new HashIndex instance.\n",
    "        \n",
    "        Args:\n",
    "            hash_type: Hash algorithm to use (default: 'sha1')\n",
    "            hashes (Optional): Initial list of hashes to insert (default: None)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            hash_obj = hashlib.new(hash_type)\n",
    "            self._hash_size: int = hash_obj.digest_size   # Return the digest size in bytes\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f'Unsupported hash type: {hash_type}') from e\n",
    "        \n",
    "        # Initialize empty table with correct schema\n",
    "        self.schema = pa.schema([\n",
    "            ('hash', pa.binary(self._hash_size)),\n",
    "            ('id', pa.int32())\n",
    "        ])\n",
    "        self.table = pa.Table.from_pydict({\n",
    "            'hash': [],\n",
    "            'id': []\n",
    "        }, schema=self.schema)\n",
    "        self.next_id: int = 0\n",
    "\n",
    "        if hashes is not None:\n",
    "            self.insert_hashes(hashes)\n",
    "\n",
    "    def __eq__(self, other: 'HashIndex') -> bool:\n",
    "        \"\"\"\n",
    "        Compare this HashIndex with another for equality.\n",
    "        \n",
    "        Two HashIndex instances are considered equal if they:\n",
    "            1. Have the same hash size\n",
    "            2. Have the same next_id\n",
    "            3. Have equal tables (same schema and data)\n",
    "        \n",
    "        Args:\n",
    "            other: Another HashIndex instance to compare with\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if the indexes are equal, False otherwise\n",
    "        \"\"\"\n",
    "        if not isinstance(other, HashIndex):\n",
    "            return False\n",
    "            \n",
    "        return (\n",
    "            self._hash_size == other._hash_size and\n",
    "            self.next_id == other.next_id and\n",
    "            self.table.equals(other.table)\n",
    "        )\n",
    "\n",
    "    def _list_to_array(self, hashes: list[bytes] | pa.Array) -> pa.Array:\n",
    "        if isinstance(hashes, list):\n",
    "            return pa.array(hashes, type=pa.binary(self._hash_size))\n",
    "        return hashes\n",
    "\n",
    "    def insert_hashes(self, hashes: list[bytes] | pa.Array) -> pa.Array:\n",
    "        \"\"\"\n",
    "        Insert new hashes and return their indices. For existing hashes, returns their\n",
    "        current indices. For new hashes, assigns and returns new indices.\n",
    "        \n",
    "        Args:\n",
    "            hashes: Array of SHA-1 hashes to insert\n",
    "        Returns:\n",
    "            Array of indices (both existing and newly assigned)\n",
    "        \"\"\"\n",
    "        hashes = self._list_to_array(hashes)\n",
    "        \n",
    "        # If table is empty, fast path to create initial table\n",
    "        if len(self.table) == 0:\n",
    "            ids = pa.array(range(len(hashes)), type=pa.int32())\n",
    "            self.table = pa.Table.from_arrays([hashes, ids], schema=self.schema)\n",
    "            self.next_id = len(hashes)\n",
    "            # Sort table by hash for future binary searches\n",
    "            self.table = self.table.sort_by('hash')\n",
    "            return ids\n",
    "\n",
    "        # Find existing hashes using binary search\n",
    "        indices = pc.index_in(hashes, self.table['hash'])\n",
    "        is_new = pc.is_null(indices)\n",
    "        new_count = pc.sum(pc.cast(is_new, pa.int32())).as_py()\n",
    "        \n",
    "        if new_count > 0:\n",
    "            # Get the new hashes\n",
    "            new_hashes = pc.filter(hashes, is_new)\n",
    "            \n",
    "            # Pre-allocate new IDs array\n",
    "            new_ids = pa.array(\n",
    "                range(self.next_id, self.next_id + new_count), \n",
    "                type=pa.int32()\n",
    "            )\n",
    "            self.next_id += new_count\n",
    "            \n",
    "            # Append in one operation and sort once\n",
    "            new_table = pa.Table.from_arrays([new_hashes, new_ids], schema=self.schema)\n",
    "            self.table = pa.concat_tables([self.table, new_table])\n",
    "            self.table = self.table.sort_by('hash')\n",
    "            \n",
    "            # Final lookup to get all IDs in correct order\n",
    "            indices = pc.index_in(hashes, self.table['hash'])\n",
    "        \n",
    "        return pc.take(self.table['id'], indices)\n",
    "\n",
    "    def get_hashes(self, ids: list[int] | pa.Array) -> pa.Array:\n",
    "        \"\"\"\n",
    "        Look up hashes by their IDs\n",
    "        \n",
    "        Args:\n",
    "            ids: Array of IDs to look up\n",
    "        Returns:\n",
    "            Array of corresponding hashes (null for unknown indices)\n",
    "        \"\"\"\n",
    "        if isinstance(ids, list):\n",
    "            ids = pa.array(ids, type=pa.int32())\n",
    "        \n",
    "        positions = pc.index_in(ids, self.table['id'])\n",
    "        return pc.take(self.table['hash'], positions)\n",
    "\n",
    "    def get_indices(self, hashes: list[bytes] | pa.Array) -> pa.Array:\n",
    "        \"\"\"\n",
    "        Look up IDs for existing hashes. Returns null for unknown hashes.\n",
    "        \n",
    "        Args:\n",
    "            hashes: Array of hashes to look up\n",
    "        Returns:\n",
    "            Array of corresponding IDs (null for unknown hashes)\n",
    "        \"\"\"\n",
    "        hashes = self._list_to_array(hashes)\n",
    "            \n",
    "        indices = pc.index_in(hashes, self.table['hash'])\n",
    "        return pc.take(self.table['id'], indices)\n",
    "    \n",
    "\n",
    "    def to_parquet(self, path: str | Path, compression: str = 'zstd') -> None:\n",
    "        \"\"\"\n",
    "        Save the HashIndex to a Parquet file.\n",
    "        \n",
    "        Args:\n",
    "            path: Path to save the Parquet file\n",
    "            compression: Compression algorithm to use (default: 'zstd')\n",
    "                Options include: 'none', 'snappy', 'gzip', 'brotli', 'lz4', 'zstd'\n",
    "        \n",
    "        Raises:\n",
    "            IOError: If the file cannot be written\n",
    "            ValueError: If the compression algorithm is not supported\n",
    "        \"\"\"\n",
    "        path = Path(path)\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        metadata = {\n",
    "            b'next_id': str(self.next_id).encode(),\n",
    "            b'hash_size': str(self._hash_size).encode()\n",
    "        }\n",
    "\n",
    "        existing_metadata = self.table.schema.metadata or {}\n",
    "        merged_metadata = {**existing_metadata, **metadata}\n",
    "        \n",
    "        try:\n",
    "            pq.write_table(\n",
    "                self.table.replace_schema_metadata(merged_metadata),\n",
    "                path,\n",
    "                compression=compression,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise IOError(f\"Failed to write Parquet file: {e}\") from e\n",
    "\n",
    "    @classmethod\n",
    "    def from_parquet(cls, path: str | Path) -> 'HashIndex':\n",
    "        \"\"\"\n",
    "        Load a HashIndex from a Parquet file.\n",
    "        \n",
    "        Args:\n",
    "            path: Path to the Parquet file\n",
    "        \n",
    "        Returns:\n",
    "            HashIndex: New HashIndex instance loaded from the file\n",
    "        \n",
    "        Raises:\n",
    "            IOError: If the file cannot be read or is invalid\n",
    "            ValueError: If the file format is invalid\n",
    "        \"\"\"\n",
    "        path = Path(path)\n",
    "        \n",
    "        try:\n",
    "            table = pq.read_table(path)\n",
    "            metadata = table.schema.metadata\n",
    "            \n",
    "            if not metadata or b'next_id' not in metadata or b'hash_size' not in metadata:\n",
    "                raise ValueError(\"Invalid Parquet file: missing required metadata\")\n",
    "            \n",
    "            # Create new instance\n",
    "            instance = cls.__new__(cls)\n",
    "            \n",
    "            instance._hash_size = int(metadata[b'hash_size'].decode())\n",
    "            instance.next_id = int(metadata[b'next_id'].decode())\n",
    "            \n",
    "            instance.schema = table.schema\n",
    "            instance.table = table\n",
    "            \n",
    "            return instance\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise IOError(f\"Failed to load Parquet file: {e}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<pyarrow.lib.BinaryArray object at 0x118363fa0>\n",
       " [\n",
       "   7BDB9431F1E8DDBF1ACCF691D4B8661CF9B43A25,\n",
       "   392BCA5B7AFC18C6727FC30EC57B718DDAAFF953,\n",
       "   F884626B7A54AEFFECB3CA3DDB1493613CC0C7A9,\n",
       "   C94022C8F551879B31D10F99BCC9A3561BAE7612,\n",
       "   08458C06AD784FCDC7809825056654AD5F119C4C\n",
       " ],\n",
       " <pyarrow.lib.BinaryArray object at 0x11f06ada0>\n",
       " [\n",
       "   31E7C33F498E4B7E95B1C52ECACAF348CEFFAE71,\n",
       "   CC141AB85B40C152864B1A24689072D718332BDD,\n",
       "   C769376877CFA78C46210E9B91D4446E82A6D7CE,\n",
       "   9263614A6B6C53967ABA62FA2859F785C58B1016,\n",
       "   15B8ABB98A8ED7D0F008C1234583DCE9DC34AFFF\n",
       " ])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hashes = fast_generate_hashes(int(2e7))\n",
    "test_hashes2 = pa.concat_arrays([fast_generate_hashes(int(2e7) / 2), test_hashes[:int(2e7 / 2)]])\n",
    "hidx_5 = HashIndex(hash_type='sha1', hashes=test_hashes)\n",
    "test_hashes[:5], test_hashes2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pa.array(random.sample(range(len(test_hashes2)), k=len(test_hashes2)))\n",
    "test_hashes3 = test_hashes2.take(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.ChunkedArray object at 0x13dc3c8b0>\n",
       "[\n",
       "  [\n",
       "    8997172,\n",
       "    6063643,\n",
       "    6260234,\n",
       "    3255430,\n",
       "    7485144,\n",
       "    ...\n",
       "    9917674,\n",
       "    5612100,\n",
       "    2723141,\n",
       "    201414,\n",
       "    4680984\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidx_5.insert_hashes(test_hashes3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<pyarrow.lib.BinaryArray object at 0x1187140a0>\n",
       " [\n",
       "   1F21ABB3185DA8D5340A8298C6CBC9C21F8AF9CD,\n",
       "   813583A225BA22DE65149EC502FA79A082F93D8C,\n",
       "   C4587910358DC70CD81B6B106FD724685D8AE971,\n",
       "   79924525EAF98B48E03E00213E0FFBFD17AD8778,\n",
       "   5E668D4D878659EE35DC8F3E7EFE5B19D68890FA\n",
       " ],\n",
       " <pyarrow.lib.BinaryArray object at 0x11ec591e0>\n",
       " [\n",
       "   BDBD93052D2290495857C2A46936C468CA4A7FD6,\n",
       "   AC0FFE1E2F41A6D12D5F56E32A00F8B48D47EF66,\n",
       "   A3DD64FC8F69428EA0693218AF31FEA22F8BCDDE,\n",
       "   0C7FF3A1FB8441369A80059DC2FA08F678A05D86,\n",
       "   BD939DD6F750A40D0C3CFCB3F1C3B202A8BEADE2\n",
       " ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hashes = fast_generate_hashes(40)\n",
    "test_hashes2 = fast_generate_hashes(40)\n",
    "hidx = HashIndex(hash_type='sha1')\n",
    "test_hashes[:5], test_hashes2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path.cwd()\n",
    "\n",
    "hidx3 = HashIndex(hash_type='sha1', hashes=fast_generate_hashes(int(2e7)))\n",
    "\n",
    "hidx3.to_parquet(file_path / 'hash_index.parquet')\n",
    "\n",
    "# del hidx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidx3 = HashIndex.from_parquet(file_path / 'hash_index.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.ChunkedArray object at 0x11ec7e6b0>\n",
       "[\n",
       "  [\n",
       "    3F0B311BB3EF9EAEB211378D538A67491E43429F,\n",
       "    1234FE9CB4C1413559FD09FBBAF512DD26DF7CF6,\n",
       "    C163CF811B2021CE8D17AFE5720F9DED2D602543,\n",
       "    D151C016C7C94022B0E8A7826EE698A0B5FB947E,\n",
       "    30EE3D4D4A6497F0C1A85158F895398C9E901F63\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidx3.get_hashes([random.randint(0, int(2e7)) for _ in range(int(2e5))])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<pyarrow.lib.Int32Array object at 0x11ecc1f60>\n",
       " [\n",
       "   0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4\n",
       " ],\n",
       " <pyarrow.lib.ChunkedArray object at 0x11ef1f600>\n",
       " [\n",
       "   [\n",
       "     40,\n",
       "     41,\n",
       "     42,\n",
       "     43,\n",
       "     44\n",
       "   ]\n",
       " ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidx.insert_hashes(test_hashes)[:5], hidx.insert_hashes(test_hashes2)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = hidx.get_hashes([0])[0]\n",
    "i = hidx.get_indices([h])[0]\n",
    "h2 = hidx.get_hashes([i])[0]\n",
    "\n",
    "h == h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = Path.cwd()\n",
    "\n",
    "hidx.to_parquet(file_path / 'hash_index.parquet')\n",
    "hidx2 = HashIndex.from_parquet(file_path / 'hash_index.parquet')\n",
    "\n",
    "hidx == hidx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.ChunkedArray object at 0x15a223d80>\n",
       "[\n",
       "  [\n",
       "    9E24FC1BC65D8A134990D02A6C4A40E3AAB20FAA,\n",
       "    09C61C3BE151A112228E61AC7C7C1B81DC625CDF,\n",
       "    CEED556D7FE741F9BC0FA5A464DEF839989F3E74,\n",
       "    76506960448B8BB6CB2AF337FBE9852E1ABB1486,\n",
       "    null\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidx.get_hashes([0, 40, 7, 42, 190])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<pyarrow.lib.ChunkedArray object at 0x120b61e90>\n",
       " [\n",
       "   [\n",
       "     0,\n",
       "     1,\n",
       "     2,\n",
       "     3,\n",
       "     4\n",
       "   ]\n",
       " ],\n",
       " <pyarrow.lib.ChunkedArray object at 0x15a2d1800>\n",
       " [\n",
       "   [\n",
       "     40,\n",
       "     41,\n",
       "     42,\n",
       "     43,\n",
       "     44\n",
       "   ]\n",
       " ])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidx.get_indices(test_hashes[:5]), hidx.get_indices(test_hashes2[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashes = fast_generate_hashes(int(2e7))\n",
    "t1 = fast_sample_pairs(hashes, int(1e8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('7653.24MB', '4135.36MB', '1907.35MB', '6042.71MB')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def index_probability_table(table: pa.Table) -> tuple[HashIndex, pa.Table]:\n",
    "\n",
    "    hidx = HashIndex(hash_type='sha1')\n",
    "\n",
    "    table = table.set_column(\n",
    "        table.column_names.index(\"left\"),\n",
    "        \"left\",\n",
    "        hidx.insert_hashes(table['left'])\n",
    "    )\n",
    "    table = table.set_column(\n",
    "        table.column_names.index(\"right\"),\n",
    "        \"right\",\n",
    "        hidx.insert_hashes(table['right'])\n",
    "    )\n",
    "    table = table.set_column(\n",
    "        table.column_names.index(\"hash\"),\n",
    "        \"hash\",\n",
    "        hidx.insert_hashes(table['hash'])\n",
    "    )\n",
    "\n",
    "    return hidx, table\n",
    "\n",
    "hidx, t2 = index_probability_table(t1)\n",
    "\n",
    "f\"{t1.nbytes / (1024 * 1024):.2f}MB\", f\"{hidx.table.nbytes / (1024 * 1024):.2f}MB\", f\"{t2.nbytes / (1024 * 1024):.2f}MB\", f\"{(hidx.table.nbytes + t2.nbytes) / (1024 * 1024):.2f}MB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.75"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(15 * 15) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.1"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(4230 / 5) / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5mb/s upload speed -- 25 mins for 7.6Gb, 14min for 4.3Gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "\n",
    "file_path = Path.cwd()\n",
    "\n",
    "pq.write_table(\n",
    "    t2,\n",
    "    file_path / 'probabilities_normalised_brot.parquet',\n",
    "    compression='BROTLI',\n",
    "    # compression_level=16,\n",
    "    # use_dictionary=True,\n",
    "    # write_statistics=True,\n",
    "    # use_byte_stream_split=True,\n",
    "    # row_group_size=1048576  # 1MB row groups\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidx.to_parquet(file_path / 'hash_index.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Impose threshold (don't store below x, default x)\n",
    "* Can we automate to check rub probabilities?\n",
    "    * Find any gigantic clusters as a result\n",
    "    * Run at plausible thresholds?\n",
    "* Is threshold but need a rule of thumb\n",
    "    * If dedupe, n^2 - n/2\n",
    "    * If link, n^2 - n/2 ish\n",
    "* \"How do you know that\" is hard in a chain, and people want that\n",
    "\n",
    "* duckdb from pg OR pg from duckdb -- both available to work with pg\n",
    "    * Bad for atomic updates\n",
    "\n",
    "---\n",
    "\n",
    "100m\n",
    "\n",
    "ZSTD 10s at 1, 1.86Gb\n",
    "ZSTD 10s at 4, 1.86Gb \n",
    "ZSTD 50s at 15, 1.86Gb\n",
    "ZSTD 4m at 16, 1.86Gb\n",
    "ZSTD 7m at 22, 1.52Gb\n",
    "BROTLI 2m at default, 1.57Gb\n",
    "\n",
    "Index\n",
    "\n",
    "Snappy 13s at default, 3.8Gb\n",
    "BROTLI 6m at default, 2.6Gb\n",
    "ZSTD 20s at default, 2.75Gb\n",
    "\n",
    "ZSTD balanced between the two: 4.235Gb (3Gb saving)\n",
    "\n",
    "--\n",
    "\n",
    "Work through pg/duckdb idea\n",
    "\n",
    "- Clusters and contains need appending -- parquet or postgres?\n",
    "- How is duckdb informed about new parquet\n",
    "    - Lambda? API?\n",
    "- Can this perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Duckdb\n",
    "    * R/W or R mode -- we'd launch in one or tother\n",
    "        * No lambdas, just run the new command\n",
    "* Consolidating a dataset (pruning old records) -- like upsert\n",
    "    * 3 ways\n",
    "        * duckdb directly, w process. Load two parquets, prune old\n",
    "        * pandas\n",
    "        * polars\n",
    "* (?) married to parquet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
