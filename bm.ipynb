{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71288cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pa\n",
    "import pyarrow as pa\n",
    "\n",
    "from matchbox.common.hash import hash_data\n",
    "\n",
    "max_source_id = 1_000_000\n",
    "total_rows = 10_000_000\n",
    "df = pa.Table.from_pydict(\n",
    "    {\n",
    "        \"cluster_id\": pa.array(range(total_rows)),\n",
    "        \"probability\": pa.array(np.random.randint(0, 100, total_rows)),\n",
    "        \"leaves\": pa.array([\n",
    "            [hash_data(int(leaf)) for leaf in np.random.randint(0, max_source_id, 5)]\n",
    "            for _ in range(total_rows)\n",
    "        ]),\n",
    "        \"cluster_hash\": pa.array([\n",
    "            hash_data(int(scid))\n",
    "            for scid in np.random.randint(0, max_source_id, total_rows)\n",
    "        ]),\n",
    "    },\n",
    "    schema=pa.schema(\n",
    "        [\n",
    "            (\"cluster_id\", pa.uint32()),\n",
    "            (\"probability\", pa.uint8()),\n",
    "            (\"cluster_hash\", pa.binary()),\n",
    "            (\"leaves\", pa.list_(pa.binary()))\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "import pyarrow.parquet as pq\n",
    "pq.write_table(df, 'large.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8978214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pa\n",
    "import pyarrow as pa\n",
    "\n",
    "from matchbox.common.hash import hash_data\n",
    "\n",
    "import random, string\n",
    "import uuid\n",
    "\n",
    "total_rows = 10*10_000_000\n",
    "df = pa.Table.from_pydict(\n",
    "    {\n",
    "        \"source_id\": pa.array(np.random.randint(0,10, total_rows)),\n",
    "        \"hash\": pa.array([\n",
    "            hash_data(int(scid)) for scid in range(total_rows)\n",
    "        ]),\n",
    "        \"key\": pa.array([str(uuid.uuid4()) for _ in range(total_rows)])\n",
    "    },\n",
    "    schema=pa.schema(\n",
    "        [\n",
    "            (\"source_id\", pa.uint32()),\n",
    "            (\"hash\", pa.binary()),\n",
    "            (\"key\", pa.string()),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "import pyarrow.parquet as pq\n",
    "# pq.write_table(df, 'large.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "196662b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(df, 'keys.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c054e64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "table = pq.read_table('large.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb3c857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b18e1807",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.from_arrow(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bac246c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume transactional DDL.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(shape: (0, 0)\n",
       " ┌┐\n",
       " ╞╡\n",
       " └┘,\n",
       " shape: (0, 0)\n",
       " ┌┐\n",
       " ╞╡\n",
       " └┘)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "from matchbox.server.base import (\n",
    "    MatchboxDBAdapter,\n",
    "    MatchboxServerSettings,\n",
    "    get_backend_settings,\n",
    "    settings_to_backend,\n",
    ")\n",
    "\n",
    "get_backend_settings(MatchboxServerSettings().backend_type)\n",
    "\n",
    "SettingsClass = get_backend_settings(MatchboxServerSettings().backend_type)\n",
    "SETTINGS = SettingsClass()\n",
    "BACKEND = settings_to_backend(SETTINGS)\n",
    "\n",
    "\n",
    "BACKEND.s3_dump(123, pl.DataFrame(), pl.DataFrame())\n",
    "BACKEND.s3_load(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bdcf0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matchbox.client.helpers.index import index\n",
    "from matchbox.common.factories.sources import (\n",
    "    FeatureConfig,\n",
    "    LinkedSourcesTestkit,\n",
    "    SourceConfig,\n",
    "    SuffixRule,\n",
    "    linked_sources_factory,\n",
    ")\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "\n",
    "user = \"warehouse_user\"\n",
    "password = \"warehouse_password\"\n",
    "host = \"localhost\"\n",
    "database = \"warehouse\"\n",
    "port = 7654\n",
    "\n",
    "postgres_warehouse = create_engine(\n",
    "    f\"postgresql+psycopg://{user}:{password}@{host}:{port}/{database}\"\n",
    ")\n",
    "\n",
    "n_true_entities = 100\n",
    "\n",
    "\n",
    "# Create feature configurations\n",
    "features = {\n",
    "    \"company_name\": FeatureConfig(\n",
    "        name=\"company_name\",\n",
    "        base_generator=\"company\",\n",
    "    ),\n",
    "    \"crn\": FeatureConfig(\n",
    "        name=\"crn\",\n",
    "        base_generator=\"bothify\",\n",
    "        parameters=((\"text\", \"???-###-???-###\"),),\n",
    "    ),\n",
    "    \"duns\": FeatureConfig(\n",
    "        name=\"duns\",\n",
    "        base_generator=\"numerify\",\n",
    "        parameters=((\"text\", \"########\"),),\n",
    "    ),\n",
    "    \"cdms\": FeatureConfig(\n",
    "        name=\"cdms\",\n",
    "        base_generator=\"numerify\",\n",
    "        parameters=((\"text\", \"ORG-########\"),),\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Create source configurations that match our test fixtures\n",
    "source_configs = (\n",
    "    SourceConfig(\n",
    "        full_name=\"e2e.crn\",\n",
    "        engine=postgres_warehouse,\n",
    "        features=(\n",
    "            features[\"company_name\"].add_variations(\n",
    "                SuffixRule(suffix=\" Limited\"),\n",
    "                SuffixRule(suffix=\" UK\"),\n",
    "                SuffixRule(suffix=\" Company\"),\n",
    "            ),\n",
    "            features[\"crn\"],\n",
    "        ),\n",
    "        drop_base=True,\n",
    "        n_true_entities=n_true_entities,\n",
    "        repetition=0,  # No duplicates within the variations\n",
    "    ),\n",
    "    SourceConfig(\n",
    "        full_name=\"e2e.duns\",\n",
    "        engine=postgres_warehouse,\n",
    "        features=(\n",
    "            features[\"company_name\"],\n",
    "            features[\"duns\"],\n",
    "        ),\n",
    "        n_true_entities=n_true_entities // 2,  # Half the companies\n",
    "        repetition=0,\n",
    "    ),\n",
    "    SourceConfig(\n",
    "        full_name=\"e2e.cdms\",\n",
    "        engine=postgres_warehouse,\n",
    "        features=(\n",
    "            features[\"crn\"],\n",
    "            features[\"cdms\"],\n",
    "        ),\n",
    "        n_true_entities=n_true_entities,\n",
    "        repetition=1,  # Duplicate all rows\n",
    "    ),\n",
    ")\n",
    "\n",
    "linked_testkit = linked_sources_factory(\n",
    "    source_configs=source_configs,\n",
    "    seed=42,  # For reproducibility\n",
    ")\n",
    "\n",
    "with postgres_warehouse.connect() as conn:\n",
    "    conn.execute(text(\"CREATE SCHEMA IF NOT EXISTS e2e;\"))\n",
    "    conn.commit()\n",
    "\n",
    "# Setup code - Create tables in warehouse\n",
    "for source_testkit in linked_testkit.sources.values():\n",
    "    source_testkit.to_warehouse(engine=postgres_warehouse)\n",
    "\n",
    "for source_testkit in linked_testkit.sources.values():\n",
    "    source = source_testkit.source\n",
    "    index(\n",
    "        full_name=source.address.full_name,\n",
    "        db_pk=\"pk\",  # Primary key in our test data\n",
    "        engine=postgres_warehouse,\n",
    "        columns=[col.model_dump() for col in source.columns],\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
